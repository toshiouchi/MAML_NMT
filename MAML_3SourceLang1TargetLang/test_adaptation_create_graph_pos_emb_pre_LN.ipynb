{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8bf21a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install janome\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "#from torchtext.vocab import vocab\n",
    "#import torchtext.transforms as T\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#from torchvision import transforms\n",
    "import numpy as np\n",
    "import math\n",
    "import janome\n",
    "from janome.tokenizer import Tokenizer\n",
    "#import spacy\n",
    "from collections import Counter\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import time\n",
    "#from torchtext.vocab import build_vocab_from_iterator\n",
    "import levenshtein\n",
    "import json\n",
    "import pickle\n",
    "from timm.scheduler import CosineLRScheduler\n",
    "import nltk\n",
    "from nltk import bleu_score\n",
    "from torch.nn.init import constant_, xavier_uniform_, kaiming_uniform_, xavier_normal_, kaiming_normal_\n",
    "from torch.nn.parameter import Parameter\n",
    "from transformers import  get_linear_schedule_with_warmup\n",
    "from collections import OrderedDict\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "244f7857-07bc-45b9-89d6-e11c2e21b9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49528 20556\n"
     ]
    }
   ],
   "source": [
    "with open( \"corpus/id_to_word_s.pkl\", \"rb\" ) as f:\n",
    "    token_list = pickle.load(f)\n",
    "with open( \"corpus/id_to_word_t.pkl\", \"rb\" ) as f:\n",
    "    token_list_en = pickle.load(f)\n",
    "with open( \"corpus/word_to_id_s.pkl\", \"rb\" ) as f:\n",
    "    idx_list = pickle.load(f)\n",
    "with open( \"corpus/word_to_id_t.pkl\", \"rb\" ) as f:\n",
    "    idx_list_en = pickle.load(f)\n",
    "\n",
    "#token_list[4] = '<blank>'\n",
    "#token_list_en[4] = '<blank>'\n",
    "#idx_list['<pad>'] = 4\n",
    "#idx_list_en['<pad>'] = 4\n",
    "\n",
    "pad_idx_s = idx_list['<pad>']\n",
    "pad_idx_t = idx_list_en['<pad>']\n",
    "\n",
    "#enc_vocab_size, dec_vocab_size = len(token_list) - 1, len(token_list_en) - 1\n",
    "enc_vocab_size, dec_vocab_size = len(token_list), len(token_list_en)\n",
    "print(enc_vocab_size, dec_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86f1161a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<sos>', '<eos>', '<unk>', '<blank>', '<mask>', 'der']\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([ 0, 1,2,3,4,5,6 ])\n",
    "\n",
    "#print( token_list[2] )\n",
    "\n",
    "#n = 0\n",
    "\n",
    "#ii = [ i for i in a ]\n",
    "\n",
    "#print( ii )\n",
    "\n",
    "b = [ token_list[i.item()] for i in a ]\n",
    "\n",
    "print( b )\n",
    "\n",
    "\n",
    "d = idx_list['<pad>']\n",
    "\n",
    "print( d )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "649837a5-2937-4e81-880b-a57f2532b967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 乱数の種を設定\n",
    "def random_seed(value):\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    torch.manual_seed(value)\n",
    "    torch.cuda.manual_seed(value)\n",
    "    np.random.seed(value)\n",
    "    random.seed(value)\n",
    "\n",
    "# list からランダムに kosuu の数値をとってきて list を作る。\n",
    "def select( list, kosuu ):\n",
    "    output = random.sample( list.tolist(), kosuu )\n",
    "    return output\n",
    "\n",
    "# ( outer_batch の次元のある)taskset から outer_batch_size の outer_batch データを作る。    \n",
    "def create_batch_of_tasks(taskset, is_shuffle = True, outer_batch_size = 4):\n",
    "\n",
    "    idxs = list(range(0,len(taskset)))\n",
    "    if is_shuffle:\n",
    "        random.shuffle(idxs)\n",
    "        idxs = torch.tensor( idxs )\n",
    "        idxss = select( idxs, outer_batch_size )\n",
    "    else:\n",
    "        idxs = torch.tensor( idxs )\n",
    "        idxss = idxs[ :outer_batch_size ]\n",
    "    \n",
    "    output = []\n",
    "\n",
    "    for i in idxss:\n",
    "        output.append( taskset[i] )\n",
    "\n",
    "    return output\n",
    "\n",
    "def build_batch_set( trainset, pad_idx_s, pad_idx_t, num_task = 17, k_support = 10, k_query = 10 ):\n",
    "\n",
    "    count = []\n",
    "    rr_s = []\n",
    "    rr_t = []\n",
    "    rr_len_s = []\n",
    "    rr_len_t = []\n",
    "    s_max = []\n",
    "    t_max = []\n",
    "    for i in range( num_task ):\n",
    "        count.append( 0 )\n",
    "        rr_s.append( [] )\n",
    "        rr_t.append( [] )\n",
    "        rr_len_s.append( [] )\n",
    "        rr_len_t.append( [] )\n",
    "        s_max.append(0)\n",
    "        t_max.append(0)\n",
    "    \n",
    "    for r in trainset:\n",
    "        for i in range( num_task ):\n",
    "            if r['domain'] == i:\n",
    "                count[i] += 1\n",
    "                rr_s[i].append( r['source'] )\n",
    "                rr_t[i].append( r['target'] )\n",
    "                rr_len_s[i].append( len( r['source']))\n",
    "                rr_len_t[i].append( len( r['target']))\n",
    "                if len( r['source'] ) > s_max[i]:\n",
    "                    s_max[i] = len( r['source'] )\n",
    "                if len( r['target'] ) > t_max[i]:\n",
    "                    t_max[i] = len( r['target'] )\n",
    "    \n",
    "    ss_max = max( s_max )\n",
    "    tt_max = max( t_max )\n",
    "    print( \"ss_max:\",ss_max )\n",
    "    print( \"tt_max:\",tt_max )\n",
    "                \n",
    "    #print( rr[0][0] )\n",
    "    \n",
    "    for i in range( num_task ):\n",
    "        print( \"i:\",i, \" count: \" ,count[i] )\n",
    "        \n",
    "    max_count = max( count )\n",
    "    min_count = min( count )\n",
    "    \n",
    "    #print( \"min_count:\", min_count )\n",
    "    \n",
    "    batch_max = max_count // (k_support + k_query )\n",
    "    print( \"batch_max:\", batch_max )\n",
    "    batch_min = min_count // (k_support + k_query )\n",
    "    print( \"batch_min:\", batch_min )\n",
    "    \n",
    "    batch = batch_min\n",
    "\n",
    "    print( batch * ( k_support + k_query ))\n",
    "    print( len( rr_s[0] ))\n",
    "    print( len( rr_t[0] ))\n",
    "    \n",
    "    output = []\n",
    "\n",
    "    for i in range( batch ):\n",
    "        if i % 10000 ==0:\n",
    "            print( \"i:\", i )\n",
    "\n",
    "        output0 = []\n",
    "        supports_source = []\n",
    "        supports_target = []\n",
    "        queries_source = []\n",
    "        queries_target = []\n",
    "        supports_source_len = []\n",
    "        supports_target_len = []\n",
    "        queries_source_len = []\n",
    "        queries_target_len = []\n",
    "        for j in range( num_task ):\n",
    "            supports_source0 = []\n",
    "            supports_target0 = []\n",
    "            queries_source0 = []\n",
    "            queries_target0 = []\n",
    "            supports_source0_len = []\n",
    "            supports_target0_len = []\n",
    "            queries_source0_len = []\n",
    "            queries_target0_len = []\n",
    "            for k in range( k_support ):\n",
    "                ik2 = i * ( k_support + k_query ) + k\n",
    "                while ik2 >= len( rr_s[j] ):\n",
    "                    ik2 -= len( rr_s[j] )\n",
    "                #print( \"i,:\",i, \"j:\", j, \"len(rr[j]):\", len(rr[j]), \"ik2:\", ik2 )\n",
    "                pad_len = ss_max - rr_len_s[j][ik2]\n",
    "                tmp_s = np.pad( rr_s[j][ik2], (0, pad_len), mode='constant', constant_values=(pad_idx_s, pad_idx_s ))\n",
    "                supports_source0.append( tmp_s )\n",
    "                pad_len = tt_max - rr_len_t[j][ik2]\n",
    "                tmp_t = np.pad( rr_t[j][ik2], (0, pad_len), mode='constant', constant_values=(pad_idx_t, pad_idx_t ))\n",
    "                supports_target0.append( tmp_t ) \n",
    "                supports_source0_len.append( rr_len_s[j][ik2])\n",
    "                supports_target0_len.append( rr_len_t[j][ik2] ) \n",
    "            for k in range( k_query ):\n",
    "                ik2 = i * ( k_support + k_query ) + k_support + k\n",
    "                while ik2 >= len( rr_s[j] ):\n",
    "                    ik2 -= len( rr_s[j] )\n",
    "                pad_len = ss_max - rr_len_s[j][ik2]\n",
    "                tmp_s = np.pad( rr_s[j][ik2], (0, pad_len), mode='constant', constant_values=(pad_idx_s, pad_idx_s ))\n",
    "                queries_source0.append( tmp_s )\n",
    "                pad_len = tt_max - rr_len_t[j][ik2]\n",
    "                tmp_t = np.pad( rr_t[j][ik2], (0, pad_len), mode='constant', constant_values=(pad_idx_t, pad_idx_t ))\n",
    "                queries_target0.append( tmp_t ) \n",
    "                queries_source0_len.append(rr_len_s[j][ik2])\n",
    "                queries_target0_len.append( rr_len_t[j][ik2] ) \n",
    "            supports_source.append(supports_source0)\n",
    "            supports_target.append( supports_target0 ) \n",
    "            queries_source.append(queries_source0)\n",
    "            queries_target.append( queries_target0 ) \n",
    "            supports_source_len.append(supports_source0_len)\n",
    "            supports_target_len.append( supports_target0_len ) \n",
    "            queries_source_len.append(queries_source0_len)\n",
    "            queries_target_len.append( queries_target0_len ) \n",
    "        output0.append( supports_source )\n",
    "        output0.append( supports_source_len )\n",
    "        output0.append( supports_target )\n",
    "        output0.append( supports_target_len )\n",
    "        output0.append( queries_source )\n",
    "        output0.append( queries_source_len )\n",
    "        output0.append( queries_target )\n",
    "        output0.append( queries_target_len )\n",
    "        output.append( output0 )\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6633015a-bc68-40d7-8d7c-cb4c63029f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ss_max: 184\n",
      "tt_max: 191\n",
      "i: 0  count:  528\n",
      "i: 1  count:  495\n",
      "i: 2  count:  478\n",
      "batch_max: 52\n",
      "batch_min: 47\n",
      "470\n",
      "528\n",
      "528\n",
      "i: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device( \"cpu\")\n",
    "\n",
    "# dataset\n",
    "with open('data_test.pkl', 'rb') as f:\n",
    "    testset = pickle.load(f)\n",
    "\n",
    "pad_idx_s = idx_list['<pad>']\n",
    "pad_idx_t = idx_list_en['<pad>']\n",
    "    \n",
    "#num_ob_val = 10\n",
    "ob_test = build_batch_set( testset, pad_idx_s, pad_idx_t, num_task = 3, k_support = 5, k_query = 5 )\n",
    "#ob_val = ob_val[:num_ob_val]\n",
    "db_test = create_batch_of_tasks(ob_test, is_shuffle = False, outer_batch_size = len( ob_test ))\n",
    "\n",
    "del testset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a02a5779",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        #self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        #return self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "434ca8e2-10dc-4f72-a464-e1155bb52d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    '''\n",
    "    位置埋め込み （Positional embedding）\n",
    "    dim_embedding: 埋込み次元\n",
    "    max_len      : 入力の最大系列長\n",
    "    '''\n",
    "    def __init__(self, dim_embedding: int, max_len: int=5000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pos_emb = nn.Embedding(max_len, dim_embedding)\n",
    "\n",
    "    '''\n",
    "    位置エンコーディングの順伝播\n",
    "    x: 位置エンコーディングを埋め込む対象のテンソル,\n",
    "       [バッチサイズ, 系列長, 埋め込み次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        seq = x.shape[1]\n",
    "        positions = torch.arange(start=0, end=seq, step=1, device=x.device).to(torch.long)\n",
    "        positions = self.pos_emb(positions)[:seq,:]\n",
    "        \n",
    "        return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79d4879d-0bb9-40d8-94da-913b2955d7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fPositionalEmbedding(nn.Module):\n",
    "    '''\n",
    "    位置埋め込み （Positional embedding）\n",
    "    dim_embedding: 埋込み次元\n",
    "    max_len      : 入力の最大系列長\n",
    "    '''\n",
    "    def __init__(self, dim_embedding: int, name, max_len: int=5000):\n",
    "        super().__init__()\n",
    "\n",
    "        #self.pos_emb = nn.Embedding(max_len, dim_embedding)\n",
    "        self.name = name\n",
    "        \n",
    "    '''\n",
    "    位置エンコーディングの順伝播\n",
    "    x: 位置エンコーディングを埋め込む対象のテンソル,\n",
    "       [バッチサイズ, 系列長, 埋め込み次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor, weights):\n",
    "        seq = x.shape[1]\n",
    "        positions = torch.arange(start=0, end=seq, step=1, device=x.device).to(torch.long)\n",
    "        #positions = self.pos_emb(positions)[:seq,:]\n",
    "        positions = F.embedding( positions, weights[self.name] )[:seq,:]\n",
    "        \n",
    "        return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de837c69-4e8b-4b21-bf3f-cb6e9f0cd0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "    '''\n",
    "    自己アテンション\n",
    "    dim_hidden: 入力特徴量の次元\n",
    "    num_heads : マルチヘッドアテンションのヘッド数\n",
    "    qkv_bias  : クエリなどを生成する全結合層のバイアスの有無\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 qkv_bias: bool=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # 特徴量を各ヘッドのために分割するので、\n",
    "        # 特徴量次元をヘッド数で割り切れるか検証\n",
    "        assert dim_hidden % num_heads == 0\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # ヘッド毎の特徴量次元\n",
    "        dim_head = dim_hidden // num_heads\n",
    "\n",
    "        # ソフトマックスのスケール値\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        # ヘッド毎にクエリ、キーおよびバリューを生成するための全結合層\n",
    "        self.proj_in_q = nn.Linear(\n",
    "            dim_hidden, dim_hidden, bias=qkv_bias)\n",
    "        self.proj_in_k = nn.Linear(\n",
    "            dim_hidden, dim_hidden, bias=qkv_bias)\n",
    "        self.proj_in_v = nn.Linear(\n",
    "            dim_hidden, dim_hidden, bias=qkv_bias)\n",
    "\n",
    "        #self.in_proj_weight = nn.Parameter( torch.randn( dim_hidden * 3, dim_hidden ) )\n",
    "        #self.in_proj_bias = nn.Parameter( torch.randn( dim_hidden * 3 ) )\n",
    "        #self.in_proj_weight = Parameter( torch.empty( dim_hidden * 3, dim_hidden ) )\n",
    "        #self.in_proj_bias = Parameter( torch.empty( dim_hidden * 3 ) )\n",
    "\n",
    "        # 各ヘッドから得られた特徴量を一つにまとめる全結合層\n",
    "        self.proj_out = nn.Linear(dim_hidden, dim_hidden)\n",
    "        \n",
    "        #self._reset_parameters()\n",
    "        \n",
    "    #def _reset_parameters(self):\n",
    "    #    xavier_uniform_( self.in_proj_weight )\n",
    "    #    constant_( self.in_proj_bias, 0.0 )\n",
    "\n",
    "    def split_head(self, x):\n",
    "        x = torch.tensor_split(x, self.num_heads, dim = 2)\n",
    "        x = torch.stack(x, dim = 1)\n",
    "        return x\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, mask: torch.Tensor):\n",
    "\n",
    "        #bs_q, ns_q, ds_q = q.size()\n",
    "        #bs_k, ns_k, ds_k = k.size()\n",
    "        \n",
    "        ##  k = v assumpotion\n",
    "        #if q is k:\n",
    "        #    qkv = q @ self.in_proj_weight.transpose(-2,-1) + self.in_proj_bias\n",
    "        #    qkv = qkv.view( bs_q, ns_q, 3, ds_q )\n",
    "        #    q, k, v = torch.unbind( qkv, dim = 2 )\n",
    "        #else:\n",
    "        #    W_q, W_kv = self.in_proj_weight.split([ds_q, ds_q * 2])\n",
    "        #    b_q, b_kv = self.in_proj_bias.split([ds_q, ds_q * 2])\n",
    "        #    q = q @ W_q.transpose(-2,-1) + b_q\n",
    "        #    kv =  k @ W_kv.transpose(-2,-1) + b_kv\n",
    "        #    kv = kv.view( bs_k, ns_k, 2, ds_k )\n",
    "        #    k, v = torch.unbind( kv, dim = 2 )\n",
    "        \n",
    "        q = self.proj_in_q(q)\n",
    "        k = self.proj_in_k(k)\n",
    "        v = self.proj_in_v(v)\n",
    "        \n",
    "        q = self.split_head(q)\n",
    "        k = self.split_head(k)\n",
    "        v = self.split_head(v)\n",
    "\n",
    "        # クエリとキーの行列積とアテンションの計算(今回マスクは不使用)\n",
    "        # attnは[バッチサイズ, ヘッド数, 特徴量数, 特徴量数]\n",
    "        attn = q.matmul(k.transpose(-2, -1))\n",
    "        #print( \"attn size:\", attn.size() )\n",
    "        #print( \"mask size:\", mask.size() )\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill_(mask, -torch.finfo(torch.float).max)\n",
    "        attn = (attn * self.scale).softmax(dim=-1)\n",
    "\n",
    "        # アテンションとバリューの行列積によりバリューを収集\n",
    "        # xは[バッチサイズ, ヘッド数, 特徴量数, ヘッドの特徴量次元]\n",
    "        x = attn.matmul(v)\n",
    "\n",
    "        # permute関数により\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数, ヘッドの特徴量次元]\n",
    "        # flatten関数により全てのヘッドから得られる特徴量を連結して、\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数 * ヘッドの特徴量次元]\n",
    "        x = x.permute(0, 2, 1, 3).flatten(2)\n",
    "        x = self.proj_out(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8aa83af-c34f-4e9a-80e0-00fef0bb5135",
   "metadata": {},
   "outputs": [],
   "source": [
    "class feMHA(nn.Module):\n",
    "    '''\n",
    "    自己アテンション\n",
    "    dim_hidden: 入力特徴量の次元\n",
    "    num_heads : マルチヘッドアテンションのヘッド数\n",
    "    qkv_bias  : クエリなどを生成する全結合層のバイアスの有無\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 qkv_bias: bool=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # 特徴量を各ヘッドのために分割するので、\n",
    "        # 特徴量次元をヘッド数で割り切れるか検証\n",
    "        assert dim_hidden % num_heads == 0\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # ヘッド毎の特徴量次元\n",
    "        dim_head = dim_hidden // num_heads\n",
    "\n",
    "        # ソフトマックスのスケール値\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        # ヘッド毎にクエリ、キーおよびバリューを生成するための全結合層\n",
    "        #self.proj_in = nn.Linear(\n",
    "        #    dim_hidden, dim_hidden * 3, bias=qkv_bias)\n",
    "\n",
    "        # 各ヘッドから得られた特徴量を一つにまとめる全結合層\n",
    "        #self.proj_out = nn.Linear(dim_hidden, dim_hidden)\n",
    "\n",
    "    def split_head(self, x):\n",
    "        x = torch.tensor_split(x, self.num_heads, dim = 2)\n",
    "        x = torch.stack(x, dim = 1)\n",
    "        return x\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor,  v: torch.Tensor, mask: torch.Tensor, i, weights ):\n",
    "        bs, ns = q.shape[:2]\n",
    "\n",
    "        #qkv = self.proj_in(x)\n",
    "        q = F.linear(q, weights['encoder.encoder_layers.' + str(i) + '.attention.proj_in_q.weight'], weights['encoder.encoder_layers.' + str(i) + '.attention.proj_in_q.bias'])\n",
    "        k = F.linear(k, weights['encoder.encoder_layers.' + str(i) + '.attention.proj_in_k.weight'], weights['encoder.encoder_layers.' + str(i) + '.attention.proj_in_k.bias'])\n",
    "        v = F.linear(v, weights['encoder.encoder_layers.' + str(i) + '.attention.proj_in_v.weight'], weights['encoder.encoder_layers.' + str(i) + '.attention.proj_in_v.bias'])\n",
    "\n",
    "        q = self.split_head( q )\n",
    "        k = self.split_head( k )\n",
    "        v = self.split_head( v )\n",
    "\n",
    "        # クエリとキーの行列積とアテンションの計算(今回マスクは不使用)\n",
    "        # attnは[バッチサイズ, ヘッド数, 特徴量数, 特徴量数]\n",
    "        attn = q.matmul(k.transpose(-2, -1))\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill_(mask, -torch.finfo(torch.float).max)\n",
    "        attn = (attn * self.scale).softmax(dim=-1)\n",
    "\n",
    "        # アテンションとバリューの行列積によりバリューを収集\n",
    "        # xは[バッチサイズ, ヘッド数, 特徴量数, ヘッドの特徴量次元]\n",
    "        x = attn.matmul(v)\n",
    "\n",
    "        # permute関数により\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数, ヘッドの特徴量次元]\n",
    "        # flatten関数により全てのヘッドから得られる特徴量を連結して、\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数 * ヘッドの特徴量次元]\n",
    "        x = x.permute(0, 2, 1, 3).flatten(2)\n",
    "        #x = self.proj_out(x)\n",
    "        x = F.linear(x, weights['encoder.encoder_layers.' + str(i) + '.attention.proj_out.weight'], weights['encoder.encoder_layers.' + str(i) + '.attention.proj_out.bias'])\n",
    "\n",
    "        return x\n",
    "\n",
    "class fdsMHA(nn.Module):\n",
    "    '''\n",
    "    自己アテンション\n",
    "    dim_hidden: 入力特徴量の次元\n",
    "    num_heads : マルチヘッドアテンションのヘッド数\n",
    "    qkv_bias  : クエリなどを生成する全結合層のバイアスの有無\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 qkv_bias: bool=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # 特徴量を各ヘッドのために分割するので、\n",
    "        # 特徴量次元をヘッド数で割り切れるか検証\n",
    "        assert dim_hidden % num_heads == 0\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # ヘッド毎の特徴量次元\n",
    "        dim_head = dim_hidden // num_heads\n",
    "\n",
    "        # ソフトマックスのスケール値\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        # ヘッド毎にクエリ、キーおよびバリューを生成するための全結合層\n",
    "        #self.proj_in = nn.Linear(\n",
    "        #    dim_hidden, dim_hidden * 3, bias=qkv_bias)\n",
    "\n",
    "        # 各ヘッドから得られた特徴量を一つにまとめる全結合層\n",
    "        #self.proj_out = nn.Linear(dim_hidden, dim_hidden)\n",
    "\n",
    "    def split_head(self, x):\n",
    "        x = torch.tensor_split(x, self.num_heads, dim = 2)\n",
    "        x = torch.stack(x, dim = 1)\n",
    "        return x\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor,  v: torch.Tensor, mask: torch.Tensor, i, weights ):\n",
    "        bs, ns = q.shape[:2]\n",
    "\n",
    "        #qkv = self.proj_in(x)\n",
    "        #print( \"size q:\", q.size() )\n",
    "        #print( \"size weigths q:\", weights['d_layers.' + str(i) + '.crossattn.proj_in_q.weight'].size() )\n",
    "        #print( \"size k:\", k.size() )\n",
    "        #print( \"size weights k:\", weights['d_layers.' + str(i) + '.crossattn.proj_in_k.weight'].size() )\n",
    "        q = F.linear(q, weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_in_q.weight'], weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_in_q.bias'])\n",
    "        k = F.linear(k, weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_in_k.weight'], weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_in_k.bias'])\n",
    "        v = F.linear(v, weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_in_v.weight'], weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_in_v.bias'])\n",
    "\n",
    "        q = self.split_head( q )\n",
    "        k = self.split_head( k )\n",
    "        v = self.split_head( v )\n",
    "\n",
    "        # view関数により\n",
    "        # [バッチサイズ, 特徴量数, QKV, ヘッド数, ヘッドの特徴量次元]\n",
    "        # permute関数により\n",
    "        # [QKV, バッチサイズ, ヘッド数, 特徴量数, ヘッドの特徴量次元]\n",
    "        #qkv = qkv.view(\n",
    "        #    bs, ns, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        # クエリ、キーおよびバリューに分解\n",
    "        #q, k, v = qkv.unbind(0)\n",
    "\n",
    "        # クエリとキーの行列積とアテンションの計算(今回マスクは不使用)\n",
    "        # attnは[バッチサイズ, ヘッド数, 特徴量数, 特徴量数]\n",
    "        attn = q.matmul(k.transpose(-2, -1))\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill_(mask, -torch.finfo(torch.float).max)      \n",
    "        attn = (attn * self.scale).softmax(dim=-1)\n",
    "\n",
    "        # アテンションとバリューの行列積によりバリューを収集\n",
    "        # xは[バッチサイズ, ヘッド数, 特徴量数, ヘッドの特徴量次元]\n",
    "        x = attn.matmul(v)\n",
    "\n",
    "        # permute関数により\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数, ヘッドの特徴量次元]\n",
    "        # flatten関数により全てのヘッドから得られる特徴量を連結して、\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数 * ヘッドの特徴量次元]\n",
    "        x = x.permute(0, 2, 1, 3).flatten(2)\n",
    "        #x = self.proj_out(x)\n",
    "        x = F.linear(x, weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_out.weight'], weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_out.bias'])\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class fdcMHA(nn.Module):\n",
    "    '''\n",
    "    自己アテンション\n",
    "    dim_hidden: 入力特徴量の次元\n",
    "    num_heads : マルチヘッドアテンションのヘッド数\n",
    "    qkv_bias  : クエリなどを生成する全結合層のバイアスの有無\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 qkv_bias: bool=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # 特徴量を各ヘッドのために分割するので、\n",
    "        # 特徴量次元をヘッド数で割り切れるか検証\n",
    "        assert dim_hidden % num_heads == 0\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # ヘッド毎の特徴量次元\n",
    "        dim_head = dim_hidden // num_heads\n",
    "\n",
    "        # ソフトマックスのスケール値\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        # ヘッド毎にクエリ、キーおよびバリューを生成するための全結合層\n",
    "        #self.proj_in = nn.Linear(\n",
    "        #    dim_hidden, dim_hidden * 3, bias=qkv_bias)\n",
    "\n",
    "        # 各ヘッドから得られた特徴量を一つにまとめる全結合層\n",
    "        #self.proj_out = nn.Linear(dim_hidden, dim_hidden)\n",
    "\n",
    "    def split_head(self, x):\n",
    "        x = torch.tensor_split(x, self.num_heads, dim = 2)\n",
    "        x = torch.stack(x, dim = 1)\n",
    "        return x\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor,  v: torch.Tensor, mask: torch.Tensor, i, weights ):\n",
    "        bs, ns = q.shape[:2]\n",
    "\n",
    "        #qkv = self.proj_in(x)\n",
    "        #print( \"size q:\", q.size() )\n",
    "        #print( \"size weigths q:\", weights['d_layers.' + str(i) + '.crossattn.proj_in_q.weight'].size() )\n",
    "        #print( \"size k:\", k.size() )\n",
    "        #print( \"size weights k:\", weights['d_layers.' + str(i) + '.crossattn.proj_in_k.weight'].size() )\n",
    "        q = F.linear(q, weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_in_q.weight'], weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_in_q.bias'])\n",
    "        k = F.linear(k, weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_in_k.weight'], weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_in_k.bias'])\n",
    "        v = F.linear(v, weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_in_v.weight'], weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_in_v.bias'])\n",
    "\n",
    "        q = self.split_head( q )\n",
    "        k = self.split_head( k )\n",
    "        v = self.split_head( v )\n",
    "\n",
    "        # view関数により\n",
    "        # [バッチサイズ, 特徴量数, QKV, ヘッド数, ヘッドの特徴量次元]\n",
    "        # permute関数により\n",
    "        # [QKV, バッチサイズ, ヘッド数, 特徴量数, ヘッドの特徴量次元]\n",
    "        #qkv = qkv.view(\n",
    "        #    bs, ns, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        # クエリ、キーおよびバリューに分解\n",
    "        #q, k, v = qkv.unbind(0)\n",
    "\n",
    "        # クエリとキーの行列積とアテンションの計算(今回マスクは不使用)\n",
    "        # attnは[バッチサイズ, ヘッド数, 特徴量数, 特徴量数]\n",
    "        attn = q.matmul(k.transpose(-2, -1))\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill_(mask, -torch.finfo(torch.float).max)      \n",
    "        attn = (attn * self.scale).softmax(dim=-1)\n",
    "\n",
    "        # アテンションとバリューの行列積によりバリューを収集\n",
    "        # xは[バッチサイズ, ヘッド数, 特徴量数, ヘッドの特徴量次元]\n",
    "        x = attn.matmul(v)\n",
    "\n",
    "        # permute関数により\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数, ヘッドの特徴量次元]\n",
    "        # flatten関数により全てのヘッドから得られる特徴量を連結して、\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数 * ヘッドの特徴量次元]\n",
    "        x = x.permute(0, 2, 1, 3).flatten(2)\n",
    "        #x = self.proj_out(x)\n",
    "        x = F.linear(x, weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_out.weight'], weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_out.bias'])\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53ed6959-9a0e-4aa9-9f27-dfa6875fd4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "    '''\n",
    "    Transformerエンコーダ内の順伝播型ニューラルネットワーク\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    dim_feedforward: 中間特徴量の次元\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, dim_feedforward: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(dim_hidden, dim_feedforward)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, dim_hidden)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fa69cc3-8e7b-4a49-bb37-bfcb117caa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class feFNN(nn.Module):\n",
    "    '''\n",
    "    Transformerエンコーダ内の順伝播型ニューラルネットワーク\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    dim_feedforward: 中間特徴量の次元\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, dim_feedforward: int):\n",
    "        super().__init__()\n",
    "\n",
    "        #self.linear1 = nn.Linear(dim_hidden, dim_feedforward)\n",
    "        #self.linear2 = nn.Linear(dim_feedforward, dim_hidden)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor, i, weights ):\n",
    "        #x = self.linear1(x)\n",
    "        x = F.linear(x, weights['encoder.encoder_layers.' + str(i) + '.fnn.linear1.weight'], weights['encoder.encoder_layers.' + str(i) + '.fnn.linear1.bias'])\n",
    "        x = self.activation(x)\n",
    "        #x = self.linear2(x)\n",
    "        x = F.linear(x, weights['encoder.encoder_layers.' + str(i) + '.fnn.linear2.weight'], weights['encoder.encoder_layers.' + str(i) + '.fnn.linear2.bias'])\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d15bdb0-9771-4199-bd25-8f557bf4229c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fdFNN(nn.Module):\n",
    "    '''\n",
    "    Transformerエンコーダ内の順伝播型ニューラルネットワーク\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    dim_feedforward: 中間特徴量の次元\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, dim_feedforward: int):\n",
    "        super().__init__()\n",
    "\n",
    "        #self.linear1 = nn.Linear(dim_hidden, dim_feedforward)\n",
    "        #self.linear2 = nn.Linear(dim_feedforward, dim_hidden)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor, i, weights ):\n",
    "        #x = self.linear1(x)\n",
    "        x = F.linear(x, weights['decoder.decoder_layers.' + str(i) + '.fnn.linear1.weight'], weights['decoder.decoder_layers.' + str(i) + '.fnn.linear1.bias'])\n",
    "        x = self.activation(x)\n",
    "        #x = self.linear2(x)\n",
    "        x = F.linear(x, weights['decoder.decoder_layers.' + str(i) + '.fnn.linear2.weight'], weights['decoder.decoder_layers.' + str(i) + '.fnn.linear2.bias'])\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a40be501-d70c-49c3-8b74-381680e36c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    '''\n",
    "    Transformerエンコーダ層\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    num_heads      : ヘッド数\n",
    "    dim_feedforward: 中間特徴量の次元\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 dim_feedforward: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = MHA(dim_hidden, num_heads, qkv_bias = True)\n",
    "        self.fnn = FNN(dim_hidden, dim_feedforward)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(dim_hidden)\n",
    "        self.norm2 = nn.LayerNorm(dim_hidden)\n",
    "        \n",
    "        self.dropout = nn.Dropout( dropout )\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor):\n",
    "        ''' B2T\n",
    "        x0 = x\n",
    "        x = self.attention( x, x, x, mask )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x = self.norm1(x)\n",
    "        x1 = x\n",
    "        x = self.fnn( x )\n",
    "        x = self.dropout( x )\n",
    "        x =  x + x1 + x0\n",
    "        x = self.norm2( x )\n",
    "        '''\n",
    "        #pre-LN\n",
    "        x0 = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attention( x, x, x, mask )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x1 = x\n",
    "        x = self.norm2( x )\n",
    "        x = self.fnn( x )\n",
    "        x = self.dropout( x )\n",
    "        x =  x + x1\n",
    "        \n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f35d262a-7d0f-42ce-8cd8-956d800598bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fTransformerEncoderLayer(nn.Module):\n",
    "    '''\n",
    "    Transformerエンコーダ層\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    num_heads      : ヘッド数\n",
    "    dim_feedforward: 中間特徴量の次元\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 dim_feedforward: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fattention = feMHA(dim_hidden, num_heads)\n",
    "        self.fffn = feFNN(dim_hidden, dim_feedforward)\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.dropout = nn.Dropout( dropout )\n",
    "        \n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor, i, weights ):\n",
    "        '''B2T\n",
    "        x0 = x\n",
    "        x = self.fattention( x, x, x, mask, i, weights )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['encoder.encoder_layers.' + str(i) + '.norm1.weight'], bias=weights['encoder.encoder_layers.' + str(i) + '.norm1.bias'], eps=1e-05)\n",
    "        x1 = x\n",
    "        x = self.fffn( x, i, weights ) \n",
    "        x = self.dropout( x )\n",
    "        x = x + x1 + x0\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['encoder.encoder_layers.' + str(i) + '.norm2.weight'], bias=weights['encoder.encoder_layers.' + str(i) + '.norm2.bias'], eps=1e-05)\n",
    "        '''\n",
    "        #pre-LN\n",
    "        x0 = x\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['encoder.encoder_layers.' + str(i) + '.norm1.weight'], bias=weights['encoder.encoder_layers.' + str(i) + '.norm1.bias'], eps=1e-05)\n",
    "        x = self.fattention( x, x, x, mask, i, weights )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x1 = x\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['encoder.encoder_layers.' + str(i) + '.norm2.weight'], bias=weights['encoder.encoder_layers.' + str(i) + '.norm2.bias'], eps=1e-05)\n",
    "        x = self.fffn( x, i, weights ) \n",
    "        x = self.dropout( x )\n",
    "        x = x + x1\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb21075b-7d9f-4a1e-a76e-9d7fd9c68345",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    '''\n",
    "    Transformerエンコーダ層\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    num_heads      : ヘッド数\n",
    "    dim_feedforward: 中間特徴量の次元\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 dim_feedforward: int, dropout: float=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.selfattn = MHA(dim_hidden, num_heads, qkv_bias = True)\n",
    "        self.crossattn = MHA(dim_hidden, num_heads, qkv_bias = True)\n",
    "        self.fnn = FNN(dim_hidden, dim_feedforward)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(dim_hidden)\n",
    "        self.norm2 = nn.LayerNorm(dim_hidden)\n",
    "        self.norm3 = nn.LayerNorm(dim_hidden)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor,  self_mask: torch.Tensor,  cross_mask: torch.Tensor):\n",
    "        '''B2T\n",
    "        x0 = x\n",
    "        x = self.selfattn( x, x, x, self_mask )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x = self.norm1( x )\n",
    "        x1 = x\n",
    "        x = self.crossattn( x, y, y, cross_mask )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x1\n",
    "        x = self.norm2( x )\n",
    "        x2 = x\n",
    "        x = self.fnn( x )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x2 + x0\n",
    "        x = self.norm3( x )\n",
    "        '''\n",
    "        #pre-LN\n",
    "        x0 = x\n",
    "        x = self.norm1( x )\n",
    "        x = self.selfattn( x, x, x, self_mask )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x1 = x\n",
    "        x = self.norm2( x )\n",
    "        x = self.crossattn( x, y, y, cross_mask )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x1\n",
    "        x2 = x\n",
    "        x = self.norm3( x )\n",
    "        x = self.fnn( x )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x2\n",
    "        \n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78cd200d-38ac-4fc4-bb4c-551fb51eb56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fTransformerDecoderLayer(nn.Module):\n",
    "    '''\n",
    "    Transformerエンコーダ層\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    num_heads      : ヘッド数\n",
    "    dim_feedforward: 中間特徴量の次元\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 dim_feedforward: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fselfattn = fdsMHA(dim_hidden, num_heads)\n",
    "        self.fcrossattn = fdcMHA(dim_hidden, num_heads)\n",
    "        self.fffn = fdFNN(dim_hidden, dim_feedforward)\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        #self.norm1 = nn.LayerNorm(dim_hidden)\n",
    "        #self.norm2 = nn.LayerNorm(dim_hidden)\n",
    "        #self.norm3 = nn.LayerNorm(dim_hidden)\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor,  self_mask: torch.Tensor,  cross_mask: torch.Tensor, i, weights):\n",
    "        '''B2T\n",
    "        x0 = x\n",
    "        x = self.fselfattn( x, x, x, self_mask, i, weights )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['decoder.decoder_layers.' + str(i) + '.norm1.weight'], bias=weights['decoder.decoder_layers.' + str(i) + '.norm1.bias'], eps=1e-05)\n",
    "        x1 = x\n",
    "        x = self.fcrossattn( x, y, y, cross_mask, i, weights )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x1\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['decoder.decoder_layers.' + str(i) + '.norm2.weight'], bias=weights['decoder.decoder_layers.' + str(i) + '.norm2.bias'], eps=1e-05)\n",
    "        x2 = x\n",
    "        x = self.fffn( x, i, weights )\n",
    "        x = self.dropout( x ) \n",
    "        x = x + x2 + x0\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['decoder.decoder_layers.' + str(i) + '.norm3.weight'], bias=weights['decoder.decoder_layers.' + str(i) + '.norm3.bias'], eps=1e-05)\n",
    "        '''\n",
    "        #pre-LN\n",
    "        x0 = x\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['decoder.decoder_layers.' + str(i) + '.norm1.weight'], bias=weights['decoder.decoder_layers.' + str(i) + '.norm1.bias'], eps=1e-05)\n",
    "        x = self.fselfattn( x, x, x, self_mask, i, weights )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x1 = x\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['decoder.decoder_layers.' + str(i) + '.norm2.weight'], bias=weights['decoder.decoder_layers.' + str(i) + '.norm2.bias'], eps=1e-05)\n",
    "        x = self.fcrossattn( x, y, y, cross_mask, i, weights )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x1\n",
    "        x2 = x\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['decoder.decoder_layers.' + str(i) + '.norm3.weight'], bias=weights['decoder.decoder_layers.' + str(i) + '.norm3.bias'], eps=1e-05)\n",
    "        x = self.fffn( x, i, weights )\n",
    "        x = self.dropout( x ) \n",
    "        x = x + x2\n",
    "        \n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73e436f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    '''\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    dim_feedforward: FNNにおける中間特徴量の次元\n",
    "    num_heads      : マルチヘッドアテンションのヘッド数\n",
    "    num_layers     : Transformerエンコーダの層数\n",
    "    '''\n",
    "    def __init__(self, text_vocab_size: int, dim_embedding: int, dim_feedforward: int,\n",
    "                 num_heads: int,  num_layers: int, pad_index:int, dropout: float = 0.1 ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 単語埋め込み\n",
    "        self.embed = nn.Embedding(\n",
    "            text_vocab_size, dim_embedding, padding_idx=pad_index)        \n",
    "        \n",
    "        # 位置エンコーディング\n",
    "        #self.pos_enc = PositionalEncoding(dim_embedding)\n",
    "        self.pos_emb = PositionalEmbedding(dim_embedding)\n",
    "        #self.dropout = nn.Dropout( dropout )\n",
    "        \n",
    "        # Transformerエンコーダ層\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(dim_hidden=dim_embedding, num_heads=num_heads, dim_feedforward = dim_feedforward, dropout = dropout )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(dim_embedding)\n",
    "       \n",
    "        self.pad_index = pad_index\n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x           : 入力, [バッチサイズ, 入力チャネル数, 高さ, 幅]\n",
    "    return_embed: 特徴量を返すかロジットを返すかを選択する真偽値\n",
    "    '''\n",
    "    def forward(self, src: torch.Tensor, src_mask: torch.Tensor=None, src_padding_mask: torch.Tensor=None ):\n",
    "\n",
    "        if src_padding_mask is not None and src_mask is None:\n",
    "            mask = src_padding_mask[:,None,None,:]\n",
    "            mask = mask.expand( (-1, self.num_heads, src.size(1), -1) )\n",
    "        elif src_padding_mask is not None and src_mask is not None:\n",
    "            mask1 = src_padding_mask[:,None,None,:]\n",
    "            mask1 = mask1.expand( (-1, self.num_heads, src.size(1), -1 ) )\n",
    "            mask2 = src_mask[None,None,:,:]\n",
    "            mask2 = mask2.expand( ( src.size(0), self.num_heads, -1, -1 ) )\n",
    "            mask = torch.logical_or(mask1, mask2 )\n",
    "        elif src_padding_mask is None and src_mask is not None:\n",
    "            mask = src_mask[None,None,:,:]\n",
    "            mask = mask.padding( ( src.size(0), src.size(1), -1, -1 ) )\n",
    "        else:\n",
    "            mask = None\n",
    "\n",
    "        x = self.embed( src ) * math.sqrt( self.dim_embedding )\n",
    "\n",
    "        #x = self.pos_enc( x )\n",
    "        position = self.pos_emb( x )\n",
    "        x = x + position\n",
    "        #x = self.dropout( x )\n",
    "        #x = self.norm( x )\n",
    "        \n",
    "        # Transformerエンコーダ層を適用\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer( x, mask )\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4962ca0-f955-4e9a-831a-e3b8b46edc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fTransformerEncoder(nn.Module):\n",
    "    '''\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    dim_feedforward: FNNにおける中間特徴量の次元\n",
    "    num_heads      : マルチヘッドアテンションのヘッド数\n",
    "    num_layers     : Transformerエンコーダの層数\n",
    "    '''\n",
    "    def __init__(self, text_vocab_size: int, dim_embedding: int, dim_feedforward: int,\n",
    "                 num_heads: int,  num_layers: int, pad_idx:int, dropout: float = 0.1 ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 単語埋め込み\n",
    "        #self.embed = nn.Embedding(\n",
    "        #    text_vocab_size, dim_embedding, padding_idx=pad_index)        \n",
    "        \n",
    "        # 位置エンコーディング\n",
    "        #self.pos_enc = PositionalEncoding(dim_embedding)\n",
    "        self.fpos_emb = fPositionalEmbedding(dim_embedding, \"encoder.pos_emb.pos_emb.weight\" )\n",
    "        #self.dropout = nn.Dropout( dropout )\n",
    "        \n",
    "        # Transformerエンコーダ層\n",
    "        self.fenclayer = fTransformerEncoderLayer(dim_hidden=dim_embedding, num_heads=num_heads, dim_feedforward = dim_feedforward, dropout = dropout )\n",
    "        \n",
    "        # ロジットを生成する前のレイヤー正規化と全結合\n",
    "        #self.norm = nn.LayerNorm(dim_embedding)\n",
    "        \n",
    "        self.pad_idx = pad_idx\n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x           : 入力, [バッチサイズ, 入力チャネル数, 高さ, 幅]\n",
    "    return_embed: 特徴量を返すかロジットを返すかを選択する真偽値\n",
    "    '''\n",
    "    def forward(self, src: torch.Tensor, src_mask: torch.Tensor=None, src_padding_mask: torch.Tensor=None, weights = None ):\n",
    "\n",
    "        if src_padding_mask is not None and src_mask is None:\n",
    "            mask = src_padding_mask[:,None,None,:]\n",
    "            mask = mask.expand( (-1, self.num_heads, src.size(1), -1) )\n",
    "        elif src_padding_mask is not None and src_mask is not None:\n",
    "            mask1 = src_padding_mask[:,None,None,:]\n",
    "            mask1 = mask1.expand( (-1, self.num_heads, src.size(1), -1 ) )\n",
    "            mask2 = src_mask[None,None,:,:]\n",
    "            mask2 = mask2.expand( ( src.size(0), self.num_heads, -1, -1 ) )\n",
    "            mask = torch.logical_or(mask1, mask2 )\n",
    "        elif src_padding_mask is None and src_mask is not None:\n",
    "            mask = src_mask[None,None,:,:]\n",
    "            mask = mask.padding( ( src.size(0), src.size(1), -1, -1 ) )\n",
    "        else:\n",
    "            mask = None\n",
    "\n",
    "        #x = self.embed( src ) * math.sqrt( self.dim_embedding )\n",
    "        x = F.embedding( src, weights['encoder.embed.weight'], padding_idx = self.pad_idx )  * math.sqrt( self.dim_embedding )\n",
    "        \n",
    "        #x = self.pos_enc( x )\n",
    "        position = self.fpos_emb( x, weights )\n",
    "        x = x + position\n",
    "        #x = self.dropout( x )\n",
    "        ##x = self.norm(x)\n",
    "        #x = F.layer_norm(x, (self.dim_embedding,), weight=weights['encoder.norm.weight'], bias=weights['encoder.norm.bias'], eps=1e-05)\n",
    "    \n",
    "        # Transformerエンコーダ層を適用\n",
    "        #for layer in self.encoder_layers:\n",
    "        #    x = layer( x, mask )\n",
    "        for block in range( self.num_layers ):\n",
    "            x = self.fenclayer(x, mask, block, weights )\n",
    "\n",
    "        #x = self.norm(x)\n",
    "        x = F.layer_norm(x, (self.dim_embedding,), weight=weights['encoder.norm.weight'], bias=weights['encoder.norm.bias'], eps=1e-05)\n",
    "        \n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59e7cd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    '''\n",
    "    CaptioningTransformerのコンストラクタ\n",
    "    dim_embedding  : 埋め込み次元\n",
    "    dim_feedforward: FNNの中間特徴次元\n",
    "    num_heads      : マルチヘッドアテンションのヘッド数\n",
    "    num_layers     : Transformerデコーダ層の数\n",
    "    vocab_size     : 辞書の次元\n",
    "    pad_index     : NULLのID\n",
    "    dropout        : ドロップアウト確率\n",
    "    '''\n",
    "    def __init__(self, vocab_size: int, dim_embedding: int, dim_feedforward: int,\n",
    "                 num_heads: int, num_layers: int, \n",
    "                 pad_index: int, dropout: float=0.1, ds_rate: float=0.1 ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 単語埋め込み\n",
    "        self.embed = nn.Embedding(\n",
    "            vocab_size, dim_embedding, padding_idx=pad_index)\n",
    "        \n",
    "        # 位置エンコーディング\n",
    "        #self.pos_enc = PositionalEncoding(dim_embedding)\n",
    "        self.pos_emb = PositionalEmbedding(dim_embedding)\n",
    "        #self.dropout = nn.Dropout( dropout )\n",
    "\n",
    "        # Transformerデコーダ\n",
    "        #self.decoder_layers = nn.ModuleList([\n",
    "        #    TransformerDecoderLayer(\n",
    "        #        dim_embedding, num_heads, dim_feedforward, dropout)\n",
    "        #    for _ in range(num_layers)\n",
    "        #])\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer(dim_hidden=dim_embedding, num_heads=num_heads, dim_feedforward = dim_feedforward, dropout = dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # ロジットを生成する前のレイヤー正規化と全結合\n",
    "        #self.norm = nn.LayerNorm(dim_embedding)\n",
    "        \n",
    "        # 単語出力分布計算\n",
    "        self.linear = nn.Linear(dim_embedding, vocab_size)\n",
    "        \n",
    "        self.pad_index = pad_index\n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "\n",
    "\n",
    "    ''' CaptioningTransformerの順伝播処理\n",
    "    features: 画像特徴量 [バッチサイズ, 埋め込み次元]\n",
    "    captions: 正解キャプション [バッチサイズ, 系列長]\n",
    "\n",
    "    '''\n",
    "    #def forward(self, features: torch.Tensor, caption_lengths: torch.Tensor):\n",
    "    #def forward(self, features: torch.Tensor, captions: torch.Tensor, padding_mask_src: torch.Tensor=None, \\\n",
    "    #            padding_mask_tgt: torch.Tensor=None, mask_tgt: torch.Tensor=None ):\n",
    "    def forward(self, features: torch.Tensor, captions: torch.Tensor, memory_padding_mask: torch.Tensor=None, \\\n",
    "                tgt_padding_mask: torch.Tensor=None, tgt_mask: torch.Tensor=None ):\n",
    "\n",
    "        #feature_lengths = torch.ones( (features.size(0) ), device=features.device ) * features.size(1)\n",
    "\n",
    "        tgt = captions\n",
    "        if tgt_padding_mask is not None and tgt_mask is not None:\n",
    "            self_mask1 = tgt_padding_mask[:,None,None,:]\n",
    "            self_mask1 = self_mask1.expand( (-1, self.num_heads, tgt.size(1), -1) )\n",
    "            self_mask2 = tgt_mask[None,None,:,:]\n",
    "            self_mask2 = self_mask2.expand( (tgt.size(0), self.num_heads, -1, -1 ))\n",
    "            self_mask = torch.logical_or(self_mask1, self_mask2 )\n",
    "        elif tgt_padding_mask is not None and tgt_mask is None:\n",
    "            self_mask = tgt_padding_mask[:,None,None,:]\n",
    "            self_mask = self_mask.expand( (-1, self.num_heads, tgt.size(1), -1 ) )\n",
    "        elif tgt_padding_mask is None and tgt_mask is not None:\n",
    "            self_mask = tgt_mask[None,None,:,:]\n",
    "            self_mask = self_mask.expand( (tgt.size(0), self.num_heads, -1, -1 ))\n",
    "        elif tgt_padding_mask is None and tgt_mask is None:\n",
    "            self_mask = None\n",
    "            \n",
    "        if memory_padding_mask is not None:\n",
    "            cross_mask = memory_padding_mask[:,None,None,:]\n",
    "            cross_mask = cross_mask.expand((-1,self.num_heads, tgt.size(1), -1))\n",
    "        else:\n",
    "            cross_mask = None\n",
    "        \n",
    "        # 単語埋め込み [バッチサイズ, 系列長]\n",
    "        # -> [バッチサイズ, 系列長, 埋め込み次元]\n",
    "        embeddings = self.embed(captions) * math.sqrt( self.dim_embedding )\n",
    "        seq = embeddings.shape[1]\n",
    "        \n",
    "        # 位置エンコーディング\n",
    "        #embeddings = self.pos_enc( embeddings )\n",
    "        positions = self.pos_emb(embeddings)\n",
    "        embeddings = embeddings + positions\n",
    "        #embeddings = self.dropout( embeddings )\n",
    "        #embeddings = self.norm(embeddings)\n",
    "        \n",
    "        # Transformerデコーダでキャプション生成\n",
    "        # 画像の特徴も入力する\n",
    "        for layer in self.decoder_layers:\n",
    "            embeddings = layer( embeddings, features, self_mask, cross_mask )\n",
    "\n",
    "        \n",
    "        # [バッチサイズ, 系列長, 埋め込み次元]\n",
    "        # -> [バッチサイズ, 系列長, 辞書の次元]\n",
    "        preds = self.linear(embeddings)\n",
    "        #print( \"argmax of preds:\", torch.argmax( preds, dim = 2 ))\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9722fd0-0e06-4fba-a794-562641fbbe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fTransformerDecoder(nn.Module):\n",
    "    '''\n",
    "    CaptioningTransformerのコンストラクタ\n",
    "    dim_embedding  : 埋め込み次元\n",
    "    dim_feedforward: FNNの中間特徴次元\n",
    "    num_heads      : マルチヘッドアテンションのヘッド数\n",
    "    num_layers     : Transformerデコーダ層の数\n",
    "    vocab_size     : 辞書の次元\n",
    "    pad_index     : NULLのID\n",
    "    dropout        : ドロップアウト確率\n",
    "    '''\n",
    "    def __init__(self, vocab_size: int, dim_embedding: int, dim_feedforward: int,\n",
    "                 num_heads: int, num_layers: int, \n",
    "                 pad_idx: int, dropout: float=0.1, ds_rate: float=0.1 ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 単語埋め込み\n",
    "        #self.embed = nn.Embedding(\n",
    "        #    vocab_size, dim_embedding, padding_idx=pad_index)\n",
    "        \n",
    "        # 位置エンコーディング\n",
    "        #self.pos_enc = PositionalEncoding(dim_embedding)\n",
    "        self.fpos_emb = fPositionalEmbedding(dim_embedding, \"decoder.pos_emb.pos_emb.weight\" )\n",
    "        #self.dropout = nn.Dropout( dropout )\n",
    "        \n",
    "        # Transformerデコーダ\n",
    "        self.fdeclayer = fTransformerDecoderLayer(dim_hidden=dim_embedding, num_heads=num_heads, dim_feedforward = dim_feedforward, dropout = dropout )\n",
    "        \n",
    "        # 単語出力分布計算\n",
    "        #self.linear = nn.Linear(dim_embedding, vocab_size)\n",
    "        \n",
    "        self.pad_idx = pad_idx\n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    ''' CaptioningTransformerの順伝播処理\n",
    "    features: 画像特徴量 [バッチサイズ, 埋め込み次元]\n",
    "    captions: 正解キャプション [バッチサイズ, 系列長]\n",
    "\n",
    "    '''\n",
    "    def forward(self, features: torch.Tensor, captions: torch.Tensor, memory_padding_mask: torch.Tensor=None, \\\n",
    "                tgt_padding_mask: torch.Tensor=None, tgt_mask: torch.Tensor=None, weights = None ):\n",
    "\n",
    "        tgt = captions\n",
    "        if tgt_padding_mask is not None and tgt_mask is not None:\n",
    "            self_mask1 = tgt_padding_mask[:,None,None,:]\n",
    "            self_mask1 = self_mask1.expand( (-1, self.num_heads, tgt.size(1), -1) )\n",
    "            self_mask2 = tgt_mask[None,None,:,:]\n",
    "            self_mask2 = self_mask2.expand( (tgt.size(0), self.num_heads, -1, -1 ))\n",
    "            self_mask = torch.logical_or(self_mask1, self_mask2 )\n",
    "        elif tgt_padding_mask is not None and tgt_mask is None:\n",
    "            self_mask = tgt_padding_mask[:,None,None,:]\n",
    "            self_mask = self_mask.expand( (-1, self.num_heads, tgt.size(1), -1 ) )\n",
    "        elif tgt_padding_mask is None and tgt_mask is not None:\n",
    "            self_mask = tgt_mask[None,None,:,:]\n",
    "            self_mask = self_mask.expand( (tgt.size(0), self.num_heads, -1, -1 ))\n",
    "        elif tgt_padding_mask is None and tgt_mask is None:\n",
    "            self_mask = None\n",
    "            \n",
    "        if memory_padding_mask is not None:\n",
    "            cross_mask = memory_padding_mask[:,None,None,:]\n",
    "            cross_mask = cross_mask.expand((-1,self.num_heads, tgt.size(1), -1))\n",
    "        else:\n",
    "            cross_mask = None\n",
    "        \n",
    "        # 単語埋め込み [バッチサイズ, 系列長]\n",
    "        # -> [バッチサイズ, 系列長, 埋め込み次元]\n",
    "        embeddings = F.embedding( captions, weights['decoder.embed.weight'], padding_idx = self.pad_idx )  * math.sqrt( self.dim_embedding )\n",
    "        seq = embeddings.shape[1]\n",
    "        \n",
    "        # 位置エンコーディング\n",
    "        #embeddings = self.pos_enc(embeddings)\n",
    "        positions = self.fpos_emb( embeddings, weights )\n",
    "        embeddings = embeddings + positions\n",
    "        #embeddings = self.dropout( embeddings )\n",
    "        ##embeddings = self.norm(embeddings)\n",
    "        #embeddings = F.layer_norm(embeddings, (self.dim_embedding,), weight=weights['decoder.norm.weight'], bias=weights['decoder.norm.bias'], eps=1e-05)\n",
    "        \n",
    "        # Transformerデコーダでキャプション生成\n",
    "        # 画像の特徴も入力する\n",
    "        #for layer in self.decoder_layers:\n",
    "        #    #embeddings = layer( embeddings, features, tgt_key_padding_mask = padding_mask_tgt, \\\n",
    "        #    #                                memory_key_padding_mask = padding_mask_src, tgt_is_causal = True, tgt_mask = mask_tgt )\n",
    "        for block in range( self.num_layers ):\n",
    "            embeddings = self.fdeclayer(embeddings, features, self_mask, cross_mask, block, weights )\n",
    "  \n",
    "        # [バッチサイズ, 系列長, 埋め込み次元]\n",
    "        # -> [バッチサイズ, 系列長, 辞書の次元]\n",
    "        #preds = self.linear(embeddings)\n",
    "        preds = F.linear( embeddings, weights['decoder.linear.weight'], weights['decoder.linear.bias'] )\n",
    "        #print( \"argmax of preds:\", torch.argmax( preds, dim = 2 ))\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c47496bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim_embedding: int, dim_feedforward: int,\n",
    "                 num_heads: int, num_layers: int, enc_vocab_size: int, dec_vocab_size: int,\n",
    "                 j_pad_index: int,e_pad_index: int, dropout: float=0.1, ds_rate: float=0.1 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = TransformerEncoder(enc_vocab_size, dim_embedding, dim_feedforward, num_heads, num_layers, j_pad_index, dropout )\n",
    "        self.decoder = TransformerDecoder(dec_vocab_size, dim_embedding, dim_feedforward, num_heads, num_layers, e_pad_index, dropout )\n",
    "        self.fencoder = fTransformerEncoder(enc_vocab_size, dim_embedding, dim_feedforward, num_heads, num_layers, j_pad_index, dropout )\n",
    "        self.fdecoder = fTransformerDecoder(dec_vocab_size, dim_embedding, dim_feedforward, num_heads, num_layers, e_pad_index, dropout )\n",
    "        self.j_pad_index = j_pad_index\n",
    "        self.e_pad_index = e_pad_index\n",
    "\n",
    "        self._reset_parameters()\n",
    "        #self._reset_parameters2()\n",
    "    \n",
    "    def _reset_parameters(self):\n",
    "        r\"\"\"Initiate parameters in the transformer model.\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                xavier_uniform_(p)\n",
    "                #xavier_normal_(p)\n",
    "                #kaiming_uniform_(p)\n",
    "                #kaiming_normal_(p)\n",
    "\n",
    "    #def _reset_parameters2(self):\n",
    "    #    for module in self.modules():\n",
    "    #        if isinstance(module, nn.Linear):\n",
    "    #            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    #            if module.bias is not None:\n",
    "    #                nn.init.zeros_(module.bias)\n",
    "    #        elif isinstance(module, nn.Embedding):\n",
    "    #            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    #        elif isinstance(module, nn.LayerNorm):\n",
    "    #            nn.init.zeros_(module.bias)\n",
    "    #            nn.init.ones_(module.weight)\n",
    "        \n",
    "    ''' CaptioningTransformerの順伝播処理\n",
    "    features: 画像特徴量 [バッチサイズ, 埋め込み次元]\n",
    "    captions: 正解キャプション [バッチサイズ, 系列長]\n",
    "\n",
    "    '''\n",
    "    def forward(self, text, dec_input):\n",
    "\n",
    "        seq_len_src = text.shape[1]\n",
    "        seq_len_tgt = dec_input.shape[1]\n",
    "\n",
    "        mask_tgt = nn.Transformer.generate_square_subsequent_mask( seq_len_tgt, dtype=bool ).to(device)\n",
    "        mask_src = torch.zeros((seq_len_src, seq_len_src), device=device).type(torch.bool)\n",
    "\n",
    "        padding_mask_src = (text == idx_list['<pad>'])\n",
    "        padding_mask_tgt = (dec_input == idx_list_en['<pad>'])\n",
    "    \n",
    "        x = self.encoder( text, mask_src, padding_mask_src )\n",
    "        preds = self.decoder(x,dec_input, padding_mask_src, padding_mask_tgt, mask_tgt )\n",
    "\n",
    "        return preds\n",
    "    \n",
    "    def adaptation(self, text, dec_input, weights):\n",
    "\n",
    "        seq_len_src = text.shape[1]\n",
    "        seq_len_tgt = dec_input.shape[1]\n",
    "\n",
    "        mask_tgt = nn.Transformer.generate_square_subsequent_mask( seq_len_tgt, dtype=bool ).to(device)\n",
    "        mask_src = torch.zeros((seq_len_src, seq_len_src), device=device).type(torch.bool)\n",
    "\n",
    "        padding_mask_src = (text == idx_list['<pad>']).to(text.device )\n",
    "        padding_mask_tgt = (dec_input == idx_list_en['<pad>']).to(text.device )\n",
    "        \n",
    "        x = self.fencoder( text, mask_src, padding_mask_src, weights)\n",
    "        preds = self.fdecoder( x, dec_input, padding_mask_src, padding_mask_tgt, mask_tgt, weights )\n",
    "        \n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99b82dac-b490-49c2-a099-52b962fe0ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def inference(input_sequence: torch.Tensor, weights, max_length: int=30):\n",
    "\n",
    "    bs = input_sequence.size(0)\n",
    "    \n",
    "    seq_len_src = input_sequence.shape[1]\n",
    "\n",
    "    padding_mask_src = (input_sequence == idx_list['<pad>'])\n",
    "\n",
    "    #model.eval()\n",
    "    enc_out = model.fencoder( input_sequence, None, padding_mask_src, weights )\n",
    "    \n",
    "    # <start> トークンで出力キャプションを初期化\n",
    "    captions = input_sequence.new_full(\n",
    "        (bs, 1), idx_list_en['<sos>'], dtype=torch.int64)\n",
    "\n",
    "    #print( \"enc_out:\", enc_out )\n",
    "    # 単語を逐次予測\n",
    "    for _ in range(max_length):\n",
    "        mask_tgt = nn.Transformer.generate_square_subsequent_mask( captions.size(1), dtype=bool ).to(device)    \n",
    "        padding_mask_tgt = (captions == idx_list_en['<pad>']).bool()\n",
    "        #print( \"captions:\",captions)\n",
    "        #preds = model.decoder(enc_out, captions, causal_mask = None, dec_padding_mask = dec_padding_mask  )\n",
    "        #model.eval()\n",
    "        #preds = model.fdecoder(enc_out, captions, padding_mask_src, padding_mask_tgt, mask_tgt, weights )    \n",
    "        #preds = model.fdecoder(enc_out, captions, padding_mask_src, tgt_padding_mask=padding_mask_tgt, tgt_mask = None, weights = weights )\n",
    "        preds = model.fdecoder(enc_out, captions, padding_mask_src, tgt_padding_mask=padding_mask_tgt, tgt_mask = mask_tgt, weights = weights )\n",
    "        #preds = model.decoder(enc_out, captions, causal_mask = None, dec_padding_mask = None  )\n",
    "        preds = preds[:, -1]\n",
    "        preds = preds.argmax(dim=1)\n",
    "        #print(\"preds:\",preds)\n",
    "        words = torch.unsqueeze( preds, dim = 1 )\n",
    "        \n",
    "        captions = torch.cat((captions, words), dim=1)\n",
    "\n",
    "    return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49856487-be45-4f42-8a72-33a95ab3577f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as tud\n",
    "\n",
    "# 推論モジュール\n",
    "@torch.no_grad()\n",
    "def beam_search(input_sequence, weights):\n",
    "    ''' ネットワーク計算(forward処理)の関数\n",
    "    input_sequence: 各発話の入力系列 [B x Tin x D]\n",
    "    input_lengths:  各発話の系列長(フレーム数) [B]\n",
    "        []の中はテンソルのサイズ\n",
    "        B:    ミニバッチ内の発話数(ミニバッチサイズ)\n",
    "        Tin:  入力テンソルの系列長(ゼロ埋め部分含む)\n",
    "        D:    入力次元数(dim_in)\n",
    "        Tout: 正解ラベル系列の系列長(ゼロ埋め部分含む)\n",
    "    '''\n",
    "    predictions = 20\n",
    "    beam_width = 5\n",
    "    batch_size = input_sequence.size(0)\n",
    "        \n",
    "    enc_mask = input_sequence.eq(idx_list['<pad>']).to(device)\n",
    "    dec_mask = enc_mask\n",
    "        \n",
    "    # エンコーダに入力する\n",
    "    #model.eval()\n",
    "    enc_out = model.fencoder(input_sequence, None, enc_mask, weights)\n",
    "        \n",
    "    #enc_lengths = input_lengths\n",
    "        \n",
    "    X = torch.ones( (enc_out.size(0), 1 ), dtype=torch.int64  ) * idx_list_en['<sos>']\n",
    "\n",
    "    dec_target_mask = X.eq(idx_list_en['<pad>']).to(device)\n",
    "    n_ctx = X.size(1)\n",
    "    causal_mask = torch.empty(n_ctx, n_ctx).fill_(1).triu_(1).to(device).bool()\n",
    "\n",
    "    #dec_out= model.decoder( enc_out, X, dec_mask, dec_target_mask, causal_mask )\n",
    "    #preds = model.decoder(enc_out, captions, padding_mask_src, None, mask_tgt )   \n",
    "    #logits = self.classifier( dec_out )\n",
    "    #model.eval()\n",
    "    logits= model.fdecoder( enc_out, X, dec_mask, dec_target_mask, causal_mask, weights )\n",
    "    next_probabilities = logits[:, -1, :]\n",
    "    vocabulary_size = next_probabilities.shape[-1]\n",
    "    probabilities, idx = next_probabilities.squeeze().log_softmax(-1)\\\n",
    "        .topk(k = beam_width, axis = -1)\n",
    "    X = X.repeat((beam_width, 1, 1)).transpose(0, 1)\\\n",
    "        .flatten(end_dim = -2)\n",
    "    next_chars = idx.reshape(-1, 1)\n",
    "    X = torch.cat((X, next_chars), axis = -1)\n",
    "    # This has to be minus one because we already produced a round\n",
    "    # of predictions before the for loop.\n",
    "    predictions_iterator = range(predictions - 1)\n",
    "\n",
    "    for i in predictions_iterator:\n",
    "        dataset = tud.TensorDataset(X)\n",
    "        loader = tud.DataLoader(dataset, batch_size = batch_size)\n",
    "        next_probabilities = []\n",
    "        iterator = iter(loader)\n",
    "        for (x,) in iterator:\n",
    "            dec_target_mask = x.eq(idx_list_en['<pad>']).to(device)\n",
    "            n_ctx = x.size(1)\n",
    "            causal_mask = torch.empty(n_ctx, n_ctx).fill_(1).triu_(1).to(device).bool()\n",
    "            #dec_out= model.decoder( enc_out, x, dec_mask, dec_target_mask, causal_mask )\n",
    "            #logits = self.classifier( dec_out )\n",
    "            #model.eval()\n",
    "            #logits = model.fdecoder( enc_out, x, dec_mask, dec_target_mask, causal_mask, weights )\n",
    "            logits = model.fdecoder(enc_out, captions, padding_mask_src, tgt_padding_mask=dec_target_mask, tgt_mask = causal_mask, weights = weights )\n",
    "            next_probabilities0 = logits[:, -1, :].log_softmax(-1)\n",
    "            next_probabilities.append(\n",
    "                next_probabilities0\n",
    "            )\n",
    "        next_probabilities = torch.cat(next_probabilities, axis = 0)\n",
    "        next_probabilities = next_probabilities.reshape(\n",
    "            (-1, beam_width, next_probabilities.shape[-1])\n",
    "        )\n",
    "        probabilities = probabilities.unsqueeze(-1) + next_probabilities\n",
    "        probabilities = probabilities.flatten(start_dim = 1)\n",
    "        probabilities, idx = probabilities.topk(\n",
    "            k = beam_width, \n",
    "            axis = -1\n",
    "        )\n",
    "        next_chars = torch.remainder(idx, vocabulary_size).flatten()\\\n",
    "            .unsqueeze(-1)\n",
    "        best_candidates = (idx / vocabulary_size).long()\n",
    "        best_candidates += torch.arange(\n",
    "            X.shape[0] // beam_width, \n",
    "            device = X.deviceoptimizer\n",
    "        ).unsqueeze(-1) * beam_width\n",
    "        X = X[best_candidates].flatten(end_dim = -2)\n",
    "        X = torch.cat((X, next_chars), axis = 1)\n",
    "        \n",
    "        #print( \"size of X:{}\", X.size() )\n",
    "        \n",
    "    output = X.reshape( input_sequence.size(0), beam_width, -1 )\n",
    "    return output[:,0,:]\n",
    "    #return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8019b3a4-8e5c-4777-bda5-5d005b990bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuning(nn.Module):\n",
    "    def __init__(self, model, loss_fn, device, id_to_word ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.device = device\n",
    "        self.id_to_word = id_to_word\n",
    "    \n",
    "    def forward(self, batch, train_step, lr1, display ):\n",
    "\n",
    "        x_train = torch.tensor( np.array( batch[0] ) ) #support tokeinzer.encode text データ\n",
    "        x_len_train = torch.tensor( batch[1] ) #support attention mask データ\n",
    "        y_train = torch.tensor( np.array( batch[2] ) ) #support ラベルデータ\n",
    "        y_len_train = torch.tensor( batch[3] ) #support attention mask データ\n",
    "        x_val = torch.tensor( np.array( batch[4] ) )   #query  tokeinzer.encode text データ\n",
    "        x_len_val = torch.tensor( batch[5] )   #query attention mask データ\n",
    "        y_val = torch.tensor( np.array( batch[6] )  )  #query ラベルデータ\n",
    "        y_len_val = torch.tensor( batch[7] )   #query attention mask データ\n",
    "\n",
    "        num_task = len( x_train )\n",
    "\n",
    "        outer_loss = 0\n",
    "        total_error = 0\n",
    "        total_token_length = 0\n",
    "\n",
    "        for idx in range(x_train.size(0)): # task\n",
    "            fast_weights = OrderedDict(self.model.named_parameters())#今回の基準パラメータ, \n",
    "                                # for 文の中だが、fast_weightsは更新されるが、model.parameter は変わらない。\n",
    "            # batch 抽出\n",
    "            input_x_len = x_len_train[idx]\n",
    "            input_y_len = y_len_train[idx]        \n",
    "            input_x = x_train[idx][:,:max(input_x_len)].to(self.device)\n",
    "            input_y = y_train[idx][:,:max(input_y_len)].to(self.device).long()\n",
    "            input_y1 = input_y[:,:-1]\n",
    "            input_y2 = input_y[:,1:]\n",
    "\n",
    "            if display:\n",
    "                print('----Task',idx, '----')\n",
    "\n",
    "            # タスクごとの損失の計算\n",
    "            loss_item = 0\n",
    "            for iter in range(train_step): # train_step のループ\n",
    "                # 未来のキャプションを参照しないようにマスク行列を生成\n",
    "                self.model.eval()\n",
    "                logits = self.model.adaptation(input_x, input_y1, fast_weights )\n",
    "                loss0 = self.loss_fn(logits.transpose(1,2), input_y2  )\n",
    "                loss_item += loss0.item()\n",
    "                gradients = torch.autograd.grad(loss0, fast_weights.values())\n",
    "                fast_weights = OrderedDict((name, param - lr1 * grad) for ((name, param),  grad) in zip(fast_weights.items(), gradients))\n",
    "\n",
    "                del logits, gradients, loss0\n",
    "                torch.cuda.empty_cache()  \n",
    "\n",
    "            loss_item = loss_item / train_step \n",
    "\n",
    "            if display:\n",
    "                print(\"Inner Loss: \", loss_item)\n",
    "\n",
    "            del loss_item, input_x, input_y, input_y1, input_y2, input_x_len, input_y_len\n",
    "            torch.cuda.empty_cache()  \n",
    "        \n",
    "            # query データからバッチ抽出\n",
    "            input_x_len = x_len_val[idx]\n",
    "            input_y_len = y_len_val[idx]        \n",
    "            input_x = x_val[idx][:,:max(input_x_len)].to(self.device)\n",
    "            input_y = y_val[idx][:,:max(input_y_len)].to(self.device).long()\n",
    "        \n",
    "            # 訓練時に query データ（inner_batch = 1, query_k * 2 クラス )　で二番目の損失関数の各タスクについての総和を求める。\n",
    "            #if train:\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                self.model.eval()\n",
    "                preds = inference(input_x, fast_weights )\n",
    "                #preds = beam_search( input_x, fast_weights )\n",
    "\n",
    "            # 更新したモデルパラメーターを用いて、損失と精度を求める。\n",
    "\n",
    "            pred_label_id = preds\n",
    "            #print( \"pred_label_id[0]:\", pred_label_id[0] )\n",
    "            predict = []\n",
    "            target = []\n",
    "            for i0, (pred, orig) in enumerate( zip( pred_label_id, input_y )):\n",
    "                hypo = []\n",
    "                for m in pred:\n",
    "                    hypo.append(self.id_to_word[m.item()])\n",
    "                    if self.id_to_word[m.item()] == '<eos>':\n",
    "                        break\n",
    "                predict.append( hypo )\n",
    "                reference = []\n",
    "                for m in orig:\n",
    "                    reference.append(self.id_to_word[m.item()])\n",
    "                    if self.id_to_word[m.item()] == '<eos>':\n",
    "                        break\n",
    "                target.append( reference )    \n",
    "                   # 認識誤りを計算\n",
    "                (error, substitute, \n",
    "                    delete, insert, ref_length) = \\\n",
    "                       levenshtein.calculate_error(hypo,\n",
    "                                               reference)\n",
    "                    \n",
    "                # 誤り文字数を累積する\n",
    "                total_error += error\n",
    "                # 文字の総数を累積する\n",
    "                total_token_length += ref_length                \n",
    "            if True:\n",
    "                #print( \"pred_label_id:\", pred_label_id )\n",
    "                for i4, (pred, tar) in enumerate( zip( predict, target ) ):\n",
    "                    if i4 >= 2:\n",
    "                        break\n",
    "                    if display:\n",
    "                        print( \"test pred:\", ' '.join(pred) )\n",
    "                        print( \"test targ:\", ' '.join(tar)  )\n",
    "\n",
    "            del input_x, input_y, input_x_len, input_y_len, fast_weights\n",
    "            torch.cuda.empty_cache()  \n",
    "        \n",
    "        avg_error = total_error / total_token_length\n",
    "\n",
    "        return avg_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88539768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device( \"cpu\")\n",
    "epoch_num = 3\n",
    "dim_hidden = 768\n",
    "dim_feedforward = dim_hidden * 4\n",
    "heads = 12\n",
    "layers =8\n",
    "dropout = 0.1\n",
    "derivative_change_rate = 0.01\n",
    "#dim_hidden = 256\n",
    "#dim_feedforward = dim_hidden * 4\n",
    "#heads = 4\n",
    "#layers =4\n",
    "#dropout = 0.0\n",
    "model = Transformer(dim_hidden, dim_feedforward, heads, layers, enc_vocab_size, dec_vocab_size, idx_list['<pad>'], idx_list_en['<pad>'], dropout = dropout ).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=idx_list_en['<pad>'])\n",
    "loss_fn = criterion\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "#optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "fine_tuning = FineTuning( model = model, loss_fn = loss_fn, device = device, id_to_word = token_list_en )\n",
    "# 全ステップ数\n",
    "#num_global_steps = len( ob_train ) * epoch_num\n",
    "#print( \"num_global_steps:\", num_global_steps )\n",
    "#num_warmup_steps = num_global_steps * 0.1\n",
    "#print( \"num_warmup_steps:\", num_warmup_steps )\n",
    "#derivative_change_step = num_global_steps * derivative_change_rate\n",
    "#print( \"derivative_change_step:\", derivative_change_step )\n",
    "#スケジューラーの定義\n",
    "#scheduler = get_linear_schedule_with_warmup( optimizer, num_warmup_steps, num_global_steps )\n",
    "#use_param_file = False\n",
    "PATH = \"./model_NTT_auto_curr3.pt\"\n",
    "print_coef = 1000\n",
    "save_file_coef = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "763ea208-7ed2-4b7b-9ccf-0228c3aa86b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs1 size: torch.Size([8, 120, 20556])\n",
      "tensor(0.4916, grad_fn=<SelectBackward0>)\n",
      "outputs2 size: torch.Size([8, 120, 20556])\n",
      "tensor(0.4916, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#seed = 0\n",
    "\n",
    "#random.seed(seed)\n",
    "#np.random.seed(seed)\n",
    "#torch.manual_seed(seed)\n",
    "#torch.backends.cudnn.benchmark = False\n",
    "#torch.backends.cudnn.deterministic = True\n",
    "\n",
    "text = torch.randint( 0, enc_vocab_size, size=(8, 100 ))\n",
    "dec_input = torch.randint( 0, dec_vocab_size, size=(1,120))\n",
    "\n",
    "seq_len_src = text.shape[1]\n",
    "seq_len_tgt = dec_input.shape[1]\n",
    "\n",
    "mask_tgt = nn.Transformer.generate_square_subsequent_mask( seq_len_tgt, dtype=bool ).to(device)\n",
    "mask_src = torch.zeros((seq_len_src, seq_len_src), device=device).type(torch.bool)\n",
    "\n",
    "padding_mask_src = (text == idx_list['<pad>'])\n",
    "padding_mask_tgt = (dec_input == idx_list_en['<pad>'])\n",
    "\n",
    "weights = OrderedDict(model.named_parameters())\n",
    "\n",
    "model.eval()\n",
    "outputs1 = model( text.to(device), dec_input.to(device) )\n",
    "#outputs1 = model.encoder( text.to(device), None, None )\n",
    "#outputs1 = model.encoder( text.to(device), dec_input None, None )\n",
    "print( \"outputs1 size:\", outputs1.size())\n",
    "print( outputs1[0][0][0])\n",
    "\n",
    "#for name in weights:\n",
    "#    print( name )\n",
    "model.eval()\n",
    "outputs2 = model.adaptation( text.to(device), dec_input.to(device), weights )\n",
    "#outputs2 = model.fencoder( text.to(device), None, None,  weights )\n",
    "print( \"outputs2 size:\", outputs2.size() )\n",
    "print( outputs2[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f385ca3d-029f-40b0-82a6-48d6ae461f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print( device )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0f4215f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの読み込み\n",
    "\n",
    "#if torch.cuda.is_available():\n",
    "if device != torch.device(\"cpu\"):\n",
    "    checkpoint = torch.load(PATH)\n",
    "else:\n",
    "    checkpoint = torch.load(PATH, map_location=torch.device('cpu'))\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device( \"cpu\")\n",
    "\n",
    "# optimizerのstateを現在のdeviceに移す。これをしないと、保存前後でdeviceの不整合が起こる可能性がある。\n",
    "#for state in optimizer.state.values():\n",
    "#    for k, v in state.items():\n",
    "#        if isinstance(v, torch.Tensor):\n",
    "#            state[k] = v.to(device)\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with open('history_train_NTT_auto_curr3.pkl', 'rb') as f:\n",
    "    history_train = pickle.load(f)\n",
    "\n",
    "with open('history_val_NTT_auto_curr3.pkl', 'rb') as f:\n",
    "    history_val = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f7db97d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#x = []\n",
    "#print( len( history[\"train_loss\"] ))\n",
    "#for i in range( len( history[\"train_loss\"] )):\n",
    "#    x.append( i )\n",
    "    \n",
    "#v_x = []\n",
    "#print( len( history[\"val_loss\"]) )\n",
    "#for i in range( len( history[\"val_loss\"]) ):\n",
    "#    v_x.append( (i )  * 100 )\n",
    "\n",
    "#print( history_train )\n",
    "#print( history_val )\n",
    "\n",
    "for i, step in enumerate( history_val[\"step\"] ):\n",
    "    step = step\n",
    "    history_val[\"step\"][i] = step\n",
    "\n",
    "plt.title( \"Loss\")\n",
    "plt.xlabel( \"Epochs\")\n",
    "plt.ylabel( \"Loss\")\n",
    "plt.plot( history_train[\"step\"], history_train[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot( history_val[\"step\"], history_val[\"val_loss\"], label=\"val_loss\")\n",
    "plt.ylim( 1, 5 )\n",
    "plt.xlim( 194000, 210000)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.title( \"WER\")\n",
    "plt.xlabel( \"Epochs\")\n",
    "plt.ylabel( \"WER\")\n",
    "plt.plot( history_train[\"step\"], history_train[\"train_wer\"], label=\"train_wer\")\n",
    "plt.plot( history_val[\"step\"], history_val[\"val_wer\"], label=\"val_wer\")\n",
    "plt.ylim( 0.3, 1.0 )\n",
    "plt.xlim( 194000, 210000)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c9e1f36d-0457-41ad-9c4b-e41b638f18f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.770816969871521\n",
      "test pred: <sos> in the eu we need a consensus on the prevention of strategies that threaten rational land use and the environment <eos>\n",
      "test targ: <sos> a consensus is necessary in the eu in order to avoid policies that threaten rational european planning and the environment <eos>\n",
      "test pred: <sos> however when the commission is constantly introducing new legislation such a policy is created in silence which is unacceptable <eos>\n",
      "test targ: <sos> if the commission then repeatedly introduces new monitoring tools into this area a policy is created in secrecy and that is unacceptable <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.548428225517273\n",
      "test pred: <sos> mr posselt we share your concern that the human rights situation and fundamental freedoms in the russian and russian spheres of activity are a concern of ours and that is\n",
      "test targ: <sos> mr posselt we do share the concern evident in your question on the position concerning human rights and fundamental freedoms in the northern caucasus and russia as a whole and for that reason too we are keeping developments under continual observation <eos>\n",
      "test pred: <sos> i agree with the importance of the social dialogue and the tripartite summit and that is why we have put this item on the agenda for yesterday's council meetings <eos>\n",
      "test targ: <sos> i agree completely that the social dialogue and the tripartite summit are important which is why we scheduled these matters for discussion over lunch at yesterday's council meeting where it was agreed that we should establish a tripartite social dialogue summit <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.1148797750473023\n",
      "test pred: <sos> this sentence does not contain anything new but it is essential for the reason that it gives the inspectors credibility and calls on the regime in baghdad to open its\n",
      "test targ: <sos> this sentence does not contain anything new but it is essential because it gives credibility to the inspectors urging the baghdad regime to open its doors to them without concealing anything so that it could avoid the situation subsequently deteriorating still further <eos>\n",
      "test pred: <sos> i therefore urge you to accept this request as a matter of urgency <eos>\n",
      "test targ: <sos> i would therefore make an urgent appeal to you now to honour the request <eos>\n",
      "Step:1  WER:0.6122448980\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.3276723861694335\n",
      "test pred: <sos> combating piracy in the field of organs <eos>\n",
      "test targ: <sos> combating piracy in the criminal field <eos>\n",
      "test pred: <sos> the most effective way of combating this problem is close cooperation between the administrative authorities in the member states <eos>\n",
      "test targ: <sos> the most effective way of preventing this type of fraud is through close administrative cooperation between the member states <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.1827503204345704\n",
      "test pred: <sos> to accept community cofinancing for a longer period would significantly reduce the availability of community funds for cofinancing of the <unk> which is clearly the overriding objective in this case\n",
      "test targ: <sos> to accept community co-financing for a longer period would significantly reduce the availability of community funds for co-financing of scrapping which is clearly the priority objective in this case <eos>\n",
      "test pred: <sos> at the beginning of the parliamentary discussion it was already clear that only cooperation with the council and the commission would make a proposal that is broadly approved <eos>\n",
      "test targ: <sos> at the start of the parliamentary discussion it was already clear that only cooperation with the council and the commission could secure the proposal wide support <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.3694837808609008\n",
      "test pred: <sos> good governance is so essential as is stability <eos>\n",
      "test targ: <sos> good governance is just as vital as stability <eos>\n",
      "test pred: <sos> will the democratic parties of europe which in future will be the <unk> <unk> will only be funded and only if they are enshrined in the founding values of europe?\n",
      "test targ: <sos> will the european democratic parties which in future appoint themselves as european parties only be supported if they stand for the values on which europe is <unk> <eos>\n",
      "Step:2  WER:0.5952890792\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.6813714504241943\n",
      "test pred: <sos> i am thinking in particular of the european added value that will give us the opportunity to strengthen the exchange of students at the level of the european union particularly\n",
      "test targ: <sos> i am thinking in particular of the european added value which we will find if there is more interaction between the countries of the european union namely through grants provided to young people so that they can go and carry out research and work in other member states and discover more about themselves <eos>\n",
      "test pred: <sos> we are trying to find a response to problems of older people and problems in healthcare with wonderful political proposals and proposals <eos>\n",
      "test targ: <sos> by means of fine policy proposals and election rhetoric we try to address the problems of the elderly in health care <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.1322855949401855\n",
      "test pred: <sos> the latter is of course based on the fundamental value of <unk> the human being <eos>\n",
      "test targ: <sos> the latter is of course based on the value of the fundamental entity namely the human being <eos>\n",
      "test pred: <sos> the report proposes a working group to which representatives of civil society should be involved <eos>\n",
      "test targ: <sos> the report proposes a working group in which representatives of civil society should be involved <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.8558172464370728\n",
      "test pred: <sos> if we fully accept this we will have got the right way <eos>\n",
      "test targ: <sos> we will be on the right track if we grasp that fact <eos>\n",
      "test pred: <sos> however the evidence of the political agenda should have been the legislative programme with the political objectives to be implemented and that is what is lacking <eos>\n",
      "test targ: <sos> however the political programme needs to be underpinned by the legislative programme required to implement the political objectives and that is what is missing <eos>\n",
      "Step:3  WER:0.6604278075\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.7594622135162354\n",
      "test pred: <sos> that is why we are increasingly convinced that immigration must be one of the union’s competences in the future <eos>\n",
      "test targ: <sos> this is why we are increasingly convinced that immigration ought to be one of the european union’s responsibilities in the future <eos>\n",
      "test pred: <sos> the prospect of membership alone will promote the development of democracy human rights and a better life for citizens <eos>\n",
      "test targ: <sos> the very perspective of becoming a member state encourages the development of democracy human rights and a better life for citizens <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.6194230318069458\n",
      "test pred: <sos> as a member of the committee on constitutional affairs i have to say on behalf of lord inglewood that a period of almost one year is enough for the powers\n",
      "test targ: <sos> as a member of the constitutional affairs committee speaking on behalf of lord inglewood i must say that it seems very nearly a year is sufficient time for the powers that be to have considered this matter <eos>\n",
      "test pred: <sos> finally my group is firmly convinced – mr duff will return to this in a moment – that the union and its institutions must put their house in order before\n",
      "test targ: <sos> finally my group feels very strongly - and mr duff will expand on this in a moment - that the union and its institutions should put its own house in order even before the next enlargement round gets underway <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  2.5208227157592775\n",
      "test pred: <sos> it does not seem appropriate to extend the health insurance system to former members and my group is not in favour of this initiative <eos>\n",
      "test targ: <sos> extending the health insurance scheme to apply to former members does not seem justified and my group is not in favour of this <eos>\n",
      "test pred: <sos> however considerable progress has been made at the conference on technical matters in particular on the mechanisms for ensuring clean development reporting and auditing <eos>\n",
      "test targ: <sos> significant progress on technical matters was made at the <unk> however these related in particular to mechanisms for clean development reporting and review procedures <eos>\n",
      "Step:4  WER:0.5509761388\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.792548155784607\n",
      "test pred: <sos> what are human rights in <unk> <eos>\n",
      "test targ: <sos> what are human rights in russia? <eos>\n",
      "test pred: <sos> the member states agree that the threat of the spread of nuclear weapons <unk> they agree with the need for the real and complete disarmament of iraq they share the\n",
      "test targ: <sos> member states agree about the threat of proliferation of weapons of mass <unk> they agree on the need for full and effective disarmament of <unk> they agree that the united nations must remain at the centre of the international order; they agree indeed that force should only be used as a last resort <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.9577941298484802\n",
      "test pred: <sos> 4 <eos>\n",
      "test targ: <sos> 4 <eos>\n",
      "test pred: <sos> madam president mr president-in-office of the council mr vice-president of the commission i have here the draft voting list for the wortmann-kool report on the taxation of heavy goods vehicles\n",
      "test targ: <sos> madam president mr president-in-office of the council mr vice-president of the commission i have here the draft voting list for the wortmann-kool report on the charging of heavy goods vehicles <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.2362596988677979\n",
      "test pred: <sos> the next item is the report <unk> by mr casaca on behalf of the committee on budgetary control on the proposal for a council regulation amending regulation (eec) no <unk>\n",
      "test targ: <sos> the next item is the report by mr casaca <unk> on behalf of the committee on budgetary control on the proposal for a council regulation on amending regulation (eec) <unk> establishing an integrated administration and control system for certain community aid schemes <unk> <unk> - <unk> - <unk> <unk> <eos>\n",
      "test pred: <sos> this increase has mainly benefited the road haulage industry particularly as regards freight transport <eos>\n",
      "test targ: <sos> this increase has mainly benefited the road sector particular as regards freight movement <eos>\n",
      "Step:5  WER:0.5095367847\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.0477968454360962\n",
      "test pred: <sos> for this reason finland will be organising an extraordinary meeting of the heads of state or government in lahti to discuss these issues <eos>\n",
      "test targ: <sos> finland will therefore host an extraordinary social summit just prior to the lahti meeting of heads of state and government where these issues will be discussed <eos>\n",
      "test pred: <sos> there is a concern that the existing restrictions introduced by the european commission are causing high costs and great harm to the environment <eos>\n",
      "test targ: <sos> there is cross-party concern that the current restrictions imposed by the european commission are disproportionately costly and disruptive <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  2.0688626289367678\n",
      "test pred: <sos> the response to this is certainly <unk> to improving coordination and rapid alert mechanisms as well as to the storage of adequate supplies and equipment at international level <eos>\n",
      "test targ: <sos> the readiness and ability to react will certainly follow on from improvements in coordination and early warning mechanisms as well as stores of appropriate materials and reserves on the international level <eos>\n",
      "test pred: <sos> there are clearly lessons to be learnt but we must not forget that india has recently been wiped out by a series of horrific acts of terrorism by the extremists\n",
      "test targ: <sos> clearly lessons have to be learned but we should not forget that india is currently <unk> from a series of atrocious terrorist attacks by islamic <unk> extremists - most recently the killing of 30 people including women and children and scores of wounded on the outskirts of jammu <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.5110275030136109\n",
      "test pred: <sos> thirdly there are no areas in which pollution from maritime or nuclear contamination is considered as the argument that international conventions exist in this respect <eos>\n",
      "test targ: <sos> thirdly oil pollution and nuclear damage are not covered with the argument that they are covered by international conventions however these generally cover traditional damage and not environmental damage <eos>\n",
      "test pred: <sos> three factors have emerged here: the level of the primary balance that is to say the <unk> of payments the rate of nominal growth of the <unk> the additional factor\n",
      "test targ: <sos> three factors have influenced this development: the level of primary balance that is excluding financial payments the nominal rate of growth of gdp and other autonomous or residual factors <eos>\n",
      "Step:6  WER:0.6349206349\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  2.095587873458862\n",
      "test pred: <sos> on behalf of the pse group - (fr) madam president commissioner ladies and gentlemen we are dealing here with an extremely important framework directive on water <eos>\n",
      "test targ: <sos> on behalf of the pse group - (fr) madam president commissioner ladies and gentlemen we have here an extremely important daughter directive of the water framework directive <eos>\n",
      "test pred: <sos> we are providing a great deal of political and financial support for this objective but i would like to stress that we are keeping a close eye on human rights\n",
      "test targ: <sos> we are extending substantial political and financial support to that goal while - and i would like to underline this - remaining very vigilant on human rights and other concerns <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.4212217569351195\n",
      "test pred: <sos> regions with low population density are badly affected and given that it is practically unthinkable for regions located in the vicinity of schools close to houses to housing we must\n",
      "test targ: <sos> as it would in desert or other <unk> areas be unrealistic to build a large number of schools near to where people live we have to take a good look at other solutions to reduce the educational deficit <eos>\n",
      "test pred: <sos> i had hoped that the initiatives put in place to obtain their support for the so-called successful proposal would be successful and that the new council would eventually see the\n",
      "test targ: <sos> it is a key country in the united nations and i hoped that the initiatives to obtain their support for mr <unk> proposal would succeed and that in the end the new council would be established with <unk> support <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.5817310571670533\n",
      "test pred: <sos> the reform is increasing <eos>\n",
      "test targ: <sos> the momentum for reform is mounting <eos>\n",
      "test pred: <sos> for ten years since globalisation the rate of growth in india has been around 6% <eos>\n",
      "test targ: <sos> over the last ten years since globalisation began india has had a rate of growth of 6 to 7% <eos>\n",
      "Step:7  WER:0.6390977444\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  2.354056167602539\n",
      "test pred: <sos> finally i should like to thank the local authorities for the aid must be provided by people on the ground <eos>\n",
      "test targ: <sos> in conclusion let me express my gratitude to the aid workers on the ground for aid has to be delivered to the local people <eos>\n",
      "test pred: <sos> firstly the position of the <unk> and its concern for privatisation <eos>\n",
      "test targ: <sos> first of all i should like to mention the position of air traffic controllers and their concerns surrounding privatisation <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.9673130989074707\n",
      "test pred: <sos> guinea is also setting an example of ghana an ancient british colony which had regained its independence a year earlier and which wished to develop an african model of socialism\n",
      "test targ: <sos> guinea on the other hand followed the example of the former british colony ghana which had gained independence one year earlier and which wanted to develop an african model of socialism with strong participation of the own population <eos>\n",
      "test pred: <sos> the best way of ensuring security of supply is certainly to have our own sources of supply and so it is to be welcomed that work is gradually being done\n",
      "test targ: <sos> the best form of security of supply is surely to have our own supplies and we must therefore welcome the fact that work is gradually being done (a) to produce high-quality gas ourselves inside the european union and (b) to feed that gas into our own available networks <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.8488765239715577\n",
      "test pred: <sos> mr president this report calls for the free circulation of waste in europe – urban waste industrial waste – and on this report i voted in favour <eos>\n",
      "test targ: <sos> mr president with this report mr blokland calls for freedom of movement in europe for waste – urban waste industrial waste – and i voted for the motion <eos>\n",
      "test pred: <sos> why then can we not be equally credible in the global <unk> <eos>\n",
      "test targ: <sos> why then can we not be equally credible in the political <unk> <eos>\n",
      "Step:8  WER:0.5645161290\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  2.413845491409302\n",
      "test pred: <sos> this creates jobs outside agriculture and creates unemployment among the eu and the member states and creates a serious threat to the continued existence of traditional family farming in europe\n",
      "test targ: <sos> with the availability of employment outside agriculture and the advances made by the eu and member states in reducing unemployment there is now a serious threat to the sustainability of europe's traditional family farm <eos>\n",
      "test pred: <sos> thirdly the commission believes that the need for action to be taken quickly and that preventive measures to be taken in the stability and growth pact must be the subject\n",
      "test targ: <sos> thirdly the commission believes that the debate must deal with the need to apply measures quickly to increase the use of the preventive measures of the stability and growth pact <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.4100107431411744\n",
      "test pred: <sos> in a letter received by richard balfe richard balfe on 19 april we will be surprised that a certain day was limited and that the meetings scheduled for this day\n",
      "test targ: <sos> for example on 19 april i received an e-mail from the <unk> mr richard balfe which stated that that day was a restricted day and that the meetings planned for that day were cancelled <eos>\n",
      "test pred: <sos> although i have not really been persuaded by everything that has been said in this context it is nevertheless true that the cultural dimension of the european project contains many\n",
      "test targ: <sos> whilst i was not actually persuaded by everything that was stated in this context it remains true that the cultural dimension of the european project contains many buried treasures and still has a lot of untold reserves <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.272175168991089\n",
      "test pred: <sos> how can we ignore the fact that the pact lays down useful rules for the member states in terms of deficit but does not define the policies necessary to realise\n",
      "test targ: <sos> who can fail to see that the pact while providing member states with a useful discipline as far as public deficits are concerned does not begin to define the policies that are required if the development targets set in lisbon are to be <unk> <eos>\n",
      "test pred: <sos> let me come back to the subject that is as serious as ever and i know that i have raised it over the course of the great <unk> and huge\n",
      "test targ: <sos> let me turn to this very serious subject which i know has raised great passions and has produced a great deal of intellectual energy over the years: the question of the systems of fixed book prices in the member states which as honourable members have said has been around for some time <eos>\n",
      "Step:9  WER:0.6406570842\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.9856199979782105\n",
      "test pred: <sos> it has the right of initiative and can decide anyway <eos>\n",
      "test targ: <sos> it has the right of initiative and can decide anyway <eos>\n",
      "test pred: <sos> but even when the profits were <unk> the lisbon strategy like our leaders for the <unk> <unk> <unk> a sort of <unk> <unk> to spend money on everything that is\n",
      "test targ: <sos> but even when the profits flooded in the lisbon strategy as our leaders chose to understand it remained an excuse – a kind of mantra – to spend the new and plentiful money on anything but the people who needed it most <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.5505639791488648\n",
      "test pred: <sos> the <unk> believes that it can deliver food at 110 000 people only 10% of the population in the area most affected to <unk> million people <eos>\n",
      "test targ: <sos> the <unk> only expects to deliver food <unk> to 110 000 people 10% of the <unk> area's <unk> million <eos>\n",
      "test pred: <sos> i would point out as the commissioner has done that this proposal deals with animal health <eos>\n",
      "test targ: <sos> i would stress as the commissioner did that this proposal deals with animal health <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.1966513633728026\n",
      "test pred: <sos> but to go further i urge above all the governments of the member states and the council to take the courageous decisions that are required <eos>\n",
      "test targ: <sos> in order to make progress however i invite above all the governments of member states and the council to take the courageous decisions that are necessary <eos>\n",
      "test pred: <sos> incidentally i would like to stress the spirit with which we are working and with which you have been working for four years a spirit inspired by the desire for\n",
      "test targ: <sos> i should like to emphasise in passing the spirit in which we are working and in which i have been working with you for the last four years a spirit which is inspired by a desire for transparency <eos>\n",
      "Step:10  WER:0.5432098765\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.5700087785720824\n",
      "test pred: <sos> i congratulate the commission on this initiative which seeks to give new impetus to legislation at european level on this issue <eos>\n",
      "test targ: <sos> i congratulate the commission on its initiative in adapting the legislation in this area to these developments at european level and also in giving new impetus <eos>\n",
      "test pred: <sos> we cannot even in europe share a europe of <unk> and <unk> <eos>\n",
      "test targ: <sos> we should not divide europe once again into the <unk> and the <unk> <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.8598031520843505\n",
      "test pred: <sos> i hope that the united kingdom will do so but it is also likely that the candidate countries in central and eastern europe will do so and that is why\n",
      "test targ: <sos> i hope that great britain will be one of them but some countries from the central and eastern european accession candidates will undoubtedly join too which means that it is essential to consider how these voting modalities can be amended and adjusted to the new dimension given that in future some 15 20 or 25 countries may be part of the <unk> <eos>\n",
      "test pred: <sos> the eu should also make a major contribution to this funding <eos>\n",
      "test targ: <sos> the eu should also make a major contribution to this shortfall <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.5257539749145508\n",
      "test pred: <sos> it is important that we apply the same rules on labelling as we do for meat imported from third countries <eos>\n",
      "test targ: <sos> it is essential that identical labelling rules are applied to beef imported from third countries <eos>\n",
      "test pred: <sos> it would be appropriate however for the president of a parliamentary group and also for those who are in charge of the european institutions - and i am talking about\n",
      "test targ: <sos> it would in any case be appropriate for the chairman of a parliamentary group and also for those holding positions of responsibility in the european institutions - i refer to the absent mr michel - not to make statements either of approval or condemnation in this chamber on the internal affairs of a member state <eos>\n",
      "Step:11  WER:0.6396181384\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.5322997570037842\n",
      "test pred: <sos> rarely have i been so proud of this position as it is today after a great deal of work in the committee <eos>\n",
      "test targ: <sos> rarely have i felt as proud of my position as i am today in relation to this report due to the work that has been carried out in committee over the last year <eos>\n",
      "test pred: <sos> when he was removed from the right of residence in slovenia he was abducted by germany where he had been in the 1960s and <unk> <eos>\n",
      "test targ: <sos> when he lost his rights of <unk> in slovenia he settled in germany where he lived and worked for 12 years <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.886980366706848\n",
      "test pred: <sos> unfortunately governments appear to have forgotten them <eos>\n",
      "test targ: <sos> unfortunately governments appear to have lost sight of this goal <eos>\n",
      "test pred: <sos> the rapporteur proposes to approve the common position which has only been added an article on <unk> already accepted by the council <eos>\n",
      "test targ: <sos> the rapporteur proposes that the common position be approved with just one additional article on the <unk> <unk> already agreed with the council <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.6346213340759277\n",
      "test pred: <sos> you have not chosen to use a <unk> model to adjust and to put it in a way that has in fact proved to be a mobile <unk> the conflict\n",
      "test targ: <sos> you did not choose the ambition of a model but settled for an accommodation and for staying afloat which in practice has proved to be a framework of shifting sands which has <unk> the power struggle between powers and between countries <eos>\n",
      "test pred: <sos> doha was supposed to be the development round <eos>\n",
      "test targ: <sos> doha should have been the development cycle <eos>\n",
      "Step:12  WER:0.5730337079\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  2.0373387813568113\n",
      "test pred: <sos> of course it is not only the company that is responsible <eos>\n",
      "test targ: <sos> of course it is not just the company that has a responsibility <eos>\n",
      "test pred: <sos> thanks to this directive we will be able to protect the health of the unborn child from harmful effects of high levels of exposure and exemptions which are evidence to\n",
      "test targ: <sos> thanks to this directive we shall be able to protect vegetation and people' s health from the damaging effects of unduly high concentrations of ozone except in those cases where that cannot be brought about by proportionate measures <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.649520254135132\n",
      "test pred: <sos> during the echelon committee we did not have any evidence that people are deliberately <unk> or that large-scale systems of interception of telecommunications are expressly used for the purposes of\n",
      "test targ: <sos> during the investigation by the echelon committee we have not obtained any evidence that citizens' privacy is being intentionally violated or that systems designed for large-scale tapping operations involving international telecommunications traffic are deliberately being used for large-scale and direct industrial espionage <eos>\n",
      "test pred: <sos> <unk> used some of the <unk> oil tanker which is <unk> several million to buy weapons <eos>\n",
      "test targ: <sos> <unk> used part of the oil <unk> s advance millions to acquire weapons <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.2457933664321899\n",
      "test pred: <sos> secondly the committee has not properly applied the rules on the reimbursement of travel expenses and other financial rules <eos>\n",
      "test targ: <sos> second that the cor failed to properly apply the rules on the reimbursement of travel expenses and other financial rules <eos>\n",
      "test pred: <sos> secondly the fisheries council has the limits imposed on cod instead of having such an opinion never issued and this is a political decision <eos>\n",
      "test targ: <sos> this is a restriction which according to the european fisheries council is required in order to protect cod whilst <unk> claim that such recommendations were never made and that this was a political decision <eos>\n",
      "Step:13  WER:0.5730659026\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.7294714212417603\n",
      "test pred: <sos> what has the finnish presidency done at european level to prevent the development of us from being in this direction because we will eventually either have either a democratic europe\n",
      "test targ: <sos> what has the finnish presidency done at european level to prevent things moving in that direction here as well in view of the fact that we will in the end have either a democratic europe or no europe? <eos>\n",
      "test pred: <sos> i would be glad mrs schroedter if you could include this amendment precisely in the light of subsidiarity in your positive considerations <eos>\n",
      "test targ: <sos> i should be very grateful mrs schroedter if you would actually include this proposed amendment in the part relating to subsidiarity in your positive deliberations <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.1757843255996705\n",
      "test pred: <sos> consequently as we have experienced with great disquiet the recent days of 9 april and 28 may will clearly only be able to be properly addressed by the mission which\n",
      "test targ: <sos> therefore since two recent days 9 april and 28 may have caused us great concern it is clear that we will only be able to act appropriately if we send observers which we hope will be possible and who we hope will be able to act effectively <eos>\n",
      "test pred: <sos> it is not only passengers but also all the <unk> that will benefit from the establishment of uniform standards in all european countries even if the number of people passing\n",
      "test targ: <sos> it is not only the passengers but also the whole crew who stand to benefit from the establishment of uniform standards in all european countries which will make it possible even though the number of people who travel by air is constantly increasing for the same safety standards to apply wherever they do so <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.9517207384109496\n",
      "test pred: <sos> twenty years after the seveso disaster to mention just two european directives to give the member states a harmonised policy on the management of industrial risks is no longer <unk>\n",
      "test targ: <sos> it is 33 years since the seveso disaster not to mention the other disasters that have occurred and two european directives seeking to provide the member states with a harmonised policy in managing major industrial risks will not have been enough to avoid another human tragedy this is a stark reality that illustrates the limits of a law even where this is binding and transnational <eos>\n",
      "test pred: <sos> in its communication of january 2000 the commission pointed out that we can only create a genuine european research area if we also promote a european area of ethics shared\n",
      "test targ: <sos> in its statement of january 2000 the commission highlighted the fact that a genuine european research area can only become a reality if we also promote a european area of ethical values which are shared throughout europe <eos>\n",
      "Step:14  WER:0.6905263158\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  2.031589317321777\n",
      "test pred: <sos> i am very glad that we are in favour of the future development of montenegro in the european union <eos>\n",
      "test targ: <sos> i am very glad that we see eye to eye on the future development of montenegro on its road towards the european union <eos>\n",
      "test pred: <sos> i would be grateful if you could do that <eos>\n",
      "test targ: <sos> i would be obliged if you could do so <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.7183631896972655\n",
      "test pred: <sos> the final version of the draft agenda for this part-session as drawn up by the conference of presidents at its meeting of thursday 8 november pursuant to rules 130 and\n",
      "test targ: <sos> the final draft of the agenda for this part-session as laid down by the conference of presidents at its meeting of thursday 8 november pursuant to rules 130 and 131 of the rules of procedure has been distributed to you <eos>\n",
      "test pred: <sos> this petition is a little commonplace it shows how complex the problem is and how necessary it is to find a balance as the commissioner said between the right to\n",
      "test targ: <sos> this request which is unusual demonstrates the complexity of the problem and the need to find a balance as the commissioner said between the right to asylum and the citizens feeling secure <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.410578441619873\n",
      "test pred: <sos> the evans report rightly says that it is important to have legal certainty for companies <eos>\n",
      "test targ: <sos> the evans report rightly stresses the importance of legal certainty for companies <eos>\n",
      "test pred: <sos> as regards the new agencies this provision will be included in the proposed <unk> <unk> <eos>\n",
      "test targ: <sos> an additional clause will be included in the instruments of incorporation of new agencies <eos>\n",
      "Step:15  WER:0.5584415584\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.5007245779037475\n",
      "test pred: <sos> although it contains some <unk> in the report it does not raise any language at all i think that is a good opportunity to say that <eos>\n",
      "test targ: <sos> the policy is implicitly included in the report but i still think that in a sense not dealing with it now is a missed opportunity <eos>\n",
      "test pred: <sos> a great deal of the improvements to the text of the amended proposal which was amended by the commission are based on this first opinion of parliament which was adopted\n",
      "test targ: <sos> this initial position taken by parliament resulted in numerous improvements to the text of the commission proposal as later amended which parliament approved on 2 december 1993 <unk> that approval on 27 october 1999 <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.2700092792510986\n",
      "test pred: <sos> therefore their consolidation in a single general clause is clear simple and straightforward <eos>\n",
      "test targ: <sos> therefore their consolidation into a single general clause is clear simple and straightforward <eos>\n",
      "test pred: <sos> we cannot allow competition to be exploited only to a few because ultimately it is a controlled society that is waiting for us to see the digital dictatorship <eos>\n",
      "test targ: <sos> competition must not be allowed to merely become a benefit for the few because at the end of that road awaits a controlled society currently the <unk> control dictatorship <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.1150166988372803\n",
      "test pred: <sos> two states have peace security <eos>\n",
      "test targ: <sos> two states both have peace both have security <eos>\n",
      "test pred: <sos> i think that we need to work on these two aspects together but i think that the report is a very valuable document <eos>\n",
      "test targ: <sos> i am keen to encourage both of those aspects but overall this report is extremely worthwhile <eos>\n",
      "Step:16  WER:0.5482866044\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.3170175313949586\n",
      "test pred: <sos> the debate is closed <eos>\n",
      "test targ: <sos> the debate is closed <eos>\n",
      "test pred: <sos> madam president i would like to say something about a point of order for wednesday which you have not mentioned <eos>\n",
      "test targ: <sos> madam president i would like to talk about an item on the agenda for wednesday which you have not yet announced <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.3873270988464355\n",
      "test pred: <sos> what we can now see however is that the old member states are trying to solve their old problems that are still outstanding at the expense of the new member\n",
      "test targ: <sos> however what we can now see is that the old member states intend to solve their long-standing unsolved problems to the detriment of the new member states <eos>\n",
      "test pred: <sos> as you said commissioner the main issue is the development of effective technical standards for products <eos>\n",
      "test targ: <sos> as you said commissioner it is largely about the development of effective technical standards for products <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.7130371570587157\n",
      "test pred: <sos> all this creates inequality which does not benefit the clarity of the legislative text and does not certainly help the passenger to understand exactly his rights <eos>\n",
      "test targ: <sos> the disparity introduced does not serve to make the legislative text clearer and certainly does not help passengers to understand their exact rights <eos>\n",
      "test pred: <sos> i have the impression that mrs wallström is paying more attention to mr barón crespo than to you mr fatuzzo <eos>\n",
      "test targ: <sos> i am afraid mrs wallström seems to be paying more attention to mr barón crespo than to you mr fatuzzo <eos>\n",
      "Step:17  WER:0.4896142433\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.4909604787826538\n",
      "test pred: <sos> first of all we must remember that people are involved and we must develop a system that allows adequate pensions with the widest possible leave to the member states <eos>\n",
      "test targ: <sos> first and foremost we need to remember that this is about people and based on this we must develop a system which makes sound pension schemes possible and we must as far as we can leave this task to the member states <eos>\n",
      "test pred: <sos> there can therefore be no talk of proportionality and furthermore the death of each individual is meaningless <eos>\n",
      "test targ: <sos> there is therefore a lack of proportionality the fact is however that each of these deaths is pointless <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.146934151649475\n",
      "test pred: <sos> in the baltic sea however the consequences would be much more disastrous and more sustainable than in the atlantic <eos>\n",
      "test targ: <sos> in the baltic sea however the consequences would be vastly more disastrous and longer-term than in the atlantic <eos>\n",
      "test pred: <sos> article <unk> of council directive <unk> on the framework agreement on fixed-term work concluded by the european trade union confederation the <unk> industry and the european centre for enterprise and\n",
      "test targ: <sos> article 2 paragraph 1 of council directive <unk> concerning the framework agreement on fixed-term work concluded by the european trade union confederation the union of industrial and employers' <unk> of europe and the european centre of enterprises with public participation obliges the member states to comply with the directive in question by 10 july 2001 <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.6796473503112792\n",
      "test pred: <sos> this instrument should enable states to contact the <unk> to pass on information but also to make progress in their dealings with them in order to be able to dismantle\n",
      "test targ: <sos> this instrument must be such as to make it possible to address states directly in order to urge them to pass on information but also to issue orders so that the networks can be dismantled <eos>\n",
      "test pred: <sos> that is why faced with so many unresolved issues small farmers cannot plan their investments and their future <eos>\n",
      "test targ: <sos> that is why when so many questions remain unanswered our small farmers cannot plan their investments and their future <eos>\n",
      "Step:18  WER:0.5707762557\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.7999889612197877\n",
      "test pred: <sos> these are the values that the european parliament has for its entire relationship with the <unk> <eos>\n",
      "test targ: <sos> these are values that have been defended by the european parliament throughout its relationship with mercosur <eos>\n",
      "test pred: <sos> we have a great deal of experience and wisdom in parliament and we are sure that parliament the commission and the council will be able to shape policy that will\n",
      "test targ: <sos> we recognise the experience and the wisdom of parliament and we are sure that together parliament commission and council will be able to frame a policy which will not only continue to command the attention of the whole house but will actually lead to the sort of beneficial consequences that we wish to see <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.4410655736923217\n",
      "test pred: <sos> we agree with you entirely to encourage <unk> and therefore to leave some of the fishermen in the fisheries sector <eos>\n",
      "test targ: <sos> we even agree on providing incentives for scrapping and therefore encouraging some fishermen to leave the fisheries sector <eos>\n",
      "test pred: <sos> the european parliament must accept the basis of this decision and extend the directive to cover environmental damage <eos>\n",
      "test targ: <sos> parliament should accept this proposal as a basis for a decision and extend the directive to environmental damage caused by nuclear activities <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.8474590063095093\n",
      "test pred: <sos> when we talk about quality we are talking primarily about a more proactive fiscal policy to increase employment promote research and innovation and to encourage investment in human capital <eos>\n",
      "test targ: <sos> when i say quality i mean especially a fiscal policy aimed more towards creating jobs supporting research and innovation and increasing investment in human resources <eos>\n",
      "test pred: <sos> a parliamentary debate at the time of the <unk> or almost the <unk> <eos>\n",
      "test targ: <sos> a parliamentary debate at the <unk> hour just before the <unk> hour <eos>\n",
      "Step:19  WER:0.6313131313\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.611215877532959\n",
      "test pred: <sos> unfortunately it has opted for <unk> without taking account of the consumers or the other eu countries <eos>\n",
      "test targ: <sos> however it has chosen to go down an imposed route without consideration for customers or indeed for other eu countries <eos>\n",
      "test pred: <sos> it is of course the commission’s task to ensure that community law is strictly observed and in such cases it must be applied in cases where appropriate measures have been\n",
      "test targ: <sos> it is of course the commission's job to guarantee that european community law is strictly complied with and in cases where inappropriate measures have been adopted leading not to the protection of workers but rather to restrictions on the freedom to provide services in the internal market the commission has to take action as enshrined in the treaty <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.0870757341384887\n",
      "test pred: <sos> only agreements at european union level can be considered <eos>\n",
      "test targ: <sos> only agreements at eu level can be considered <eos>\n",
      "test pred: <sos> the eu is consulting the six makes the best possible contribution to this process while remaining in favour of the implementation of security council resolution <unk> <eos>\n",
      "test targ: <sos> the eu is consulting with the six offering the best means by which we can assist in this process while remaining committed to the implementation of security council resolution <unk> <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.4644462585449218\n",
      "test pred: <sos> mr president at present a group of experts is operational in finland for border control <eos>\n",
      "test targ: <sos> – mr president at present a team of experts in the field of border control operate in finland <eos>\n",
      "test pred: <sos> as regards the method of family reunification the requirements relating to the provision of physical and mental resources for the disease will remain unchanged <eos>\n",
      "test targ: <sos> in matters of family reunification procedures requirements concerning accommodation material resources and sickness insurance will remain optional <eos>\n",
      "Step:20  WER:0.6040100251\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.4162629127502442\n",
      "test pred: <sos> we have been told however that they have to leave us because of the <unk> of <unk> <eos>\n",
      "test targ: <sos> however we were informed that owing to constraints of time you had to leave earlier <eos>\n",
      "test pred: <sos> i would like to say once again that i am delighted that we managed in the conciliation procedure to ensure that the services needed to search and demand general aid\n",
      "test targ: <sos> i should like to repeat to the house that i am delighted that during the conciliation procedure we managed to ensure that in general services for searching and consulting spatial information must be made available free of charge <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.7676441192626953\n",
      "test pred: <sos> we are in general agreement however with this project and i would like to thank the rapporteur mr sacconi for his commitment and for the work he has done <eos>\n",
      "test targ: <sos> nevertheless we are generally in agreement regarding this project and i should to thank the rapporteur mr sacconi for his commitment and for the work he has done <eos>\n",
      "test pred: <sos> in the joint resolution we call on the council and the member states to redouble their efforts to respect minorities and to build democratic structures and to ensure that all\n",
      "test targ: <sos> in the joint resolution we ask the council and the member states to double their efforts to ensure respect for minority groups and to work towards the creation of democratic structures giving a voice to all those forces which still believe in peaceful coexistence <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.0458722591400147\n",
      "test pred: <sos> thirdly the issue of the sale of minors is also important <eos>\n",
      "test targ: <sos> thirdly the issue of selling to under-age people is also an important one <eos>\n",
      "test pred: <sos> this means increasing the risks of the budget on the one hand and in relation to the environment on the other hand we must not forget that cotton cultivation is\n",
      "test targ: <sos> this poses new risks both to the budget and to the environment for we must not forget that cotton planting is a very intensive form of crop farming and involves the highly concentrated use of fertilisers and pesticides <eos>\n",
      "Step:21  WER:0.6025104603\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.6311128377914428\n",
      "test pred: <sos> if so we can withdraw the request for a split vote <eos>\n",
      "test targ: <sos> if so we can withdraw the attempt at a split vote <eos>\n",
      "test pred: <sos> in that way much of what parliament has sought would be achieved and we would also be treating a comparison <eos>\n",
      "test targ: <sos> we should in that way obtain most of what parliament has wanted at the same time as being able to avoid conciliation <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  2.0275343894958495\n",
      "test pred: <sos> in this respect we need more transparency and more information on the rights and obligations of companies that employ workers <eos>\n",
      "test targ: <sos> what is needed therefore is greater transparency and more information on the rights and obligations to be complied with when businesses become involved in the posting of workers <eos>\n",
      "test pred: <sos> at the same time as these trade measures the european union must establish a specific transitional aid plan for restructuring and conversion from the whole sector to help the textile\n",
      "test targ: <sos> in parallel with these trade measures the european union should implement a transitional practical plan to assist restructuring and retraining for the entire sector with a view to helping <unk> regions and safeguarding the future and competitiveness of the sector on international markets <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.9092201709747314\n",
      "test pred: <sos> finally we must ensure that the rights of the people living in areas close to the risk of disaster are protected <eos>\n",
      "test targ: <sos> finally we also need to ensure that the rights of people living near areas vulnerable to disasters are protected <eos>\n",
      "test pred: <sos> our group agrees with developing countries on the agenda for new issues because an enlargement of the agenda would put a strain on the ability of many developing countries to\n",
      "test targ: <sos> our group agrees with developing countries when they oppose new issues on the agenda on the grounds that expanding that agenda would over stretch the capacity of many developing countries which are already struggling to participate effectively in the wto process <eos>\n",
      "Step:22  WER:0.6236080178\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.4267914533615111\n",
      "test pred: <sos> i would like to thank you for giving me the opportunity to present my thoughts at least in more depth than ever before in this house <eos>\n",
      "test targ: <sos> i should like to thank you for giving me the opportunity at least to stretch my thoughts a little bit further than i have ever been allowed to do in this house before <eos>\n",
      "test pred: <sos> i therefore call on you to ensure that it is not only a matter of investigating but that the americans clearly declare the international convention banning the use of <unk>\n",
      "test targ: <sos> it is for that reason that i call upon you to ensure that there are not just investigations but that the americans also sign the international convention specifically outlawing the use of white <unk> <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.6560911417007447\n",
      "test pred: <sos> this point is already settled at the moment by the <unk> of the specific characteristics of products of the <unk> <eos>\n",
      "test targ: <sos> it is already included now: <unk> of the particular characteristics due to the manufacturing <unk> <eos>\n",
      "test pred: <sos> a <unk> as it was understood that not only the multilingual version is still in force but also that the simultaneous interpretation to all official languages should be made available\n",
      "test targ: <sos> this is a fiction to the extent that it has been agreed that not only will the multilingual version be retained but the simultaneous translation into all the official languages should also be made available on request to all the members of the house as well as to the public at large <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.3342558145523071\n",
      "test pred: <sos> the forthcoming elections on 17 november will certainly be an important step for the future of kosovo and will allow direct participation of the local population and political leaders in\n",
      "test targ: <sos> the elections on 17 november will certainly be a key event for the future of kosovo and will facilitate a greater level of direct participation by the population and by local political leaders in the operation of the provisional constitutional framework established by unmik <eos>\n",
      "test pred: <sos> wednesday: <eos>\n",
      "test targ: <sos> <unk> <eos>\n",
      "Step:23  WER:0.5876865672\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.7463845491409302\n",
      "test pred: <sos> it is also very important that in the framework of europol better cooperation between the law enforcement authorities in the member states should be permitted <eos>\n",
      "test targ: <sos> in addition it is very important to organise better cooperation between member states police forces through europol <eos>\n",
      "test pred: <sos> it is crucial that recognition - not only of the systems and procedures of third countries but also of the need to respect the requirements of this - be strictly\n",
      "test targ: <sos> it is essential that recognition not only of third countries' systems and procedures but also of their continued compliance be for a substantial initial period of time strictly monitored at european level as such recognition will be global in nature <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.9001995921134949\n",
      "test pred: <sos> while it is true that the initiatives of the commission in particular awareness actions are not enough to underestimate them unfortunately because sexual exploitation of children and child tourism are\n",
      "test targ: <sos> of course the commission' s initiatives especially the activities to increase awareness should not be underestimated but unfortunately they are proving to be inadequate since the sexual exploitation of children and child tourism are continually assuming greater dimensions not only in <unk> underdeveloped countries but within the european union itself <eos>\n",
      "test pred: <sos> what will happen in 20 years' time when their grandchildren sitting on their legs in their <unk> <unk> have been <unk> in this great disaster when europe has not been\n",
      "test targ: <sos> what will they do in 20 years' time when they are sitting with their grandchildren on their knees and their grandchildren say: 'what was your real role in the great disaster when europe failed to grasp its own <unk> <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.9786833286285401\n",
      "test pred: <sos> finally in response to mrs van lancker on the intentions and will of the swedish presidency with regard to the issue of discrimination i shall inform myself of the council's\n",
      "test targ: <sos> let me finally say the following in answer to mrs van <unk> s question on what the swedish presidency intends or would like to do to tackle the problems of discrimination <eos>\n",
      "test pred: <sos> there is the nice system which is based on a weighting of votes and there is the system of a constitution based on a double majority that is the majority\n",
      "test targ: <sos> there is the nice system based on a scale and there is the draft constitution system based on a double <unk> a majority of the member states and a majority of the represented population which we have set at 60% <eos>\n",
      "Step:24  WER:0.7063063063\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.4517634153366088\n",
      "test pred: <sos> you know me from the chair and know the situation there is a very good thing <eos>\n",
      "test targ: <sos> you know i am from a border region and i know exactly what the situation is on the ground <eos>\n",
      "test pred: <sos> this programme can also involve the candidate countries <eos>\n",
      "test targ: <sos> the countries seeking to join the union can also join this programme <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.9682612180709839\n",
      "test pred: <sos> i can only agree with that <eos>\n",
      "test targ: <sos> i very much support this <eos>\n",
      "test pred: <sos> this <unk> behaviour is very serious and unfair to those who live in countries that have chosen to provide themselves with sound civil protection <eos>\n",
      "test targ: <sos> this problem of <unk> is very serious and is unfair towards people in those countries that choose to have strong civil protection <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.9563532590866088\n",
      "test pred: <sos> thank you commissioner <eos>\n",
      "test targ: <sos> thank you commissioner <eos>\n",
      "test pred: <sos> thank you very much commissioner nielson <eos>\n",
      "test targ: <sos> thank you very much commissioner <eos>\n",
      "Step:25  WER:0.5217391304\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.4914816617965698\n",
      "test pred: <sos> mr president waste management as well as the whole of the eu is always an environmental policy area <eos>\n",
      "test targ: <sos> mr president the treatment of waste and the whole policy on waste is becoming an ever more important part of eu environmental policy <eos>\n",
      "test pred: <sos> in the review ten years later the commission is still proposing that it should remain in the opt-out <eos>\n",
      "test targ: <sos> in the review now 10 years on the commission is still proposing that the opt-out should continue to exist <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.6850104093551637\n",
      "test pred: <sos> the pollution of the environment from cadmium implies a very important health and economic cost in terms of <unk> in particular <eos>\n",
      "test targ: <sos> cadmium pollution of the environment carries very high health and economic costs including water purification costs <eos>\n",
      "test pred: <sos> finally mr president i really believe that the present paper should become a definition of packaging <eos>\n",
      "test targ: <sos> finally mr president i really do think that gift wrap should be covered by the definition of packaging <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.4816314220428466\n",
      "test pred: <sos> the reality is that under the institutional solutions we are taking the ecb is afraid of some people because they are afraid that they may end up with other institutional\n",
      "test targ: <sos> the truth is that in fact the institutional solutions we adopt with regard to the central bank are sometimes frightening because we sometimes have the impression that they could be imposed on other institutional mechanisms or even the union itself <eos>\n",
      "test pred: <sos> six out of the twelve candidate countries have a unemployment rate above 10% which creates social exclusion and poverty <eos>\n",
      "test targ: <sos> in six of the 12 candidate countries unemployment exceeds 10% leading to social exclusion and poverty <eos>\n",
      "Step:26  WER:0.5598958333\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.684684681892395\n",
      "test pred: <sos> we as members of parliament cannot deal with this way <eos>\n",
      "test targ: <sos> we as parliamentarians cannot be treated in this way <eos>\n",
      "test pred: <sos> the european parliament has spoken to the council and the commission with one voice despite the political and the countries of origin and the countries of origin so that it\n",
      "test targ: <sos> the european parliament spoke with one voice at the negotiating table with the council and the commission going beyond political affiliations and countries of origin and thus succeeded in having a greater impact on the results of the negotiation helping to significantly improve the initial structure of the provisions <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.2116801738739014\n",
      "test pred: <sos> curiously we always felt that we had the best consumer protection system in the world <eos>\n",
      "test targ: <sos> the funny thing is that we always claim that we have the best consumer protection in the world <eos>\n",
      "test pred: <sos> mr president commissioner ladies and gentlemen the world economic forum was held in switzerland the world social forum in brazil <eos>\n",
      "test targ: <sos> mr president commissioner ladies and gentlemen the world economic forum met in switzerland and the world social forum in brazil <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.0244709491729735\n",
      "test pred: <sos> the majority of the <unk> million people living in the least advanced countries barely a tenth of the total population worldwide are fighting to survive on less than two dollars\n",
      "test targ: <sos> most of the <unk> million people who live in the least developed countries just over <unk> of the world's entire population are struggling to survive on less than two dollars a day <eos>\n",
      "test pred: <sos> the directive also lays down that if the transmission bodies are given access to online services in future they will be able to provide online services with their own <unk>\n",
      "test targ: <sos> finally the directive also regulates when broadcasting organisations are entitled to use their archive productions for <unk> services and when they must first sign a contract with all the rightholders involved in the tv production giving them the right to do so <eos>\n",
      "Step:27  WER:0.6155462185\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.4971571922302247\n",
      "test pred: <sos> mr president political realism has led the commission to the conclusion that it would be inappropriate to amend the 1997 proposal at this time <eos>\n",
      "test targ: <sos> mr president political realism has led the commission to conclude that it is not appropriate at this stage to modify its proposal of 1997 <eos>\n",
      "test pred: <sos> we all noted that the problem is caused by the increase in trade particularly by the gas gap between the eu and russia on the one hand and the general\n",
      "test targ: <sos> we all noted that the origin of this problem was the growth in trade namely the increase in trade between the eu and russia on the one hand and the overall increase of trade with russia including <unk> on the other <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.2194382429122925\n",
      "test pred: <sos> that is the sort of thing we can expect from a negotiator and so there is no agreement that is better than a bad deal <eos>\n",
      "test targ: <sos> this is the sort of thing we can expect of a negotiator so it is better not to have an agreement at all than to have a flawed one <eos>\n",
      "test pred: <sos> the commission communication under examination concerns as everyone knows the commitment made by the european automobile industry to increase the chances of survival of pedestrians and cyclists involved in a\n",
      "test targ: <sos> as we all know the commission communication we are examining today is about a commitment by the european automobile industry designed to increase the chances of survival of pedestrians and cyclists involved in collisions with motor vehicles <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.9444860935211181\n",
      "test pred: <sos> the regional government believes that its inclusion in the <unk> of high-speed and specifically the high-speed train at the high speed of the mediterranean sea would contribute significantly to its\n",
      "test targ: <sos> the regional government considers that including it in the european high-speed train routes and more specifically in the route for the high-speed train from madrid to <unk> would bring it closer to the axis of the mediterranean basin considerably helping its development and its recovery from the shortfall that has accumulated in recent decades in terms of communications <eos>\n",
      "test pred: <sos> for example the total reduction in emissions from japan is 14% and the demand from canada is around 15% and this is a figure which will require a real commitment\n",
      "test targ: <sos> the total emission reduction required for example for japan is 14% and for canada around 15% so there is still a real effort to be made for these countries <eos>\n",
      "Step:28  WER:0.6458333333\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.2188096523284913\n",
      "test pred: <sos> this legislative package must be based on the individual approach adopted by the european parliament <eos>\n",
      "test targ: <sos> in accordance with the european parliament's proposal individual responsibility must form the basis for this collection target <eos>\n",
      "test pred: <sos> it is not a very simple matter when we consider that the principle of the free movement of goods and goods is one of the cornerstones of the european union\n",
      "test targ: <sos> it is not a very simple issue when we consider that the european union's basic pillars incorporate the notion of the free movement of people and goods <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.3636660814285277\n",
      "test pred: <sos> 'a little less <unk> is a <unk> <eos>\n",
      "test targ: <sos> 'not so <unk> says the italian <eos>\n",
      "test pred: <sos> we are not at all reassured particularly as i said we are well aware of mr bolkestein's intentions <eos>\n",
      "test targ: <sos> we are certainly not reassured by this particularly since as i have just said to you we are perfectly well aware of mr bolkestein's intentions <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.4071558713912964\n",
      "test pred: <sos> that is why last month we put forward ideas for making the adjustment and taking more flexible criteria into the application of the pact so that it becomes an increasingly\n",
      "test targ: <sos> this is why last month we floated ideas for making adjustments and introducing more sophisticated criteria for applying the pact with a view to making it an increasingly valuable instrument for promoting stability and growth <eos>\n",
      "test pred: <sos> this is therefore a simple formal competence which is fully regulated <eos>\n",
      "test targ: <sos> we are therefore talking about a formal and fully regulated competence <eos>\n",
      "Step:29  WER:0.6268656716\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.3209473848342896\n",
      "test pred: <sos> this process was completed at the end of 1993 <eos>\n",
      "test targ: <sos> this process was concluded as early as the end of 1993 <eos>\n",
      "test pred: <sos> another issue was internal conflicts which are at least being fought in two of the countries affected by the disaster <eos>\n",
      "test targ: <sos> another question touched upon the internal conflicts raging in at least two of the countries affected by the disaster <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.1543215990066529\n",
      "test pred: <sos> as a result we have not yet achieved what we wanted namely a commission in which the officials are actually responsible and have the same responsibility <eos>\n",
      "test targ: <sos> as a result we still have not got what we set out to achieve namely a commission in which officials are really responsible and can also shoulder that responsibility <eos>\n",
      "test pred: <sos> (the sitting was suspended at <unk> <unk> and resumed at 9 <unk> <eos>\n",
      "test targ: <sos> (the sitting was adjourned at <unk> <unk> and resumed at 9 <unk> <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.2185184717178346\n",
      "test pred: <sos> commissioner you are right to ask for more particularly on the subject of human rights and on the armenian issue but we cannot acknowledge the steps that turkey has already\n",
      "test targ: <sos> they must be fully <unk> commissioner you are right to demand more particularly regarding political rights and cyprus <eos>\n",
      "test pred: <sos> my group also intends to strengthen the provisions on family reunification <eos>\n",
      "test targ: <sos> my group also aims to further strengthen the provisions on family unity <eos>\n",
      "Step:30  WER:0.6352201258\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.6919857025146485\n",
      "test pred: <sos> i know that we have found ourselves in a council and commission position to find ourselves in a position to find solutions but i believe that in the many discussions\n",
      "test targ: <sos> i know that this puts us into a conflict situation with the council and the commission but i believe that our numerous discussions have brought solutions which of course ensure that the commission's responsibility is made clear that it will continue so to speak to supervise the aviation safety agency but that it is clear at the same time that the aviation safety agency must also take responsibility for itself in its area of expertise <eos>\n",
      "test pred: <sos> the institutions of the european union and the president-in-office of the council must not allow this situation to be <unk> <eos>\n",
      "test targ: <sos> the institutions of the european union and you as president-in-office of the council cannot tolerate this situation <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.2469779014587403\n",
      "test pred: <sos> if china does not go well then china needs to be <unk> <eos>\n",
      "test targ: <sos> if china does not accept this then we must expel china from the <unk> <eos>\n",
      "test pred: <sos> europe can do a lot but colleagues who say that it must go into a thorough <unk> perhaps are right not only in terms of what it does not do\n",
      "test targ: <sos> europe has the potential to do a great deal but those members may be right who say that it needs to carry out a considerable amount of <unk> not only in terms of what it fails to give as regards development aid but what it fails to give by refusing to change its policies too <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.4827550172805786\n",
      "test pred: <sos> indeed laboratories researchers scientists scientists responsible and <unk> like some think are making efforts to ensure that millions and millions of human beings are brought to life in a horrible\n",
      "test targ: <sos> they hope very soon to make it possible for untold millions of human beings to escape a dreadful fate these people would then be able to resume a normal life and regain their dignity as human beings <eos>\n",
      "test pred: <sos> i do not understand why the president was allowed to reject my question because when i spoke to mr aznar i made it very clear that this was the intention\n",
      "test targ: <sos> i fail to understand on what grounds the president has decided to reject my question because when i spoke about mr aznar i did so objectively and clearly: this was mr <unk> clear intention in threatening chile and mexico countries which furthermore are <unk> which makes the situation all the more unacceptable <eos>\n",
      "Step:31  WER:0.7292069632\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  2.3017048835754395\n",
      "test pred: <sos> mr president i believe that we can be satisfied - my group is in any case not just because this debate is taking place on a topical and urgent subject\n",
      "test targ: <sos> mr president i feel that we can be satisfied - at least my group is satisfied - that not only is this debate being held at an important juncture when a process is under way to mobilise the international press a process which has currently been suspended in south africa but it also has a symbolic <unk> do poor communities have priority access to medication which can relieve their suffering and delay their death or not? <eos>\n",
      "test pred: <sos> mr corbett's proposal is therefore to restrict access to the work of the citizens of europe who elected them to represent them in the european union and to defend their\n",
      "test targ: <sos> what mr corbett is proposing is therefore to limit the access of european citizens to the work of those whom they have elected to represent them and to defend their interests within the european union <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.2269340753555298\n",
      "test pred: <sos> mr president commissioner ladies and gentlemen there are still two different opinions on migration <eos>\n",
      "test targ: <sos> – mr president commissioner ladies and gentlemen two points of view are always <unk> with as regards the topic of migration <eos>\n",
      "test pred: <sos> the united nations is best placed to find a lasting solution to the conflict in the middle east <eos>\n",
      "test targ: <sos> the un is the international organisation best placed to bring about a lasting solution to the conflict in the middle east <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.7532841444015503\n",
      "test pred: <sos> the key element is that these sectors will at last have legislation and will also be covered by the directive <eos>\n",
      "test targ: <sos> it is now crucial that these sectors can at long last look forward to a regulation so that they too come under the scope of the directive <eos>\n",
      "test pred: <sos> i hope that we will be able to obtain the support of other groups on these and other important amendments <eos>\n",
      "test targ: <sos> i hope that we are successful in winning the support of other groups for this and other important amendments <eos>\n",
      "Step:32  WER:0.5857843137\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.772247052192688\n",
      "test pred: <sos> more legislation does not mean more consumer protection <eos>\n",
      "test targ: <sos> more statutory provisions do not mean greater protection of consumers <eos>\n",
      "test pred: <sos> in my country ireland in eight days' time a <unk> <unk> <unk> will be carried out in the 15 member states on the treaty of nice <eos>\n",
      "test targ: <sos> in my country the republic of ireland in eight days time there is a referendum on the treaty of nice unique among the 15 member states <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.7950010538101195\n",
      "test pred: <sos> we could do much better because we have made the great commitments we made to reduce co2 and greenhouse gases and reduce dependence on imports of energy <eos>\n",
      "test targ: <sos> we could do rather more considering that we have made great commitments in reducing co2 and greenhouse gases and to reduce dependence on energy imports <eos>\n",
      "test pred: <sos> in this context it is important that khartoum and the rebels contribute to the work of the international criminal tribunal for darfur and that the donor community should be ready\n",
      "test targ: <sos> in this context it is important that khartoum and the rebels contribute and assist the work of the international criminal court on darfur and that the donor community is ready to provide an immediate peace dividend once a positive outcome emerges from abuja <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.253907060623169\n",
      "test pred: <sos> of course we must give women the same opportunities to themselves the same conditions pension programmes and we must make it easier for both sexes to reconcile work and family\n",
      "test targ: <sos> women must of course be given the same job opportunities working conditions and pension schemes as men and we must make it easier for both sexes to reconcile work and family life especially if we want to encourage people to have children <eos>\n",
      "test pred: <sos> i am delighted that in the three years and more years i have been in kosovo we have not only witnessed a change in the economic and social situation on\n",
      "test targ: <sos> i am very pleased that in the three years and more that i have been visiting kosovo we have seen not only a transformation of the economic and social situation on the ground but the establishment of the provisional <unk> the political structure to reflect the wishes of the people of kosovo themselves <eos>\n",
      "Step:33  WER:0.5942028986\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.8314680099487304\n",
      "test pred: <sos> i would like to take advantage of the opportunity presented by our debate as a member of stoa to point out that we in the european parliament have a <unk>\n",
      "test targ: <sos> i should also like to remind you at this juncture as a member of the scientific and technological options assessment <unk> that we have a novel type of body available to us in the european parliament the stoa panel whose function is to provide members of this house with aspects of scientific assessment that they may need in order to clarify the positions that our various committees are required to take <eos>\n",
      "test pred: <sos> it is true that the world of the <unk> has in the end been adopting the niger <unk> <eos>\n",
      "test targ: <sos> admittedly the world has finally turned its attention to the fate of niger <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.1742186784744262\n",
      "test pred: <sos> as mayor of <unk> <unk> i was directly affected by the floods in the czech republic at this time many of our cities were <unk> and many people lost their\n",
      "test targ: <sos> as mayor of <unk> <unk> i had first-hand experience of the floods which hit the czech republic and which led to a number of our towns being inundated and many people being left homeless <eos>\n",
      "test pred: <sos> we must avoid a civil war being <unk> a crisis scenario in the middle east which would be extremely dangerous and regrettable for the oppressed and all its inhabitants <eos>\n",
      "test targ: <sos> we must avoid being caught up in a civil war between ethnic groups a crisis situation similar to that in the congo which would be extremely dangerous and regrettable for côte d'ivoire and all its inhabitants <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.1660035371780395\n",
      "test pred: <sos> mr president the european commission shares the concerns about child labour in general and in particular the production of sports goods <eos>\n",
      "test targ: <sos> mr president the european commission shares the concerns expressed about the use of child labour in general and particularly in the production of sports equipment <eos>\n",
      "test pred: <sos> it is not by chance that we can make buses available to us but that we can make it possible to provide <unk> in both rural and concentrated areas in\n",
      "test targ: <sos> it was not created in order to employ as many bus drivers as possible; on the contrary it was created so that attractive facilities can be offered in rural and more densely populated areas to ensure the mobility of all <eos>\n",
      "Step:34  WER:0.6809338521\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.424211573600769\n",
      "test pred: <sos> as you know this summit was negotiated with one other party <eos>\n",
      "test targ: <sos> as you will be aware this summit was negotiated with another <unk> it was not imposed on them nor could it be <eos>\n",
      "test pred: <sos> – the next item is the vote <eos>\n",
      "test targ: <sos> – the next item is voting time <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.7947728991508484\n",
      "test pred: <sos> i commend the report however and hope that the commissioner will be able to make this a legislative instrument <eos>\n",
      "test targ: <sos> but i commend the report and hope that the commissioner will be able to take it forward to a legislative instrument <eos>\n",
      "test pred: <sos> during the european development days this issue was discussed in detail by kofi annan which created a foundation to help rural areas and develop agriculture <eos>\n",
      "test targ: <sos> during the european development days this question was dealt with in some detail by kofi annan who has set up a foundation to aid rural regions and develop agriculture <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  2.2282981395721437\n",
      "test pred: <sos> the reasons for excluding pilotage from the scope of the directive are in my view applied to all the technical services provided that the quality of the service and safety\n",
      "test targ: <sos> as i see it the reasons for excluding pilotage services from the scope of the directive apply to all technical nautical services quality of service and the safety of operations are part and parcel of all these services <eos>\n",
      "test pred: <sos> these are the observations which indicate that there is still a great need for considerable commitment on the part of the candidate countries <eos>\n",
      "test targ: <sos> these are quite specific points that show where individual candidate countries still have to make enormous efforts <eos>\n",
      "Step:35  WER:0.5600000000\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.640576148033142\n",
      "test pred: <sos> unfortunately our economies are still developing in a way that fails to address the needs of children <eos>\n",
      "test targ: <sos> unfortunately our economies still operate in a way that fails to take account of children’s needs <eos>\n",
      "test pred: <sos> for all these reasons i am presenting to you as someone who has participated in the drawing up of the report i am very much in favour of its adoption\n",
      "test targ: <sos> for all these reasons as someone who has been involved in working on the report i wholeheartedly commend the report to you for adoption <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.3913267135620118\n",
      "test pred: <sos> it is important to focus on reducing environmental dangers with repercussions for human health or on diseases with considerable economic and social cost <eos>\n",
      "test targ: <sos> emphasis must be placed on reducing the environmental dangers which have repercussions on human health or cause diseases at huge economic and social cost <eos>\n",
      "test pred: <sos> in addition in recent years there has been an interesting and <unk> development in solvit in particular the fact that the resolution of problems by various centres of solvit is\n",
      "test targ: <sos> moreover over the past few years we have seen interesting spontaneous developments such as the more structural resolution of problems by several solvit centres even though this is not part of the solvit mandate as described in the commission recommendation <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.316526174545288\n",
      "test pred: <sos> this proposal is intended to <unk> the european union <eos>\n",
      "test targ: <sos> this proposal is designed to <unk> the european union <eos>\n",
      "test pred: <sos> we need to take measures to protect the rights of the unborn child because in the final analysis they are precisely the following: they are not condemned and therefore their\n",
      "test targ: <sos> something needs to be put in there to guarantee the rights of suspects because at the end of the day they are <unk> they have not been convicted and therefore their rights should be protected <eos>\n",
      "Step:36  WER:0.6274089936\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.7596386194229126\n",
      "test pred: <sos> mr president once again we are faced with the progress that biotechnology has made on people with an ethical dimension <eos>\n",
      "test targ: <sos> mr president once again we face a fundamental ethical debate on developments in biotechnology as applied to humans <eos>\n",
      "test pred: <sos> the <unk> really does think that we should dare to go down <eos>\n",
      "test targ: <sos> the committee on agriculture does in fact believe that we should have the courage to begin to move out of it <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.1439701080322267\n",
      "test pred: <sos> the most important issue though is that we are now beginning to realise that we cannot concentrate on this issue as regards what will happen during the next commitment period\n",
      "test targ: <sos> the most important issue even if we are now beginning to understand that it will probably not be possible to concentrate on it is about what will happen during the next commitment period <eos>\n",
      "test pred: <sos> i hear of new substantial aid the european union is preparing to provide <eos>\n",
      "test targ: <sos> i hear talk of very large sums of money that the european union is about to appropriate in further aid <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.4904967069625854\n",
      "test pred: <sos> at this stage i have to say that the meetings arranged by the european parliament in cancún were particularly fruitful and we received considerable support from a number of developing\n",
      "test targ: <sos> at this point i must say how well the european parliament meetings in cancún were received and how well we were supported by some of the developing countries <eos>\n",
      "test pred: <sos> i congratulate mr watson and all those involved in the drafting of this document <eos>\n",
      "test targ: <sos> i congratulate mr watson and all concerned in putting together this report <eos>\n",
      "Step:37  WER:0.6141176471\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.9778000116348267\n",
      "test pred: <sos> the principle of unanimity means paralysis <eos>\n",
      "test targ: <sos> unanimity is the system that allows a few to impose their will on everyone else <eos>\n",
      "test pred: <sos> in february 2007 we started negotiations on a new partnership and cooperation agreement with the eu <eos>\n",
      "test targ: <sos> in february 2007 we began negotiations on a new partnership and association agreement between ukraine and the eu <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  2.1243381023406984\n",
      "test pred: <sos> if however an international tribunal <unk> the trial of a few <unk> who bear a large part of the massacres in former yugoslavia if anything else for the genocide in\n",
      "test targ: <sos> even with one international criminal court conducting an investigation into a few <unk> who bear a large part of the responsibility for the massacres in the former yugoslavia and another conducting an investigation into the genocide in rwanda however there is absolutely nothing to suggest that all those guilty of offences which are just as serious will be prosecuted <eos>\n",
      "test pred: <sos> we also welcome the introduction by uzbekistan of a body of habeas corpus and the abolition of the death penalty which should come into effect in january 2008 <eos>\n",
      "test targ: <sos> we also welcome <unk> introduction of habeas corpus and the abolition of the death penalty which should come into effect in january 2008 <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.4191617250442505\n",
      "test pred: <sos> it is time to secure a future for the stability and growth pact <eos>\n",
      "test targ: <sos> now is also the time to ensure that the stability and growth pact will have a future <eos>\n",
      "test pred: <sos> i regret that the ppe-de group is now tabling rule 115 because even the last version - the french language - appeared yesterday at <unk> <unk> <eos>\n",
      "test targ: <sos> i regret that the group of the european people' s party (christian democrats) and european democrats have quoted rule 115 because the last version of the text which was the french one was actually issued at <unk> <unk> <eos>\n",
      "Step:38  WER:0.6363636364\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.5866996765136718\n",
      "test pred: <sos> the challenges posed by coal are greater due to its use in terms of energy production and its role in achieving our objectives to combat climate change <eos>\n",
      "test targ: <sos> with regard to coal the challenges are even greater due to its implications for energy production and for achieving our objectives for combating climate change <eos>\n",
      "test pred: <sos> it would have been nice to know who these candidates are <eos>\n",
      "test targ: <sos> it would have been good to know who the candidates are <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.3262310743331909\n",
      "test pred: <sos> the andersson report has turned its attention to the future but it is also aimed at modernising the system to ensure that pensions are made more effective and that our\n",
      "test targ: <sos> the andersson report is thinking about the next generations but is also thinking about bringing things up to date so that social security pensions fulfil our objectives and combat poverty <eos>\n",
      "test pred: <sos> the united nations security council remains the main <unk> for the resolution of the iraq question <eos>\n",
      "test targ: <sos> the un security council is still the main forum for resolving the iraqi issue <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  0.7164225459098816\n",
      "test pred: <sos> the fact is however that the proposal does not introduce patents on software and will not have all the dreadful effects that <unk> <unk> wish to make <eos>\n",
      "test targ: <sos> but the fact is that the proposal does not introduce software patents and will not have all the terrible effects that the <unk> of doom would have you believe <eos>\n",
      "test pred: <sos> consequently the primary aim of this technology appears to be another - to speed up the liberalisation of international trade and to increase the <unk> of production and distribution of\n",
      "test targ: <sos> consequently the overriding aim of this technology appears to be something else - speeding up the liberalisation of international trade and increasing the <unk> of the mass production and distribution of foods throughout the world with consequences for the <unk> of agri-food production and for the decline of small-scale local farmers thereby calling sustainable development into question <eos>\n",
      "Step:39  WER:0.5231213873\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.4433874368667603\n",
      "test pred: <sos> the decisions taken by the council during the irish presidency were – and are – a good basis for our efforts to prevent exposure to asbestos <eos>\n",
      "test targ: <sos> the decisions taken by the council during the irish presidency were – and continue to be – a good start in our efforts to prevent cardiovascular diseases <eos>\n",
      "test pred: <sos> first of all it is fair to say that soil has an internationally accepted character as a air and water which is always a thing of the past <eos>\n",
      "test targ: <sos> the issue of land use was outside the scope of that report <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.905025064945221\n",
      "test pred: <sos> it is quite simply more than a case of the mobile phones claiming the source of our brains <eos>\n",
      "test targ: <sos> however it is easier to claim that mobile phones <unk> our brains <eos>\n",
      "test pred: <sos> the same applies to bananas but we have agreed that an additional evaluation is needed in particular to take account of the outermost regions of the eu and so it\n",
      "test targ: <sos> the same principle applies on bananas but we agreed on additional evaluation notably to take into account the eu's outermost regions and that will happen <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.5222676992416382\n",
      "test pred: <sos> we hope that the formulations adopted by the commission will make it possible for the european parties to adopt a statute without giving too much space to the needs of\n",
      "test targ: <sos> we hope that the wording selected will allow the commission to finalise a european party statute quickly not set the majority requirements in the council too high and respect full co-decision for the european parliament <eos>\n",
      "test pred: <sos> the committee on human genetics and other new technologies in modern medicine has not achieved the aim i would like to see <eos>\n",
      "test targ: <sos> the temporary committee on human genetics and other new technologies in modern medicine did not reach the destination i expected <eos>\n",
      "Step:40  WER:0.6587112172\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.5540351390838623\n",
      "test pred: <sos> i said to president abbas that we welcome this initiative but i have no doubt that such an agreement is not the real objective but simply the beginning of a\n",
      "test targ: <sos> i have commended president abbas for taking this initiative but at the same time i have been clear that it is not an end in itself but only the start of a process <eos>\n",
      "test pred: <sos> we have finally ensured our representatives and i thank you for pointing this out <eos>\n",
      "test targ: <sos> we have at last secured our representative and thank you for pointing that out <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.611626386642456\n",
      "test pred: <sos> this madam president ladies and gentlemen is an ambitious proposal to improve road safety to ensure free movement in order to combat the fraud in driving licences <eos>\n",
      "test targ: <sos> madam president ladies and gentlemen here is an ambitious proposal for improving road safety ensuring freedom of movement and combating driving licence fraud <eos>\n",
      "test pred: <sos> it is inadequate <eos>\n",
      "test targ: <sos> it is inadequate <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.5809905052185058\n",
      "test pred: <sos> the vote will take place tomorrow at 11 <unk> <eos>\n",
      "test targ: <sos> the vote will take place tomorrow at 12 noon <eos>\n",
      "test pred: <sos> mr president commissioners ladies and gentlemen i think i have fought a sort of record today because i have been given 15 seconds of speaking time as rapporteur on three\n",
      "test targ: <sos> mr president ladies and gentlemen commissioners i have probably set some kind of record here today for i have been given a whole 15 minutes in which to speak because i am rapporteur for three different but quite similar matters <eos>\n",
      "Step:41  WER:0.6400911162\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.2771646499633789\n",
      "test pred: <sos> i am therefore very much afraid mr president that if the commission proposes a legislative package parliament will simply be a little <unk> <eos>\n",
      "test targ: <sos> thus i am very much afraid mr president that when the commission proposed a <unk> <unk> parliament gave it only a small <unk> <unk> <eos>\n",
      "test pred: <sos> the netherlands has made a clear application and has tabled a number of amendments <eos>\n",
      "test targ: <sos> the netherlands has clearly formulated a request and tabled quite a few amendments <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.021380352973938\n",
      "test pred: <sos> if this house adopts resolutions that reflect the thinking of the social democrats that we are in favour of them <eos>\n",
      "test targ: <sos> if this house adopts resolutions reflecting what we social democrats think then they will enjoy our support <eos>\n",
      "test pred: <sos> this right will then have to be adapted to the development of competences at national level in various sectors <eos>\n",
      "test targ: <sos> this right to skill development must then be adapted at national level within different sectors <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.6222459077835083\n",
      "test pred: <sos> non-tariff barriers also constitute serious obstacles to access to the european union's market <eos>\n",
      "test targ: <sos> non-tariff barriers also constitute major obstacles to accessing the european union market <eos>\n",
      "test pred: <sos> women only have to do with water <eos>\n",
      "test targ: <sos> they are the water carriers <eos>\n",
      "Step:42  WER:0.5140186916\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.7263277769088745\n",
      "test pred: <sos> despite some of the understandable political differences all those who feel committed to these principles must build a genuine european partnership <eos>\n",
      "test targ: <sos> despite some natural political and ideological differences those who are committed to these principles should build a truly european partnership <eos>\n",
      "test pred: <sos> their energy their enthusiasm and last but not least their enthusiasm for dealing with difficult issues are worth noting <eos>\n",
      "test targ: <sos> indeed their vigour their enthusiasm and most importantly their all-round good humour in dealing with difficult issues should be recognised <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.278392481803894\n",
      "test pred: <sos> indeed the conclusions do not seem to me to match the needs and possibilities of the moment <eos>\n",
      "test targ: <sos> indeed the conclusions do not strike me as being adequate to address current needs and opportunities <eos>\n",
      "test pred: <sos> all this should improve the controls make them effective and ensure better cooperation <eos>\n",
      "test targ: <sos> all of this should enable us to improve the checks make them effective and guarantee greater cooperation <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.6130954027175903\n",
      "test pred: <sos> on the basis of the commission report i can only conclude that the member states that wish to join the european union still have an awful lot to do <eos>\n",
      "test targ: <sos> i have no option but to conclude from the commission report that the member states wishing to accede still have to sort out an awful lot of things <eos>\n",
      "test pred: <sos> this proposal for a regulation will now enable us to define the legal basis for the implementation of a partnership for accession so as to facilitate fulfilment by turkey of\n",
      "test targ: <sos> this proposal for a regulation will now enable us to create the necessary legal basis to establish an accession partnership in order to help turkey fulfil the copenhagen criteria <eos>\n",
      "Step:43  WER:0.6072351421\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  2.0132339239120483\n",
      "test pred: <sos> it is our taxpayers' money that we have to ensure that it is used to achieve the goals of the international community and the european union <eos>\n",
      "test targ: <sos> we are talking about taxpayers' money here and we must be sure that it is used to achieve the goals set by the international community and the european union <eos>\n",
      "test pred: <sos> for this reason we are very much asking for an explanation and we hope that they will support the amendment <eos>\n",
      "test targ: <sos> for this reason we are asking most emphatically for an evaluation proposal and we hope that you will support the amendment <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.2315548181533813\n",
      "test pred: <sos> mr president i would just like to make a brief comment on the santini report the members of the pse have approved the santini report and the essential amendments from\n",
      "test targ: <sos> mr president i just want to make a brief comment on the santini report which along with the substantive amendments from the committee on citizens' freedoms and rights justice and home affairs the members belonging to the group of the party of european socialists have voted for even though it is clear to us that there are elements in this report about which the council will have serious misgivings some of them originating from the german government <eos>\n",
      "test pred: <sos> according to the european commission communication the gap between total public and private expenditure in the usa and the eu is constantly increasing it has risen from eur 12 billion\n",
      "test targ: <sos> according to the commission communication the gap between total public and private expenditure on research in the usa and in the eu is continuing to widen having grown from eur 12 billion in 1992 to some eur 60 billion in 1998 <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.2975939750671386\n",
      "test pred: <sos> i would like to draw your attention to the fact that the french minister for agriculture mr <unk> recently announced that he would carry out tests of the type we\n",
      "test targ: <sos> i would also draw your attention to the fact that recently mr <unk> the french minister of agriculture announced his intention of putting in place approximately 40 000 tests of the type that i have been discussing with you here this afternoon to determine the presence of bse and the levels of <unk> in france <eos>\n",
      "test pred: <sos> why do we send refugees into the hands of the regime that we want to <unk> <eos>\n",
      "test targ: <sos> why do we throw those refugees back into the hands of the regime we want to change? <eos>\n",
      "Step:44  WER:0.6190476190\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.148277497291565\n",
      "test pred: <sos> they claim to quote the democratic deficit in european <unk> the first step in overcoming this deficit is that the large companies will no longer be allowed to decide on\n",
      "test targ: <sos> you claim that you want to overcome and i quote 'the democratic deficit in european economic policy' but the first step you must take in order to overcome this deficit would be to deprive large employers of their divine right to get rid of all the people who work to improve their existence and you would do this by banning employers from making collective redundancies and forcing them to deduct from profits and from dividends stored up by shareholders whatever is necessary to safeguard all the jobs that are at risk <eos>\n",
      "test pred: <sos> let us therefore take a step forward and deal with these issues in a specific directive on water <eos>\n",
      "test targ: <sos> <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.077656626701355\n",
      "test pred: <sos> mr president i had asked for the floor on monday when we had the debate that members should be aware of the correspondence between mr bush and mr prodi <eos>\n",
      "test targ: <sos> mr president on monday when the sitting opened i asked for members to be provided with a copy of the letters exchanged between mr bush and mr prodi <eos>\n",
      "test pred: <sos> i have consulted and already consulted those who tabled the motion and who are supporting it <eos>\n",
      "test targ: <sos> i have discussed it with those who tabled the motion and they have no objection <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  0.8470634460449219\n",
      "test pred: <sos> we live with the <unk> they are part of our soul and we cannot live without them <eos>\n",
      "test targ: <sos> we live with <unk> they are part of our soul and we cannot live without them <eos>\n",
      "test pred: <sos> it has emerged that people are unfairly treated as a candidate country in the light of what was said during the dutch government's crisis <eos>\n",
      "test targ: <sos> there it emerged that people felt very unfairly identified as an unreliable candidate country in connection with what had been said during the dutch government crisis <eos>\n",
      "Step:45  WER:0.6982248521\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.5184510946273804\n",
      "test pred: <sos> if you think that the legislation is perfect you are presenting us with a review <eos>\n",
      "test targ: <sos> if you think the legislation is no good let us revise it <eos>\n",
      "test pred: <sos> in this respect effective enforcement of national rules on matters that contribute significantly to the implementation of the tax from the point of view of the <unk> to the consideration\n",
      "test targ: <sos> effective enforcement of existing national legislation on issues ranging from <unk> to age limits could make a significant difference <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.4669751167297362\n",
      "test pred: <sos> i hope that the expert opinion on pvc will be given as soon as possible <eos>\n",
      "test targ: <sos> i hope that the last outstanding report will be included in the assessment of soft pvc in the green paper <eos>\n",
      "test pred: <sos> i have no idea as to whether the advisory group for legal services created under the interinstitutional agreement of 20 december 1994 on a fast-track working method for the codification\n",
      "test targ: <sos> i do not know whether this was referred to the consultative working party of the legal services set up under the interinstitutional agreement of 20 december 1994 on an accelerated working method for official codification of legislative texts <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.408451795578003\n",
      "test pred: <sos> finally i should like to assure parliament that finding a peaceful solution to the conflict based on the positions of the european union is a priority of the irish presidency\n",
      "test targ: <sos> finally i should like to assure this house that the search for a peaceful solution to the <unk> conflict based on the well-established positions of the european union is a priority of the irish presidency and that we shall make every attempt to take the roadmap forward and to convince the parties to the conflict to make the necessary efforts to achieve a comprehensive and lasting peace for the benefit of all the peoples and states of the region <eos>\n",
      "test pred: <sos> pvc also has other properties including the very dangerous refusal <eos>\n",
      "test targ: <sos> however pvc also has other qualities being the source of unusually hazardous waste <eos>\n",
      "Step:46  WER:0.5950782998\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.5656105995178222\n",
      "test pred: <sos> this directive has contributed to reducing the environmental burden and strengthening the internal market for packaging and packaging waste <eos>\n",
      "test targ: <sos> in this way the directive has helped to reduce environmental impacts and to strengthen the internal market for packaging and packaging waste <eos>\n",
      "test pred: <sos> this is the council regulation on the criteria for determining the member state responsible for assessing an asylum application which was the subject of the marinho report <eos>\n",
      "test targ: <sos> i refer to the council regulation establishing the criteria for determining the member states responsible for examining asylum applications which was the subject of the marinho report <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.2034005403518677\n",
      "test pred: <sos> the recitals of the draft which state that firm action is needed to combat trafficking in human beings and the references to the un texts appear to us to be\n",
      "test targ: <sos> those recitals in the draft which state that there is a need to take strong measures to halt trafficking in human beings and the references to the un texts appear to us to be broadly correct <eos>\n",
      "test pred: <sos> i have had the opportunity to see the scenario of these manoeuvring and they have indeed been able to see with the refugees <eos>\n",
      "test targ: <sos> i have had sight of the scenario for these manoeuvres which do indeed have something to do with <unk> refugees <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.6055504322052\n",
      "test pred: <sos> i have the impression - and this should not be a drop in your merits commissioner byrne - that whenever the codecision procedure is applied the commission proposals are more\n",
      "test targ: <sos> i have the impression - and this is not meant to detract from your invaluable contribution mr byrne - that whenever the codecision procedure is applied the commission' s proposals are more concrete more <unk> and better adapted from the outset to the wishes and the working methods of the european parliament; these proposals serve as an example of the way we should be working here all the time <eos>\n",
      "test pred: <sos> i would add one more thing to that: there has been a list of projects <eos>\n",
      "test targ: <sos> there is something else i would <unk> we have produced a list known as the quick start list which provides for a series of projects <eos>\n",
      "Step:47  WER:0.5492957746\n",
      "Step:47  avg_WER:0.6039918325\n"
     ]
    }
   ],
   "source": [
    "test_print_coef = 10\n",
    "\n",
    "history = {\"test_loss\": [], \"test_wer\": [] }\n",
    "\n",
    "n = 0\n",
    "\n",
    "f_test = open( \"test.log\", mode=\"w\", encoding = \"UTF-8\" )\n",
    "\n",
    "tes_global_step = 0\n",
    "\n",
    "print( \"Test\")\n",
    "test_wer = 0\n",
    "n = 0\n",
    "model.eval()\n",
    "for i, test_task in enumerate(db_test):\n",
    "    tes_global_step += 1\n",
    "    display = True\n",
    "    print(\"\\n-----------------Test Mode-----------------\\n\")\n",
    "    wer = fine_tuning( batch = test_task, train_step = 5, lr1 = 1e-4, display = display )\n",
    "\n",
    "    \n",
    "    test_wer += wer\n",
    "    n += 1\n",
    "    history[\"test_wer\"].append( test_wer / n )\n",
    "    print(f\"Step:{tes_global_step}  WER:{ wer:.10f}\")\n",
    "    f_test.write( \"Epoch: \" + str(epoch) + \", Step: \" + str(tes_global_step) + \", WER: \" + str(test_wer / n ) + \"\\n\" )\n",
    "    f_test.flush()\n",
    "\n",
    "f_test.close()\n",
    "print(f\"Step:{tes_global_step}  avg_WER:{ test_wer / n :.10f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "a758d471-3194-407c-a9a8-b4cb7abb7469",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"best_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "77ac70b1-d9ba-4ffd-ad05-234f8ea3a61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.to('cpu'), \"best_model_cpu.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "04c8f963-4254-4313-8c7b-46dbf5510e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = torch.load( \"best_model_cpu.pth\", weights_only = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4970d79-2b46-4f25-9803-d1f7d9323fb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
