{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8bf21a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install janome\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "#from torchtext.vocab import vocab\n",
    "#import torchtext.transforms as T\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#from torchvision import transforms\n",
    "import numpy as np\n",
    "import math\n",
    "import janome\n",
    "from janome.tokenizer import Tokenizer\n",
    "#import spacy\n",
    "from collections import Counter\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import time\n",
    "#from torchtext.vocab import build_vocab_from_iterator\n",
    "import levenshtein\n",
    "import json\n",
    "import pickle\n",
    "from timm.scheduler import CosineLRScheduler\n",
    "import nltk\n",
    "from nltk import bleu_score\n",
    "from torch.nn.init import constant_, xavier_uniform_, kaiming_uniform_, xavier_normal_, kaiming_normal_\n",
    "from torch.nn.parameter import Parameter\n",
    "from transformers import  get_linear_schedule_with_warmup\n",
    "from collections import OrderedDict\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "244f7857-07bc-45b9-89d6-e11c2e21b9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49528 20556\n"
     ]
    }
   ],
   "source": [
    "with open( \"corpus/id_to_word_s.pkl\", \"rb\" ) as f:\n",
    "    token_list = pickle.load(f)\n",
    "with open( \"corpus/id_to_word_t.pkl\", \"rb\" ) as f:\n",
    "    token_list_en = pickle.load(f)\n",
    "with open( \"corpus/word_to_id_s.pkl\", \"rb\" ) as f:\n",
    "    idx_list = pickle.load(f)\n",
    "with open( \"corpus/word_to_id_t.pkl\", \"rb\" ) as f:\n",
    "    idx_list_en = pickle.load(f)\n",
    "\n",
    "#token_list[4] = '<blank>'\n",
    "#token_list_en[4] = '<blank>'\n",
    "#idx_list['<pad>'] = 4\n",
    "#idx_list_en['<pad>'] = 4\n",
    "\n",
    "pad_idx_s = idx_list['<pad>']\n",
    "pad_idx_t = idx_list_en['<pad>']\n",
    "\n",
    "#enc_vocab_size, dec_vocab_size = len(token_list) - 1, len(token_list_en) - 1\n",
    "enc_vocab_size, dec_vocab_size = len(token_list), len(token_list_en)\n",
    "print(enc_vocab_size, dec_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86f1161a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<sos>', '<eos>', '<unk>', '<blank>', '<mask>', 'der']\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([ 0, 1,2,3,4,5,6 ])\n",
    "\n",
    "#print( token_list[2] )\n",
    "\n",
    "#n = 0\n",
    "\n",
    "#ii = [ i for i in a ]\n",
    "\n",
    "#print( ii )\n",
    "\n",
    "b = [ token_list[i.item()] for i in a ]\n",
    "\n",
    "print( b )\n",
    "\n",
    "\n",
    "d = idx_list['<pad>']\n",
    "\n",
    "print( d )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "649837a5-2937-4e81-880b-a57f2532b967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 乱数の種を設定\n",
    "def random_seed(value):\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    torch.manual_seed(value)\n",
    "    torch.cuda.manual_seed(value)\n",
    "    np.random.seed(value)\n",
    "    random.seed(value)\n",
    "\n",
    "# list からランダムに kosuu の数値をとってきて list を作る。\n",
    "def select( list, kosuu ):\n",
    "    output = random.sample( list.tolist(), kosuu )\n",
    "    return output\n",
    "\n",
    "# ( outer_batch の次元のある)taskset から outer_batch_size の outer_batch データを作る。    \n",
    "def create_batch_of_tasks(taskset, is_shuffle = True, outer_batch_size = 4):\n",
    "\n",
    "    idxs = list(range(0,len(taskset)))\n",
    "    if is_shuffle:\n",
    "        random.shuffle(idxs)\n",
    "        idxs = torch.tensor( idxs )\n",
    "        idxss = select( idxs, outer_batch_size )\n",
    "    else:\n",
    "        idxs = torch.tensor( idxs )\n",
    "        idxss = idxs[ :outer_batch_size ]\n",
    "    \n",
    "    output = []\n",
    "\n",
    "    for i in idxss:\n",
    "        output.append( taskset[i] )\n",
    "\n",
    "    return output\n",
    "\n",
    "def build_batch_set( trainset, pad_idx_s, pad_idx_t, num_task = 17, k_support = 10, k_query = 10 ):\n",
    "\n",
    "    count = []\n",
    "    rr_s = []\n",
    "    rr_t = []\n",
    "    rr_len_s = []\n",
    "    rr_len_t = []\n",
    "    s_max = []\n",
    "    t_max = []\n",
    "    for i in range( num_task ):\n",
    "        count.append( 0 )\n",
    "        rr_s.append( [] )\n",
    "        rr_t.append( [] )\n",
    "        rr_len_s.append( [] )\n",
    "        rr_len_t.append( [] )\n",
    "        s_max.append(0)\n",
    "        t_max.append(0)\n",
    "    \n",
    "    for r in trainset:\n",
    "        for i in range( num_task ):\n",
    "            if r['domain'] == i:\n",
    "                count[i] += 1\n",
    "                rr_s[i].append( r['source'] )\n",
    "                rr_t[i].append( r['target'] )\n",
    "                rr_len_s[i].append( len( r['source']))\n",
    "                rr_len_t[i].append( len( r['target']))\n",
    "                if len( r['source'] ) > s_max[i]:\n",
    "                    s_max[i] = len( r['source'] )\n",
    "                if len( r['target'] ) > t_max[i]:\n",
    "                    t_max[i] = len( r['target'] )\n",
    "    \n",
    "    ss_max = max( s_max )\n",
    "    tt_max = max( t_max )\n",
    "    print( \"ss_max:\",ss_max )\n",
    "    print( \"tt_max:\",tt_max )\n",
    "                \n",
    "    #print( rr[0][0] )\n",
    "    \n",
    "    for i in range( num_task ):\n",
    "        print( \"i:\",i, \" count: \" ,count[i] )\n",
    "        \n",
    "    max_count = max( count )\n",
    "    min_count = min( count )\n",
    "    \n",
    "    #print( \"min_count:\", min_count )\n",
    "    \n",
    "    batch_max = max_count // (k_support + k_query )\n",
    "    print( \"batch_max:\", batch_max )\n",
    "    batch_min = min_count // (k_support + k_query )\n",
    "    print( \"batch_min:\", batch_min )\n",
    "    \n",
    "    batch = batch_min\n",
    "\n",
    "    print( batch * ( k_support + k_query ))\n",
    "    print( len( rr_s[0] ))\n",
    "    print( len( rr_t[0] ))\n",
    "    \n",
    "    output = []\n",
    "\n",
    "    for i in range( batch ):\n",
    "        if i % 10000 ==0:\n",
    "            print( \"i:\", i )\n",
    "\n",
    "        output0 = []\n",
    "        supports_source = []\n",
    "        supports_target = []\n",
    "        queries_source = []\n",
    "        queries_target = []\n",
    "        supports_source_len = []\n",
    "        supports_target_len = []\n",
    "        queries_source_len = []\n",
    "        queries_target_len = []\n",
    "        for j in range( num_task ):\n",
    "            supports_source0 = []\n",
    "            supports_target0 = []\n",
    "            queries_source0 = []\n",
    "            queries_target0 = []\n",
    "            supports_source0_len = []\n",
    "            supports_target0_len = []\n",
    "            queries_source0_len = []\n",
    "            queries_target0_len = []\n",
    "            for k in range( k_support ):\n",
    "                ik2 = i * ( k_support + k_query ) + k\n",
    "                while ik2 >= len( rr_s[j] ):\n",
    "                    ik2 -= len( rr_s[j] )\n",
    "                #print( \"i,:\",i, \"j:\", j, \"len(rr[j]):\", len(rr[j]), \"ik2:\", ik2 )\n",
    "                pad_len = ss_max - rr_len_s[j][ik2]\n",
    "                tmp_s = np.pad( rr_s[j][ik2], (0, pad_len), mode='constant', constant_values=(pad_idx_s, pad_idx_s ))\n",
    "                supports_source0.append( tmp_s )\n",
    "                pad_len = tt_max - rr_len_t[j][ik2]\n",
    "                tmp_t = np.pad( rr_t[j][ik2], (0, pad_len), mode='constant', constant_values=(pad_idx_t, pad_idx_t ))\n",
    "                supports_target0.append( tmp_t ) \n",
    "                supports_source0_len.append( rr_len_s[j][ik2])\n",
    "                supports_target0_len.append( rr_len_t[j][ik2] ) \n",
    "            for k in range( k_query ):\n",
    "                ik2 = i * ( k_support + k_query ) + k_support + k\n",
    "                while ik2 >= len( rr_s[j] ):\n",
    "                    ik2 -= len( rr_s[j] )\n",
    "                pad_len = ss_max - rr_len_s[j][ik2]\n",
    "                tmp_s = np.pad( rr_s[j][ik2], (0, pad_len), mode='constant', constant_values=(pad_idx_s, pad_idx_s ))\n",
    "                queries_source0.append( tmp_s )\n",
    "                pad_len = tt_max - rr_len_t[j][ik2]\n",
    "                tmp_t = np.pad( rr_t[j][ik2], (0, pad_len), mode='constant', constant_values=(pad_idx_t, pad_idx_t ))\n",
    "                queries_target0.append( tmp_t ) \n",
    "                queries_source0_len.append(rr_len_s[j][ik2])\n",
    "                queries_target0_len.append( rr_len_t[j][ik2] ) \n",
    "            supports_source.append(supports_source0)\n",
    "            supports_target.append( supports_target0 ) \n",
    "            queries_source.append(queries_source0)\n",
    "            queries_target.append( queries_target0 ) \n",
    "            supports_source_len.append(supports_source0_len)\n",
    "            supports_target_len.append( supports_target0_len ) \n",
    "            queries_source_len.append(queries_source0_len)\n",
    "            queries_target_len.append( queries_target0_len ) \n",
    "        output0.append( supports_source )\n",
    "        output0.append( supports_source_len )\n",
    "        output0.append( supports_target )\n",
    "        output0.append( supports_target_len )\n",
    "        output0.append( queries_source )\n",
    "        output0.append( queries_source_len )\n",
    "        output0.append( queries_target )\n",
    "        output0.append( queries_target_len )\n",
    "        output.append( output0 )\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6633015a-bc68-40d7-8d7c-cb4c63029f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ss_max: 184\n",
      "tt_max: 191\n",
      "i: 0  count:  528\n",
      "i: 1  count:  495\n",
      "i: 2  count:  478\n",
      "batch_max: 52\n",
      "batch_min: 47\n",
      "470\n",
      "528\n",
      "528\n",
      "i: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device( \"cpu\")\n",
    "\n",
    "# dataset\n",
    "with open('data_test.pkl', 'rb') as f:\n",
    "    testset = pickle.load(f)\n",
    "\n",
    "pad_idx_s = idx_list['<pad>']\n",
    "pad_idx_t = idx_list_en['<pad>']\n",
    "    \n",
    "#num_ob_val = 10\n",
    "ob_test = build_batch_set( testset, pad_idx_s, pad_idx_t, num_task = 3, k_support = 5, k_query = 5 )\n",
    "#ob_val = ob_val[:num_ob_val]\n",
    "db_test = create_batch_of_tasks(ob_test, is_shuffle = False, outer_batch_size = len( ob_test ))\n",
    "\n",
    "del testset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a02a5779",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        #self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        #return self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "434ca8e2-10dc-4f72-a464-e1155bb52d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    '''\n",
    "    位置埋め込み （Positional embedding）\n",
    "    dim_embedding: 埋込み次元\n",
    "    max_len      : 入力の最大系列長\n",
    "    '''\n",
    "    def __init__(self, dim_embedding: int, max_len: int=5000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pos_emb = nn.Embedding(max_len, dim_embedding)\n",
    "\n",
    "    '''\n",
    "    位置エンコーディングの順伝播\n",
    "    x: 位置エンコーディングを埋め込む対象のテンソル,\n",
    "       [バッチサイズ, 系列長, 埋め込み次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        seq = x.shape[1]\n",
    "        positions = torch.arange(start=0, end=seq, step=1, device=x.device).to(torch.long)\n",
    "        positions = self.pos_emb(positions)[:seq,:]\n",
    "        \n",
    "        return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79d4879d-0bb9-40d8-94da-913b2955d7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fPositionalEmbedding(nn.Module):\n",
    "    '''\n",
    "    位置埋め込み （Positional embedding）\n",
    "    dim_embedding: 埋込み次元\n",
    "    max_len      : 入力の最大系列長\n",
    "    '''\n",
    "    def __init__(self, dim_embedding: int, name, max_len: int=5000):\n",
    "        super().__init__()\n",
    "\n",
    "        #self.pos_emb = nn.Embedding(max_len, dim_embedding)\n",
    "        self.name = name\n",
    "        \n",
    "    '''\n",
    "    位置エンコーディングの順伝播\n",
    "    x: 位置エンコーディングを埋め込む対象のテンソル,\n",
    "       [バッチサイズ, 系列長, 埋め込み次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor, weights):\n",
    "        seq = x.shape[1]\n",
    "        positions = torch.arange(start=0, end=seq, step=1, device=x.device).to(torch.long)\n",
    "        #positions = self.pos_emb(positions)[:seq,:]\n",
    "        positions = F.embedding( positions, weights[self.name] )[:seq,:]\n",
    "        \n",
    "        return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de837c69-4e8b-4b21-bf3f-cb6e9f0cd0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "    '''\n",
    "    自己アテンション\n",
    "    dim_hidden: 入力特徴量の次元\n",
    "    num_heads : マルチヘッドアテンションのヘッド数\n",
    "    qkv_bias  : クエリなどを生成する全結合層のバイアスの有無\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 qkv_bias: bool=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # 特徴量を各ヘッドのために分割するので、\n",
    "        # 特徴量次元をヘッド数で割り切れるか検証\n",
    "        assert dim_hidden % num_heads == 0\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # ヘッド毎の特徴量次元\n",
    "        dim_head = dim_hidden // num_heads\n",
    "\n",
    "        # ソフトマックスのスケール値\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        # ヘッド毎にクエリ、キーおよびバリューを生成するための全結合層\n",
    "        self.proj_in_q = nn.Linear(\n",
    "            dim_hidden, dim_hidden, bias=qkv_bias)\n",
    "        self.proj_in_k = nn.Linear(\n",
    "            dim_hidden, dim_hidden, bias=qkv_bias)\n",
    "        self.proj_in_v = nn.Linear(\n",
    "            dim_hidden, dim_hidden, bias=qkv_bias)\n",
    "\n",
    "        #self.in_proj_weight = nn.Parameter( torch.randn( dim_hidden * 3, dim_hidden ) )\n",
    "        #self.in_proj_bias = nn.Parameter( torch.randn( dim_hidden * 3 ) )\n",
    "        #self.in_proj_weight = Parameter( torch.empty( dim_hidden * 3, dim_hidden ) )\n",
    "        #self.in_proj_bias = Parameter( torch.empty( dim_hidden * 3 ) )\n",
    "\n",
    "        # 各ヘッドから得られた特徴量を一つにまとめる全結合層\n",
    "        self.proj_out = nn.Linear(dim_hidden, dim_hidden)\n",
    "        \n",
    "        #self._reset_parameters()\n",
    "        \n",
    "    #def _reset_parameters(self):\n",
    "    #    xavier_uniform_( self.in_proj_weight )\n",
    "    #    constant_( self.in_proj_bias, 0.0 )\n",
    "\n",
    "    def split_head(self, x):\n",
    "        x = torch.tensor_split(x, self.num_heads, dim = 2)\n",
    "        x = torch.stack(x, dim = 1)\n",
    "        return x\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, mask: torch.Tensor):\n",
    "\n",
    "        #bs_q, ns_q, ds_q = q.size()\n",
    "        #bs_k, ns_k, ds_k = k.size()\n",
    "        \n",
    "        ##  k = v assumpotion\n",
    "        #if q is k:\n",
    "        #    qkv = q @ self.in_proj_weight.transpose(-2,-1) + self.in_proj_bias\n",
    "        #    qkv = qkv.view( bs_q, ns_q, 3, ds_q )\n",
    "        #    q, k, v = torch.unbind( qkv, dim = 2 )\n",
    "        #else:\n",
    "        #    W_q, W_kv = self.in_proj_weight.split([ds_q, ds_q * 2])\n",
    "        #    b_q, b_kv = self.in_proj_bias.split([ds_q, ds_q * 2])\n",
    "        #    q = q @ W_q.transpose(-2,-1) + b_q\n",
    "        #    kv =  k @ W_kv.transpose(-2,-1) + b_kv\n",
    "        #    kv = kv.view( bs_k, ns_k, 2, ds_k )\n",
    "        #    k, v = torch.unbind( kv, dim = 2 )\n",
    "        \n",
    "        q = self.proj_in_q(q)\n",
    "        k = self.proj_in_k(k)\n",
    "        v = self.proj_in_v(v)\n",
    "        \n",
    "        q = self.split_head(q)\n",
    "        k = self.split_head(k)\n",
    "        v = self.split_head(v)\n",
    "\n",
    "        # クエリとキーの行列積とアテンションの計算(今回マスクは不使用)\n",
    "        # attnは[バッチサイズ, ヘッド数, 特徴量数, 特徴量数]\n",
    "        attn = q.matmul(k.transpose(-2, -1))\n",
    "        #print( \"attn size:\", attn.size() )\n",
    "        #print( \"mask size:\", mask.size() )\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill_(mask, -torch.finfo(torch.float).max)\n",
    "        attn = (attn * self.scale).softmax(dim=-1)\n",
    "\n",
    "        # アテンションとバリューの行列積によりバリューを収集\n",
    "        # xは[バッチサイズ, ヘッド数, 特徴量数, ヘッドの特徴量次元]\n",
    "        x = attn.matmul(v)\n",
    "\n",
    "        # permute関数により\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数, ヘッドの特徴量次元]\n",
    "        # flatten関数により全てのヘッドから得られる特徴量を連結して、\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数 * ヘッドの特徴量次元]\n",
    "        x = x.permute(0, 2, 1, 3).flatten(2)\n",
    "        x = self.proj_out(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8aa83af-c34f-4e9a-80e0-00fef0bb5135",
   "metadata": {},
   "outputs": [],
   "source": [
    "class feMHA(nn.Module):\n",
    "    '''\n",
    "    自己アテンション\n",
    "    dim_hidden: 入力特徴量の次元\n",
    "    num_heads : マルチヘッドアテンションのヘッド数\n",
    "    qkv_bias  : クエリなどを生成する全結合層のバイアスの有無\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 qkv_bias: bool=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # 特徴量を各ヘッドのために分割するので、\n",
    "        # 特徴量次元をヘッド数で割り切れるか検証\n",
    "        assert dim_hidden % num_heads == 0\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # ヘッド毎の特徴量次元\n",
    "        dim_head = dim_hidden // num_heads\n",
    "\n",
    "        # ソフトマックスのスケール値\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        # ヘッド毎にクエリ、キーおよびバリューを生成するための全結合層\n",
    "        #self.proj_in = nn.Linear(\n",
    "        #    dim_hidden, dim_hidden * 3, bias=qkv_bias)\n",
    "\n",
    "        # 各ヘッドから得られた特徴量を一つにまとめる全結合層\n",
    "        #self.proj_out = nn.Linear(dim_hidden, dim_hidden)\n",
    "\n",
    "    def split_head(self, x):\n",
    "        x = torch.tensor_split(x, self.num_heads, dim = 2)\n",
    "        x = torch.stack(x, dim = 1)\n",
    "        return x\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor,  v: torch.Tensor, mask: torch.Tensor, i, weights ):\n",
    "        bs, ns = q.shape[:2]\n",
    "\n",
    "        #qkv = self.proj_in(x)\n",
    "        q = F.linear(q, weights['encoder.encoder_layers.' + str(i) + '.attention.proj_in_q.weight'], weights['encoder.encoder_layers.' + str(i) + '.attention.proj_in_q.bias'])\n",
    "        k = F.linear(k, weights['encoder.encoder_layers.' + str(i) + '.attention.proj_in_k.weight'], weights['encoder.encoder_layers.' + str(i) + '.attention.proj_in_k.bias'])\n",
    "        v = F.linear(v, weights['encoder.encoder_layers.' + str(i) + '.attention.proj_in_v.weight'], weights['encoder.encoder_layers.' + str(i) + '.attention.proj_in_v.bias'])\n",
    "\n",
    "        q = self.split_head( q )\n",
    "        k = self.split_head( k )\n",
    "        v = self.split_head( v )\n",
    "\n",
    "        # クエリとキーの行列積とアテンションの計算(今回マスクは不使用)\n",
    "        # attnは[バッチサイズ, ヘッド数, 特徴量数, 特徴量数]\n",
    "        attn = q.matmul(k.transpose(-2, -1))\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill_(mask, -torch.finfo(torch.float).max)\n",
    "        attn = (attn * self.scale).softmax(dim=-1)\n",
    "\n",
    "        # アテンションとバリューの行列積によりバリューを収集\n",
    "        # xは[バッチサイズ, ヘッド数, 特徴量数, ヘッドの特徴量次元]\n",
    "        x = attn.matmul(v)\n",
    "\n",
    "        # permute関数により\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数, ヘッドの特徴量次元]\n",
    "        # flatten関数により全てのヘッドから得られる特徴量を連結して、\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数 * ヘッドの特徴量次元]\n",
    "        x = x.permute(0, 2, 1, 3).flatten(2)\n",
    "        #x = self.proj_out(x)\n",
    "        x = F.linear(x, weights['encoder.encoder_layers.' + str(i) + '.attention.proj_out.weight'], weights['encoder.encoder_layers.' + str(i) + '.attention.proj_out.bias'])\n",
    "\n",
    "        return x\n",
    "\n",
    "class fdsMHA(nn.Module):\n",
    "    '''\n",
    "    自己アテンション\n",
    "    dim_hidden: 入力特徴量の次元\n",
    "    num_heads : マルチヘッドアテンションのヘッド数\n",
    "    qkv_bias  : クエリなどを生成する全結合層のバイアスの有無\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 qkv_bias: bool=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # 特徴量を各ヘッドのために分割するので、\n",
    "        # 特徴量次元をヘッド数で割り切れるか検証\n",
    "        assert dim_hidden % num_heads == 0\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # ヘッド毎の特徴量次元\n",
    "        dim_head = dim_hidden // num_heads\n",
    "\n",
    "        # ソフトマックスのスケール値\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        # ヘッド毎にクエリ、キーおよびバリューを生成するための全結合層\n",
    "        #self.proj_in = nn.Linear(\n",
    "        #    dim_hidden, dim_hidden * 3, bias=qkv_bias)\n",
    "\n",
    "        # 各ヘッドから得られた特徴量を一つにまとめる全結合層\n",
    "        #self.proj_out = nn.Linear(dim_hidden, dim_hidden)\n",
    "\n",
    "    def split_head(self, x):\n",
    "        x = torch.tensor_split(x, self.num_heads, dim = 2)\n",
    "        x = torch.stack(x, dim = 1)\n",
    "        return x\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor,  v: torch.Tensor, mask: torch.Tensor, i, weights ):\n",
    "        bs, ns = q.shape[:2]\n",
    "\n",
    "        #qkv = self.proj_in(x)\n",
    "        #print( \"size q:\", q.size() )\n",
    "        #print( \"size weigths q:\", weights['d_layers.' + str(i) + '.crossattn.proj_in_q.weight'].size() )\n",
    "        #print( \"size k:\", k.size() )\n",
    "        #print( \"size weights k:\", weights['d_layers.' + str(i) + '.crossattn.proj_in_k.weight'].size() )\n",
    "        q = F.linear(q, weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_in_q.weight'], weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_in_q.bias'])\n",
    "        k = F.linear(k, weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_in_k.weight'], weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_in_k.bias'])\n",
    "        v = F.linear(v, weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_in_v.weight'], weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_in_v.bias'])\n",
    "\n",
    "        q = self.split_head( q )\n",
    "        k = self.split_head( k )\n",
    "        v = self.split_head( v )\n",
    "\n",
    "        # view関数により\n",
    "        # [バッチサイズ, 特徴量数, QKV, ヘッド数, ヘッドの特徴量次元]\n",
    "        # permute関数により\n",
    "        # [QKV, バッチサイズ, ヘッド数, 特徴量数, ヘッドの特徴量次元]\n",
    "        #qkv = qkv.view(\n",
    "        #    bs, ns, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        # クエリ、キーおよびバリューに分解\n",
    "        #q, k, v = qkv.unbind(0)\n",
    "\n",
    "        # クエリとキーの行列積とアテンションの計算(今回マスクは不使用)\n",
    "        # attnは[バッチサイズ, ヘッド数, 特徴量数, 特徴量数]\n",
    "        attn = q.matmul(k.transpose(-2, -1))\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill_(mask, -torch.finfo(torch.float).max)      \n",
    "        attn = (attn * self.scale).softmax(dim=-1)\n",
    "\n",
    "        # アテンションとバリューの行列積によりバリューを収集\n",
    "        # xは[バッチサイズ, ヘッド数, 特徴量数, ヘッドの特徴量次元]\n",
    "        x = attn.matmul(v)\n",
    "\n",
    "        # permute関数により\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数, ヘッドの特徴量次元]\n",
    "        # flatten関数により全てのヘッドから得られる特徴量を連結して、\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数 * ヘッドの特徴量次元]\n",
    "        x = x.permute(0, 2, 1, 3).flatten(2)\n",
    "        #x = self.proj_out(x)\n",
    "        x = F.linear(x, weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_out.weight'], weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_out.bias'])\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class fdcMHA(nn.Module):\n",
    "    '''\n",
    "    自己アテンション\n",
    "    dim_hidden: 入力特徴量の次元\n",
    "    num_heads : マルチヘッドアテンションのヘッド数\n",
    "    qkv_bias  : クエリなどを生成する全結合層のバイアスの有無\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 qkv_bias: bool=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # 特徴量を各ヘッドのために分割するので、\n",
    "        # 特徴量次元をヘッド数で割り切れるか検証\n",
    "        assert dim_hidden % num_heads == 0\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # ヘッド毎の特徴量次元\n",
    "        dim_head = dim_hidden // num_heads\n",
    "\n",
    "        # ソフトマックスのスケール値\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        # ヘッド毎にクエリ、キーおよびバリューを生成するための全結合層\n",
    "        #self.proj_in = nn.Linear(\n",
    "        #    dim_hidden, dim_hidden * 3, bias=qkv_bias)\n",
    "\n",
    "        # 各ヘッドから得られた特徴量を一つにまとめる全結合層\n",
    "        #self.proj_out = nn.Linear(dim_hidden, dim_hidden)\n",
    "\n",
    "    def split_head(self, x):\n",
    "        x = torch.tensor_split(x, self.num_heads, dim = 2)\n",
    "        x = torch.stack(x, dim = 1)\n",
    "        return x\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor,  v: torch.Tensor, mask: torch.Tensor, i, weights ):\n",
    "        bs, ns = q.shape[:2]\n",
    "\n",
    "        #qkv = self.proj_in(x)\n",
    "        #print( \"size q:\", q.size() )\n",
    "        #print( \"size weigths q:\", weights['d_layers.' + str(i) + '.crossattn.proj_in_q.weight'].size() )\n",
    "        #print( \"size k:\", k.size() )\n",
    "        #print( \"size weights k:\", weights['d_layers.' + str(i) + '.crossattn.proj_in_k.weight'].size() )\n",
    "        q = F.linear(q, weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_in_q.weight'], weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_in_q.bias'])\n",
    "        k = F.linear(k, weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_in_k.weight'], weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_in_k.bias'])\n",
    "        v = F.linear(v, weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_in_v.weight'], weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_in_v.bias'])\n",
    "\n",
    "        q = self.split_head( q )\n",
    "        k = self.split_head( k )\n",
    "        v = self.split_head( v )\n",
    "\n",
    "        # view関数により\n",
    "        # [バッチサイズ, 特徴量数, QKV, ヘッド数, ヘッドの特徴量次元]\n",
    "        # permute関数により\n",
    "        # [QKV, バッチサイズ, ヘッド数, 特徴量数, ヘッドの特徴量次元]\n",
    "        #qkv = qkv.view(\n",
    "        #    bs, ns, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        # クエリ、キーおよびバリューに分解\n",
    "        #q, k, v = qkv.unbind(0)\n",
    "\n",
    "        # クエリとキーの行列積とアテンションの計算(今回マスクは不使用)\n",
    "        # attnは[バッチサイズ, ヘッド数, 特徴量数, 特徴量数]\n",
    "        attn = q.matmul(k.transpose(-2, -1))\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill_(mask, -torch.finfo(torch.float).max)      \n",
    "        attn = (attn * self.scale).softmax(dim=-1)\n",
    "\n",
    "        # アテンションとバリューの行列積によりバリューを収集\n",
    "        # xは[バッチサイズ, ヘッド数, 特徴量数, ヘッドの特徴量次元]\n",
    "        x = attn.matmul(v)\n",
    "\n",
    "        # permute関数により\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数, ヘッドの特徴量次元]\n",
    "        # flatten関数により全てのヘッドから得られる特徴量を連結して、\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数 * ヘッドの特徴量次元]\n",
    "        x = x.permute(0, 2, 1, 3).flatten(2)\n",
    "        #x = self.proj_out(x)\n",
    "        x = F.linear(x, weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_out.weight'], weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_out.bias'])\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53ed6959-9a0e-4aa9-9f27-dfa6875fd4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "    '''\n",
    "    Transformerエンコーダ内の順伝播型ニューラルネットワーク\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    dim_feedforward: 中間特徴量の次元\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, dim_feedforward: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(dim_hidden, dim_feedforward)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, dim_hidden)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fa69cc3-8e7b-4a49-bb37-bfcb117caa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class feFNN(nn.Module):\n",
    "    '''\n",
    "    Transformerエンコーダ内の順伝播型ニューラルネットワーク\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    dim_feedforward: 中間特徴量の次元\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, dim_feedforward: int):\n",
    "        super().__init__()\n",
    "\n",
    "        #self.linear1 = nn.Linear(dim_hidden, dim_feedforward)\n",
    "        #self.linear2 = nn.Linear(dim_feedforward, dim_hidden)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor, i, weights ):\n",
    "        #x = self.linear1(x)\n",
    "        x = F.linear(x, weights['encoder.encoder_layers.' + str(i) + '.fnn.linear1.weight'], weights['encoder.encoder_layers.' + str(i) + '.fnn.linear1.bias'])\n",
    "        x = self.activation(x)\n",
    "        #x = self.linear2(x)\n",
    "        x = F.linear(x, weights['encoder.encoder_layers.' + str(i) + '.fnn.linear2.weight'], weights['encoder.encoder_layers.' + str(i) + '.fnn.linear2.bias'])\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d15bdb0-9771-4199-bd25-8f557bf4229c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fdFNN(nn.Module):\n",
    "    '''\n",
    "    Transformerエンコーダ内の順伝播型ニューラルネットワーク\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    dim_feedforward: 中間特徴量の次元\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, dim_feedforward: int):\n",
    "        super().__init__()\n",
    "\n",
    "        #self.linear1 = nn.Linear(dim_hidden, dim_feedforward)\n",
    "        #self.linear2 = nn.Linear(dim_feedforward, dim_hidden)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor, i, weights ):\n",
    "        #x = self.linear1(x)\n",
    "        x = F.linear(x, weights['decoder.decoder_layers.' + str(i) + '.fnn.linear1.weight'], weights['decoder.decoder_layers.' + str(i) + '.fnn.linear1.bias'])\n",
    "        x = self.activation(x)\n",
    "        #x = self.linear2(x)\n",
    "        x = F.linear(x, weights['decoder.decoder_layers.' + str(i) + '.fnn.linear2.weight'], weights['decoder.decoder_layers.' + str(i) + '.fnn.linear2.bias'])\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a40be501-d70c-49c3-8b74-381680e36c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    '''\n",
    "    Transformerエンコーダ層\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    num_heads      : ヘッド数\n",
    "    dim_feedforward: 中間特徴量の次元\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 dim_feedforward: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = MHA(dim_hidden, num_heads, qkv_bias = True)\n",
    "        self.fnn = FNN(dim_hidden, dim_feedforward)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(dim_hidden)\n",
    "        self.norm2 = nn.LayerNorm(dim_hidden)\n",
    "        \n",
    "        self.dropout = nn.Dropout( dropout )\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor):\n",
    "        ''' B2T\n",
    "        x0 = x\n",
    "        x = self.attention( x, x, x, mask )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x = self.norm1(x)\n",
    "        x1 = x\n",
    "        x = self.fnn( x )\n",
    "        x = self.dropout( x )\n",
    "        x =  x + x1 + x0\n",
    "        x = self.norm2( x )\n",
    "        '''\n",
    "        #pre-LN\n",
    "        x0 = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attention( x, x, x, mask )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x1 = x\n",
    "        x = self.norm2( x )\n",
    "        x = self.fnn( x )\n",
    "        x = self.dropout( x )\n",
    "        x =  x + x1\n",
    "        \n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f35d262a-7d0f-42ce-8cd8-956d800598bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fTransformerEncoderLayer(nn.Module):\n",
    "    '''\n",
    "    Transformerエンコーダ層\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    num_heads      : ヘッド数\n",
    "    dim_feedforward: 中間特徴量の次元\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 dim_feedforward: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fattention = feMHA(dim_hidden, num_heads)\n",
    "        self.fffn = feFNN(dim_hidden, dim_feedforward)\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.dropout = nn.Dropout( dropout )\n",
    "        \n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor, i, weights ):\n",
    "        '''B2T\n",
    "        x0 = x\n",
    "        x = self.fattention( x, x, x, mask, i, weights )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['encoder.encoder_layers.' + str(i) + '.norm1.weight'], bias=weights['encoder.encoder_layers.' + str(i) + '.norm1.bias'], eps=1e-05)\n",
    "        x1 = x\n",
    "        x = self.fffn( x, i, weights ) \n",
    "        x = self.dropout( x )\n",
    "        x = x + x1 + x0\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['encoder.encoder_layers.' + str(i) + '.norm2.weight'], bias=weights['encoder.encoder_layers.' + str(i) + '.norm2.bias'], eps=1e-05)\n",
    "        '''\n",
    "        #pre-LN\n",
    "        x0 = x\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['encoder.encoder_layers.' + str(i) + '.norm1.weight'], bias=weights['encoder.encoder_layers.' + str(i) + '.norm1.bias'], eps=1e-05)\n",
    "        x = self.fattention( x, x, x, mask, i, weights )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x1 = x\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['encoder.encoder_layers.' + str(i) + '.norm2.weight'], bias=weights['encoder.encoder_layers.' + str(i) + '.norm2.bias'], eps=1e-05)\n",
    "        x = self.fffn( x, i, weights ) \n",
    "        x = self.dropout( x )\n",
    "        x = x + x1\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb21075b-7d9f-4a1e-a76e-9d7fd9c68345",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    '''\n",
    "    Transformerエンコーダ層\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    num_heads      : ヘッド数\n",
    "    dim_feedforward: 中間特徴量の次元\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 dim_feedforward: int, dropout: float=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.selfattn = MHA(dim_hidden, num_heads, qkv_bias = True)\n",
    "        self.crossattn = MHA(dim_hidden, num_heads, qkv_bias = True)\n",
    "        self.fnn = FNN(dim_hidden, dim_feedforward)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(dim_hidden)\n",
    "        self.norm2 = nn.LayerNorm(dim_hidden)\n",
    "        self.norm3 = nn.LayerNorm(dim_hidden)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor,  self_mask: torch.Tensor,  cross_mask: torch.Tensor):\n",
    "        '''B2T\n",
    "        x0 = x\n",
    "        x = self.selfattn( x, x, x, self_mask )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x = self.norm1( x )\n",
    "        x1 = x\n",
    "        x = self.crossattn( x, y, y, cross_mask )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x1\n",
    "        x = self.norm2( x )\n",
    "        x2 = x\n",
    "        x = self.fnn( x )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x2 + x0\n",
    "        x = self.norm3( x )\n",
    "        '''\n",
    "        #pre-LN\n",
    "        x0 = x\n",
    "        x = self.norm1( x )\n",
    "        x = self.selfattn( x, x, x, self_mask )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x1 = x\n",
    "        x = self.norm2( x )\n",
    "        x = self.crossattn( x, y, y, cross_mask )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x1\n",
    "        x2 = x\n",
    "        x = self.norm3( x )\n",
    "        x = self.fnn( x )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x2\n",
    "        \n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78cd200d-38ac-4fc4-bb4c-551fb51eb56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fTransformerDecoderLayer(nn.Module):\n",
    "    '''\n",
    "    Transformerエンコーダ層\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    num_heads      : ヘッド数\n",
    "    dim_feedforward: 中間特徴量の次元\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 dim_feedforward: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fselfattn = fdsMHA(dim_hidden, num_heads)\n",
    "        self.fcrossattn = fdcMHA(dim_hidden, num_heads)\n",
    "        self.fffn = fdFNN(dim_hidden, dim_feedforward)\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        #self.norm1 = nn.LayerNorm(dim_hidden)\n",
    "        #self.norm2 = nn.LayerNorm(dim_hidden)\n",
    "        #self.norm3 = nn.LayerNorm(dim_hidden)\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor,  self_mask: torch.Tensor,  cross_mask: torch.Tensor, i, weights):\n",
    "        '''B2T\n",
    "        x0 = x\n",
    "        x = self.fselfattn( x, x, x, self_mask, i, weights )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['decoder.decoder_layers.' + str(i) + '.norm1.weight'], bias=weights['decoder.decoder_layers.' + str(i) + '.norm1.bias'], eps=1e-05)\n",
    "        x1 = x\n",
    "        x = self.fcrossattn( x, y, y, cross_mask, i, weights )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x1\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['decoder.decoder_layers.' + str(i) + '.norm2.weight'], bias=weights['decoder.decoder_layers.' + str(i) + '.norm2.bias'], eps=1e-05)\n",
    "        x2 = x\n",
    "        x = self.fffn( x, i, weights )\n",
    "        x = self.dropout( x ) \n",
    "        x = x + x2 + x0\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['decoder.decoder_layers.' + str(i) + '.norm3.weight'], bias=weights['decoder.decoder_layers.' + str(i) + '.norm3.bias'], eps=1e-05)\n",
    "        '''\n",
    "        #pre-LN\n",
    "        x0 = x\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['decoder.decoder_layers.' + str(i) + '.norm1.weight'], bias=weights['decoder.decoder_layers.' + str(i) + '.norm1.bias'], eps=1e-05)\n",
    "        x = self.fselfattn( x, x, x, self_mask, i, weights )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x1 = x\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['decoder.decoder_layers.' + str(i) + '.norm2.weight'], bias=weights['decoder.decoder_layers.' + str(i) + '.norm2.bias'], eps=1e-05)\n",
    "        x = self.fcrossattn( x, y, y, cross_mask, i, weights )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x1\n",
    "        x2 = x\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['decoder.decoder_layers.' + str(i) + '.norm3.weight'], bias=weights['decoder.decoder_layers.' + str(i) + '.norm3.bias'], eps=1e-05)\n",
    "        x = self.fffn( x, i, weights )\n",
    "        x = self.dropout( x ) \n",
    "        x = x + x2\n",
    "        \n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73e436f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    '''\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    dim_feedforward: FNNにおける中間特徴量の次元\n",
    "    num_heads      : マルチヘッドアテンションのヘッド数\n",
    "    num_layers     : Transformerエンコーダの層数\n",
    "    '''\n",
    "    def __init__(self, text_vocab_size: int, dim_embedding: int, dim_feedforward: int,\n",
    "                 num_heads: int,  num_layers: int, pad_index:int, dropout: float = 0.1 ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 単語埋め込み\n",
    "        self.embed = nn.Embedding(\n",
    "            text_vocab_size, dim_embedding, padding_idx=pad_index)        \n",
    "        \n",
    "        # 位置エンコーディング\n",
    "        #self.pos_enc = PositionalEncoding(dim_embedding)\n",
    "        self.pos_emb = PositionalEmbedding(dim_embedding)\n",
    "        #self.dropout = nn.Dropout( dropout )\n",
    "        \n",
    "        # Transformerエンコーダ層\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(dim_hidden=dim_embedding, num_heads=num_heads, dim_feedforward = dim_feedforward, dropout = dropout )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(dim_embedding)\n",
    "       \n",
    "        self.pad_index = pad_index\n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x           : 入力, [バッチサイズ, 入力チャネル数, 高さ, 幅]\n",
    "    return_embed: 特徴量を返すかロジットを返すかを選択する真偽値\n",
    "    '''\n",
    "    def forward(self, src: torch.Tensor, src_mask: torch.Tensor=None, src_padding_mask: torch.Tensor=None ):\n",
    "\n",
    "        if src_padding_mask is not None and src_mask is None:\n",
    "            mask = src_padding_mask[:,None,:,None]\n",
    "            mask = mask.expand( (-1, self.num_heads, -1, src.size(1)))\n",
    "        elif src_padding_mask is not None and src_mask is not None:\n",
    "            mask1 = src_padding_mask[:,None,:,None]\n",
    "            mask1 = mask1.expand( (-1, self.num_heads, -1, src.size(1)))\n",
    "            mask2 = src_mask[None,None,:,:]\n",
    "            mask2 = mask2.expand( ( src.size(0), self.num_heads, -1, -1 ) )\n",
    "            mask = torch.logical_or(mask1, mask2 )\n",
    "        elif src_padding_mask is None and src_mask is not None:\n",
    "            mask = src_mask[None,None,:,:]\n",
    "            mask = mask.padding( ( src.size(0), src.size(1), -1, -1 ) )\n",
    "        else:\n",
    "            mask = None\n",
    "\n",
    "        x = self.embed( src ) * math.sqrt( self.dim_embedding )\n",
    "\n",
    "        #x = self.pos_enc( x )\n",
    "        position = self.pos_emb( x )\n",
    "        x = x + position\n",
    "        #x = self.dropout( x )\n",
    "        #x = self.norm( x )\n",
    "        \n",
    "        # Transformerエンコーダ層を適用\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer( x, mask )\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4962ca0-f955-4e9a-831a-e3b8b46edc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fTransformerEncoder(nn.Module):\n",
    "    '''\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    dim_feedforward: FNNにおける中間特徴量の次元\n",
    "    num_heads      : マルチヘッドアテンションのヘッド数\n",
    "    num_layers     : Transformerエンコーダの層数\n",
    "    '''\n",
    "    def __init__(self, text_vocab_size: int, dim_embedding: int, dim_feedforward: int,\n",
    "                 num_heads: int,  num_layers: int, pad_idx:int, dropout: float = 0.1 ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 単語埋め込み\n",
    "        #self.embed = nn.Embedding(\n",
    "        #    text_vocab_size, dim_embedding, padding_idx=pad_index)        \n",
    "        \n",
    "        # 位置エンコーディング\n",
    "        #self.pos_enc = PositionalEncoding(dim_embedding)\n",
    "        self.fpos_emb = fPositionalEmbedding(dim_embedding, \"encoder.pos_emb.pos_emb.weight\" )\n",
    "        #self.dropout = nn.Dropout( dropout )\n",
    "        \n",
    "        # Transformerエンコーダ層\n",
    "        self.fenclayer = fTransformerEncoderLayer(dim_hidden=dim_embedding, num_heads=num_heads, dim_feedforward = dim_feedforward, dropout = dropout )\n",
    "        \n",
    "        # ロジットを生成する前のレイヤー正規化と全結合\n",
    "        #self.norm = nn.LayerNorm(dim_embedding)\n",
    "        \n",
    "        self.pad_idx = pad_idx\n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x           : 入力, [バッチサイズ, 入力チャネル数, 高さ, 幅]\n",
    "    return_embed: 特徴量を返すかロジットを返すかを選択する真偽値\n",
    "    '''\n",
    "    def forward(self, src: torch.Tensor, src_mask: torch.Tensor=None, src_padding_mask: torch.Tensor=None, weights = None ):\n",
    "\n",
    "        if src_padding_mask is not None and src_mask is None:\n",
    "            mask = src_padding_mask[:,None,:,None]\n",
    "            mask = mask.expand( (-1, self.num_heads, -1, src.size(1)))\n",
    "        elif src_padding_mask is not None and src_mask is not None:\n",
    "            mask1 = src_padding_mask[:,None,:,None]\n",
    "            mask1 = mask1.expand( (-1, self.num_heads, -1, src.size(1)))\n",
    "            mask2 = src_mask[None,None,:,:]\n",
    "            mask2 = mask2.expand( ( src.size(0), self.num_heads, -1, -1 ) )\n",
    "            mask = torch.logical_or(mask1, mask2 )\n",
    "        elif src_padding_mask is None and src_mask is not None:\n",
    "            mask = src_mask[None,None,:,:]\n",
    "            mask = mask.padding( ( src.size(0), src.size(1), -1, -1 ) )\n",
    "        else:\n",
    "            mask = None\n",
    "\n",
    "        #x = self.embed( src ) * math.sqrt( self.dim_embedding )\n",
    "        x = F.embedding( src, weights['encoder.embed.weight'], padding_idx = self.pad_idx )  * math.sqrt( self.dim_embedding )\n",
    "        \n",
    "        #x = self.pos_enc( x )\n",
    "        position = self.fpos_emb( x, weights )\n",
    "        x = x + position\n",
    "        #x = self.dropout( x )\n",
    "        ##x = self.norm(x)\n",
    "        #x = F.layer_norm(x, (self.dim_embedding,), weight=weights['encoder.norm.weight'], bias=weights['encoder.norm.bias'], eps=1e-05)\n",
    "    \n",
    "        # Transformerエンコーダ層を適用\n",
    "        #for layer in self.encoder_layers:\n",
    "        #    x = layer( x, mask )\n",
    "        for block in range( self.num_layers ):\n",
    "            x = self.fenclayer(x, mask, block, weights )\n",
    "\n",
    "        #x = self.norm(x)\n",
    "        x = F.layer_norm(x, (self.dim_embedding,), weight=weights['encoder.norm.weight'], bias=weights['encoder.norm.bias'], eps=1e-05)\n",
    "        \n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59e7cd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    '''\n",
    "    CaptioningTransformerのコンストラクタ\n",
    "    dim_embedding  : 埋め込み次元\n",
    "    dim_feedforward: FNNの中間特徴次元\n",
    "    num_heads      : マルチヘッドアテンションのヘッド数\n",
    "    num_layers     : Transformerデコーダ層の数\n",
    "    vocab_size     : 辞書の次元\n",
    "    pad_index     : NULLのID\n",
    "    dropout        : ドロップアウト確率\n",
    "    '''\n",
    "    def __init__(self, vocab_size: int, dim_embedding: int, dim_feedforward: int,\n",
    "                 num_heads: int, num_layers: int, \n",
    "                 pad_index: int, dropout: float=0.1, ds_rate: float=0.1 ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 単語埋め込み\n",
    "        self.embed = nn.Embedding(\n",
    "            vocab_size, dim_embedding, padding_idx=pad_index)\n",
    "        \n",
    "        # 位置エンコーディング\n",
    "        #self.pos_enc = PositionalEncoding(dim_embedding)\n",
    "        self.pos_emb = PositionalEmbedding(dim_embedding)\n",
    "        #self.dropout = nn.Dropout( dropout )\n",
    "\n",
    "        # Transformerデコーダ\n",
    "        #self.decoder_layers = nn.ModuleList([\n",
    "        #    TransformerDecoderLayer(\n",
    "        #        dim_embedding, num_heads, dim_feedforward, dropout)\n",
    "        #    for _ in range(num_layers)\n",
    "        #])\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer(dim_hidden=dim_embedding, num_heads=num_heads, dim_feedforward = dim_feedforward, dropout = dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # ロジットを生成する前のレイヤー正規化と全結合\n",
    "        #self.norm = nn.LayerNorm(dim_embedding)\n",
    "        \n",
    "        # 単語出力分布計算\n",
    "        self.linear = nn.Linear(dim_embedding, vocab_size)\n",
    "        \n",
    "        self.pad_index = pad_index\n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "\n",
    "\n",
    "    ''' CaptioningTransformerの順伝播処理\n",
    "    features: 画像特徴量 [バッチサイズ, 埋め込み次元]\n",
    "    captions: 正解キャプション [バッチサイズ, 系列長]\n",
    "\n",
    "    '''\n",
    "    #def forward(self, features: torch.Tensor, caption_lengths: torch.Tensor):\n",
    "    #def forward(self, features: torch.Tensor, captions: torch.Tensor, padding_mask_src: torch.Tensor=None, \\\n",
    "    #            padding_mask_tgt: torch.Tensor=None, mask_tgt: torch.Tensor=None ):\n",
    "    def forward(self, features: torch.Tensor, captions: torch.Tensor, memory_padding_mask: torch.Tensor=None, \\\n",
    "                tgt_padding_mask: torch.Tensor=None, tgt_mask: torch.Tensor=None ):\n",
    "\n",
    "        #feature_lengths = torch.ones( (features.size(0) ), device=features.device ) * features.size(1)\n",
    "\n",
    "        tgt = captions\n",
    "        if tgt_padding_mask is not None and tgt_mask is not None:\n",
    "            self_mask1 = tgt_padding_mask[:,None,:,None]\n",
    "            self_mask1 = self_mask1.expand( (-1, self.num_heads, -1, tgt.size(1)))\n",
    "            #print( \"self_mask1:\", self_mask1.size() )\n",
    "            self_mask2 = tgt_mask[None,None,:,:]\n",
    "            self_mask2 = self_mask2.expand( (tgt.size(0), self.num_heads, -1, -1 ))\n",
    "            #print( \"self_mask2:\", self_mask2.size() )\n",
    "            self_mask = torch.logical_or(self_mask1, self_mask2 )\n",
    "            #print( \"0 self_mask:\", self_mask.size() )\n",
    "        elif tgt_padding_mask is not None and tgt_mask is None:\n",
    "            self_mask = tgt_padding_mask[:,None,:,None]\n",
    "            self_mask = self_mask.expand( (-1, self.num_heads, -1, tgt.size(1)))\n",
    "        elif tgt_padding_mask is None and tgt_mask is not None:\n",
    "            #print(\"tgt_padding_mask is None\")\n",
    "            self_mask = tgt_mask[None,None,:,:]\n",
    "            self_mask = self_mask.expand( (tgt.size(0), self.num_heads, -1, -1 ))\n",
    "            #print( \"0 self_mask size():\", self_mask.size() )\n",
    "        elif tgt_padding_mask is None and tgt_mask is None:\n",
    "            self_mask = None\n",
    "            \n",
    "        if memory_padding_mask is not None:\n",
    "            cross_mask = memory_padding_mask[:,None,None,:]\n",
    "            cross_mask = cross_mask.expand((-1,self.num_heads, tgt.size(1), -1))\n",
    "        else:\n",
    "            cross_mask = None\n",
    "        \n",
    "        # 単語埋め込み [バッチサイズ, 系列長]\n",
    "        # -> [バッチサイズ, 系列長, 埋め込み次元]\n",
    "        embeddings = self.embed(captions) * math.sqrt( self.dim_embedding )\n",
    "        seq = embeddings.shape[1]\n",
    "        \n",
    "        # 位置エンコーディング\n",
    "        #embeddings = self.pos_enc( embeddings )\n",
    "        positions = self.pos_emb(embeddings)\n",
    "        embeddings = embeddings + positions\n",
    "        #embeddings = self.dropout( embeddings )\n",
    "        #embeddings = self.norm(embeddings)\n",
    "        \n",
    "        # Transformerデコーダでキャプション生成\n",
    "        # 画像の特徴も入力する\n",
    "        for layer in self.decoder_layers:\n",
    "            embeddings = layer( embeddings, features, self_mask, cross_mask )\n",
    "\n",
    "        \n",
    "        # [バッチサイズ, 系列長, 埋め込み次元]\n",
    "        # -> [バッチサイズ, 系列長, 辞書の次元]\n",
    "        preds = self.linear(embeddings)\n",
    "        #print( \"argmax of preds:\", torch.argmax( preds, dim = 2 ))\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9722fd0-0e06-4fba-a794-562641fbbe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fTransformerDecoder(nn.Module):\n",
    "    '''\n",
    "    CaptioningTransformerのコンストラクタ\n",
    "    dim_embedding  : 埋め込み次元\n",
    "    dim_feedforward: FNNの中間特徴次元\n",
    "    num_heads      : マルチヘッドアテンションのヘッド数\n",
    "    num_layers     : Transformerデコーダ層の数\n",
    "    vocab_size     : 辞書の次元\n",
    "    pad_index     : NULLのID\n",
    "    dropout        : ドロップアウト確率\n",
    "    '''\n",
    "    def __init__(self, vocab_size: int, dim_embedding: int, dim_feedforward: int,\n",
    "                 num_heads: int, num_layers: int, \n",
    "                 pad_idx: int, dropout: float=0.1, ds_rate: float=0.1 ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 単語埋め込み\n",
    "        #self.embed = nn.Embedding(\n",
    "        #    vocab_size, dim_embedding, padding_idx=pad_index)\n",
    "        \n",
    "        # 位置エンコーディング\n",
    "        #self.pos_enc = PositionalEncoding(dim_embedding)\n",
    "        self.fpos_emb = fPositionalEmbedding(dim_embedding, \"decoder.pos_emb.pos_emb.weight\" )\n",
    "        #self.dropout = nn.Dropout( dropout )\n",
    "        \n",
    "        # Transformerデコーダ\n",
    "        self.fdeclayer = fTransformerDecoderLayer(dim_hidden=dim_embedding, num_heads=num_heads, dim_feedforward = dim_feedforward, dropout = dropout )\n",
    "        \n",
    "        # 単語出力分布計算\n",
    "        #self.linear = nn.Linear(dim_embedding, vocab_size)\n",
    "        \n",
    "        self.pad_idx = pad_idx\n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    ''' CaptioningTransformerの順伝播処理\n",
    "    features: 画像特徴量 [バッチサイズ, 埋め込み次元]\n",
    "    captions: 正解キャプション [バッチサイズ, 系列長]\n",
    "\n",
    "    '''\n",
    "    def forward(self, features: torch.Tensor, captions: torch.Tensor, memory_padding_mask: torch.Tensor=None, \\\n",
    "                tgt_padding_mask: torch.Tensor=None, tgt_mask: torch.Tensor=None, weights = None ):\n",
    "\n",
    "        tgt = captions\n",
    "        if tgt_padding_mask is not None and tgt_mask is not None:\n",
    "            self_mask1 = tgt_padding_mask[:,None,:,None]\n",
    "            self_mask1 = self_mask1.expand( (-1, self.num_heads, -1, tgt.size(1)))\n",
    "            #print( \"self_mask1:\", self_mask1.size() )\n",
    "            self_mask2 = tgt_mask[None,None,:,:]\n",
    "            self_mask2 = self_mask2.expand( (tgt.size(0), self.num_heads, -1, -1 ))\n",
    "            #print( \"self_mask2:\", self_mask2.size() )\n",
    "            self_mask = torch.logical_or(self_mask1, self_mask2 )\n",
    "            #print( \"0 self_mask:\", self_mask.size() )\n",
    "        elif tgt_padding_mask is not None and tgt_mask is None:\n",
    "            self_mask = tgt_padding_mask[:,None,:,None]\n",
    "            self_mask = self_mask.expand( (-1, self.num_heads, -1, tgt.size(1)))\n",
    "        elif tgt_padding_mask is None and tgt_mask is not None:\n",
    "            #print(\"tgt_padding_mask is None\")\n",
    "            self_mask = tgt_mask[None,None,:,:]\n",
    "            self_mask = self_mask.expand( (tgt.size(0), self.num_heads, -1, -1 ))\n",
    "            #print( \"0 self_mask size():\", self_mask.size() )\n",
    "        elif tgt_padding_mask is None and tgt_mask is None:\n",
    "            self_mask = None\n",
    "            \n",
    "        if memory_padding_mask is not None:\n",
    "            cross_mask = memory_padding_mask[:,None,None,:]\n",
    "            cross_mask = cross_mask.expand((-1,self.num_heads, tgt.size(1), -1))\n",
    "        else:\n",
    "            cross_mask = None\n",
    "        \n",
    "        # 単語埋め込み [バッチサイズ, 系列長]\n",
    "        # -> [バッチサイズ, 系列長, 埋め込み次元]\n",
    "        embeddings = F.embedding( captions, weights['decoder.embed.weight'], padding_idx = self.pad_idx )  * math.sqrt( self.dim_embedding )\n",
    "        seq = embeddings.shape[1]\n",
    "        \n",
    "        # 位置エンコーディング\n",
    "        #embeddings = self.pos_enc(embeddings)\n",
    "        positions = self.fpos_emb( embeddings, weights )\n",
    "        embeddings = embeddings + positions\n",
    "        #embeddings = self.dropout( embeddings )\n",
    "        ##embeddings = self.norm(embeddings)\n",
    "        #embeddings = F.layer_norm(embeddings, (self.dim_embedding,), weight=weights['decoder.norm.weight'], bias=weights['decoder.norm.bias'], eps=1e-05)\n",
    "        \n",
    "        # Transformerデコーダでキャプション生成\n",
    "        # 画像の特徴も入力する\n",
    "        #for layer in self.decoder_layers:\n",
    "        #    #embeddings = layer( embeddings, features, tgt_key_padding_mask = padding_mask_tgt, \\\n",
    "        #    #                                memory_key_padding_mask = padding_mask_src, tgt_is_causal = True, tgt_mask = mask_tgt )\n",
    "        for block in range( self.num_layers ):\n",
    "            embeddings = self.fdeclayer(embeddings, features, self_mask, cross_mask, block, weights )\n",
    "  \n",
    "        # [バッチサイズ, 系列長, 埋め込み次元]\n",
    "        # -> [バッチサイズ, 系列長, 辞書の次元]\n",
    "        #preds = self.linear(embeddings)\n",
    "        preds = F.linear( embeddings, weights['decoder.linear.weight'], weights['decoder.linear.bias'] )\n",
    "        #print( \"argmax of preds:\", torch.argmax( preds, dim = 2 ))\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c47496bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim_embedding: int, dim_feedforward: int,\n",
    "                 num_heads: int, num_layers: int, enc_vocab_size: int, dec_vocab_size: int,\n",
    "                 j_pad_index: int,e_pad_index: int, dropout: float=0.1, ds_rate: float=0.1 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = TransformerEncoder(enc_vocab_size, dim_embedding, dim_feedforward, num_heads, num_layers, j_pad_index, dropout )\n",
    "        self.decoder = TransformerDecoder(dec_vocab_size, dim_embedding, dim_feedforward, num_heads, num_layers, e_pad_index, dropout )\n",
    "        self.fencoder = fTransformerEncoder(enc_vocab_size, dim_embedding, dim_feedforward, num_heads, num_layers, j_pad_index, dropout )\n",
    "        self.fdecoder = fTransformerDecoder(dec_vocab_size, dim_embedding, dim_feedforward, num_heads, num_layers, e_pad_index, dropout )\n",
    "        self.j_pad_index = j_pad_index\n",
    "        self.e_pad_index = e_pad_index\n",
    "\n",
    "        self._reset_parameters()\n",
    "        #self._reset_parameters2()\n",
    "    \n",
    "    def _reset_parameters(self):\n",
    "        r\"\"\"Initiate parameters in the transformer model.\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                xavier_uniform_(p)\n",
    "                #xavier_normal_(p)\n",
    "                #kaiming_uniform_(p)\n",
    "                #kaiming_normal_(p)\n",
    "\n",
    "    #def _reset_parameters2(self):\n",
    "    #    for module in self.modules():\n",
    "    #        if isinstance(module, nn.Linear):\n",
    "    #            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    #            if module.bias is not None:\n",
    "    #                nn.init.zeros_(module.bias)\n",
    "    #        elif isinstance(module, nn.Embedding):\n",
    "    #            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    #        elif isinstance(module, nn.LayerNorm):\n",
    "    #            nn.init.zeros_(module.bias)\n",
    "    #            nn.init.ones_(module.weight)\n",
    "        \n",
    "    ''' CaptioningTransformerの順伝播処理\n",
    "    features: 画像特徴量 [バッチサイズ, 埋め込み次元]\n",
    "    captions: 正解キャプション [バッチサイズ, 系列長]\n",
    "\n",
    "    '''\n",
    "    def forward(self, text, dec_input):\n",
    "\n",
    "        seq_len_src = text.shape[1]\n",
    "        seq_len_tgt = dec_input.shape[1]\n",
    "\n",
    "        mask_tgt = nn.Transformer.generate_square_subsequent_mask( seq_len_tgt, dtype=bool ).to(device)\n",
    "        mask_src = torch.zeros((seq_len_src, seq_len_src), device=device).type(torch.bool)\n",
    "\n",
    "        padding_mask_src = (text == idx_list['<pad>'])\n",
    "        padding_mask_tgt = (dec_input == idx_list_en['<pad>'])\n",
    "    \n",
    "        x = self.encoder( text, mask_src, padding_mask_src )\n",
    "        preds = self.decoder(x,dec_input, padding_mask_src, padding_mask_tgt, mask_tgt )\n",
    "\n",
    "        return preds\n",
    "    \n",
    "    def adaptation(self, text, dec_input, weights):\n",
    "\n",
    "        seq_len_src = text.shape[1]\n",
    "        seq_len_tgt = dec_input.shape[1]\n",
    "\n",
    "        mask_tgt = nn.Transformer.generate_square_subsequent_mask( seq_len_tgt, dtype=bool ).to(device)\n",
    "        mask_src = torch.zeros((seq_len_src, seq_len_src), device=device).type(torch.bool)\n",
    "\n",
    "        padding_mask_src = (text == idx_list['<pad>']).to(text.device )\n",
    "        padding_mask_tgt = (dec_input == idx_list_en['<pad>']).to(text.device )\n",
    "        \n",
    "        x = self.fencoder( text, mask_src, padding_mask_src, weights)\n",
    "        preds = self.fdecoder( x, dec_input, padding_mask_src, padding_mask_tgt, mask_tgt, weights )\n",
    "        \n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "99b82dac-b490-49c2-a099-52b962fe0ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def inference(input_sequence: torch.Tensor, weights, max_length: int=30):\n",
    "\n",
    "    bs = input_sequence.size(0)\n",
    "    \n",
    "    seq_len_src = input_sequence.shape[1]\n",
    "\n",
    "    padding_mask_src = (input_sequence == idx_list['<pad>'])\n",
    "\n",
    "    #model.eval()\n",
    "    enc_out = model.fencoder( input_sequence, None, padding_mask_src, weights )\n",
    "    \n",
    "    # <start> トークンで出力キャプションを初期化\n",
    "    captions = input_sequence.new_full(\n",
    "        (bs, 1), idx_list_en['<sos>'], dtype=torch.int64)\n",
    "\n",
    "    #print( \"enc_out:\", enc_out )\n",
    "    # 単語を逐次予測\n",
    "    for _ in range(max_length):\n",
    "        mask_tgt = nn.Transformer.generate_square_subsequent_mask( captions.size(1), dtype=bool ).to(device)    \n",
    "        padding_mask_tgt = (captions == idx_list_en['<pad>']).bool()\n",
    "        #print( \"captions:\",captions)\n",
    "        #preds = model.decoder(enc_out, captions, causal_mask = None, dec_padding_mask = dec_padding_mask  )\n",
    "        #model.eval()\n",
    "        #preds = model.fdecoder(enc_out, captions, padding_mask_src, padding_mask_tgt, mask_tgt, weights )    \n",
    "        #preds = model.fdecoder(enc_out, captions, padding_mask_src, tgt_padding_mask=padding_mask_tgt, tgt_mask = None, weights = weights )\n",
    "        preds = model.fdecoder(enc_out, captions, padding_mask_src, tgt_padding_mask=padding_mask_tgt, tgt_mask = mask_tgt, weights = weights )\n",
    "        #preds = model.decoder(enc_out, captions, causal_mask = None, dec_padding_mask = None  )\n",
    "        preds = preds[:, -1]\n",
    "        preds = preds.argmax(dim=1)\n",
    "        #print(\"preds:\",preds)\n",
    "        words = torch.unsqueeze( preds, dim = 1 )\n",
    "        \n",
    "        captions = torch.cat((captions, words), dim=1)\n",
    "\n",
    "    return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "49856487-be45-4f42-8a72-33a95ab3577f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as tud\n",
    "\n",
    "# 推論モジュール\n",
    "@torch.no_grad()\n",
    "def beam_search(input_sequence, weights):\n",
    "    ''' ネットワーク計算(forward処理)の関数\n",
    "    input_sequence: 各発話の入力系列 [B x Tin x D]\n",
    "    input_lengths:  各発話の系列長(フレーム数) [B]\n",
    "        []の中はテンソルのサイズ\n",
    "        B:    ミニバッチ内の発話数(ミニバッチサイズ)\n",
    "        Tin:  入力テンソルの系列長(ゼロ埋め部分含む)\n",
    "        D:    入力次元数(dim_in)\n",
    "        Tout: 正解ラベル系列の系列長(ゼロ埋め部分含む)\n",
    "    '''\n",
    "    predictions = 20\n",
    "    beam_width = 5\n",
    "    batch_size = input_sequence.size(0)\n",
    "        \n",
    "    enc_mask = input_sequence.eq(idx_list['<pad>']).to(device)\n",
    "    dec_mask = enc_mask\n",
    "        \n",
    "    # エンコーダに入力する\n",
    "    #model.eval()\n",
    "    enc_out = model.fencoder(input_sequence, None, enc_mask, weights)\n",
    "        \n",
    "    #enc_lengths = input_lengths\n",
    "        \n",
    "    X = torch.ones( (enc_out.size(0), 1 ), dtype=torch.int64  ) * idx_list_en['<sos>']\n",
    "\n",
    "    dec_target_mask = X.eq(idx_list_en['<pad>']).to(device)\n",
    "    n_ctx = X.size(1)\n",
    "    causal_mask = torch.empty(n_ctx, n_ctx).fill_(1).triu_(1).to(device).bool()\n",
    "\n",
    "    #dec_out= model.decoder( enc_out, X, dec_mask, dec_target_mask, causal_mask )\n",
    "    #preds = model.decoder(enc_out, captions, padding_mask_src, None, mask_tgt )   \n",
    "    #logits = self.classifier( dec_out )\n",
    "    #model.eval()\n",
    "    logits= model.fdecoder( enc_out, X, dec_mask, dec_target_mask, causal_mask, weights )\n",
    "    next_probabilities = logits[:, -1, :]\n",
    "    vocabulary_size = next_probabilities.shape[-1]\n",
    "    probabilities, idx = next_probabilities.squeeze().log_softmax(-1)\\\n",
    "        .topk(k = beam_width, axis = -1)\n",
    "    X = X.repeat((beam_width, 1, 1)).transpose(0, 1)\\\n",
    "        .flatten(end_dim = -2)\n",
    "    next_chars = idx.reshape(-1, 1)\n",
    "    X = torch.cat((X, next_chars), axis = -1)\n",
    "    # This has to be minus one because we already produced a round\n",
    "    # of predictions before the for loop.\n",
    "    predictions_iterator = range(predictions - 1)\n",
    "\n",
    "    for i in predictions_iterator:\n",
    "        dataset = tud.TensorDataset(X)\n",
    "        loader = tud.DataLoader(dataset, batch_size = batch_size)\n",
    "        next_probabilities = []\n",
    "        iterator = iter(loader)\n",
    "        for (x,) in iterator:\n",
    "            dec_target_mask = x.eq(idx_list_en['<pad>']).to(device)\n",
    "            n_ctx = x.size(1)\n",
    "            causal_mask = torch.empty(n_ctx, n_ctx).fill_(1).triu_(1).to(device).bool()\n",
    "            #dec_out= model.decoder( enc_out, x, dec_mask, dec_target_mask, causal_mask )\n",
    "            #logits = self.classifier( dec_out )\n",
    "            #model.eval()\n",
    "            #logits = model.fdecoder( enc_out, x, dec_mask, dec_target_mask, causal_mask, weights )\n",
    "            logits = model.fdecoder(enc_out, captions, padding_mask_src, tgt_padding_mask=dec_target_mask, tgt_mask = causal_mask, weights = weights )\n",
    "            next_probabilities0 = logits[:, -1, :].log_softmax(-1)\n",
    "            next_probabilities.append(\n",
    "                next_probabilities0\n",
    "            )\n",
    "        next_probabilities = torch.cat(next_probabilities, axis = 0)\n",
    "        next_probabilities = next_probabilities.reshape(\n",
    "            (-1, beam_width, next_probabilities.shape[-1])\n",
    "        )\n",
    "        probabilities = probabilities.unsqueeze(-1) + next_probabilities\n",
    "        probabilities = probabilities.flatten(start_dim = 1)\n",
    "        probabilities, idx = probabilities.topk(\n",
    "            k = beam_width, \n",
    "            axis = -1\n",
    "        )\n",
    "        next_chars = torch.remainder(idx, vocabulary_size).flatten()\\\n",
    "            .unsqueeze(-1)\n",
    "        best_candidates = (idx / vocabulary_size).long()\n",
    "        best_candidates += torch.arange(\n",
    "            X.shape[0] // beam_width, \n",
    "            device = X.deviceoptimizer\n",
    "        ).unsqueeze(-1) * beam_width\n",
    "        X = X[best_candidates].flatten(end_dim = -2)\n",
    "        X = torch.cat((X, next_chars), axis = 1)\n",
    "        \n",
    "        #print( \"size of X:{}\", X.size() )\n",
    "        \n",
    "    output = X.reshape( input_sequence.size(0), beam_width, -1 )\n",
    "    return output[:,0,:]\n",
    "    #return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "8019b3a4-8e5c-4777-bda5-5d005b990bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuning(nn.Module):\n",
    "    def __init__(self, model, loss_fn, device, id_to_word ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.device = device\n",
    "        self.id_to_word = id_to_word\n",
    "    \n",
    "    def forward(self, batch, train_step, lr1, display ):\n",
    "\n",
    "        x_train = torch.tensor( np.array( batch[0] ) ) #support tokeinzer.encode text データ\n",
    "        x_len_train = torch.tensor( batch[1] ) #support attention mask データ\n",
    "        y_train = torch.tensor( np.array( batch[2] ) ) #support ラベルデータ\n",
    "        y_len_train = torch.tensor( batch[3] ) #support attention mask データ\n",
    "        x_val = torch.tensor( np.array( batch[4] ) )   #query  tokeinzer.encode text データ\n",
    "        x_len_val = torch.tensor( batch[5] )   #query attention mask データ\n",
    "        y_val = torch.tensor( np.array( batch[6] )  )  #query ラベルデータ\n",
    "        y_len_val = torch.tensor( batch[7] )   #query attention mask データ\n",
    "\n",
    "        num_task = len( x_train )\n",
    "\n",
    "        outer_loss = 0\n",
    "        total_error = 0\n",
    "        total_token_length = 0\n",
    "\n",
    "        for idx in range(x_train.size(0)): # task\n",
    "            fast_weights = OrderedDict(self.model.named_parameters())#今回の基準パラメータ, \n",
    "                                # for 文の中だが、fast_weightsは更新されるが、model.parameter は変わらない。\n",
    "            # batch 抽出\n",
    "            input_x_len = x_len_train[idx]\n",
    "            input_y_len = y_len_train[idx]        \n",
    "            input_x = x_train[idx][:,:max(input_x_len)].to(self.device)\n",
    "            input_y = y_train[idx][:,:max(input_y_len)].to(self.device).long()\n",
    "            input_y1 = input_y[:,:-1]\n",
    "            input_y2 = input_y[:,1:]\n",
    "\n",
    "            if display:\n",
    "                print('----Task',idx, '----')\n",
    "\n",
    "            # タスクごとの損失の計算\n",
    "            loss_item = 0\n",
    "            for iter in range(train_step): # train_step のループ\n",
    "                # 未来のキャプションを参照しないようにマスク行列を生成\n",
    "                self.model.eval()\n",
    "                logits = self.model.adaptation(input_x, input_y1, fast_weights )\n",
    "                loss0 = self.loss_fn(logits.transpose(1,2), input_y2  )\n",
    "                loss_item += loss0.item()\n",
    "                gradients = torch.autograd.grad(loss0, fast_weights.values())\n",
    "                fast_weights = OrderedDict((name, param - lr1 * grad) for ((name, param),  grad) in zip(fast_weights.items(), gradients))\n",
    "\n",
    "                del logits, gradients, loss0\n",
    "                torch.cuda.empty_cache()  \n",
    "\n",
    "            loss_item = loss_item / train_step \n",
    "\n",
    "            if display:\n",
    "                print(\"Inner Loss: \", loss_item)\n",
    "\n",
    "            del loss_item, input_x, input_y, input_y1, input_y2, input_x_len, input_y_len\n",
    "            torch.cuda.empty_cache()  \n",
    "        \n",
    "            # query データからバッチ抽出\n",
    "            input_x_len = x_len_val[idx]\n",
    "            input_y_len = y_len_val[idx]        \n",
    "            input_x = x_val[idx][:,:max(input_x_len)].to(self.device)\n",
    "            input_y = y_val[idx][:,:max(input_y_len)].to(self.device).long()\n",
    "        \n",
    "            # 訓練時に query データ（inner_batch = 1, query_k * 2 クラス )　で二番目の損失関数の各タスクについての総和を求める。\n",
    "            #if train:\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                self.model.eval()\n",
    "                preds = inference(input_x, fast_weights )\n",
    "                #preds = beam_search( input_x, fast_weights )\n",
    "\n",
    "            # 更新したモデルパラメーターを用いて、損失と精度を求める。\n",
    "\n",
    "            pred_label_id = preds\n",
    "            #print( \"pred_label_id[0]:\", pred_label_id[0] )\n",
    "            predict = []\n",
    "            target = []\n",
    "            for i0, (pred, orig) in enumerate( zip( pred_label_id, input_y )):\n",
    "                hypo = []\n",
    "                for m in pred:\n",
    "                    hypo.append(self.id_to_word[m.item()])\n",
    "                    if self.id_to_word[m.item()] == '<eos>':\n",
    "                        break\n",
    "                predict.append( hypo )\n",
    "                reference = []\n",
    "                for m in orig:\n",
    "                    reference.append(self.id_to_word[m.item()])\n",
    "                    if self.id_to_word[m.item()] == '<eos>':\n",
    "                        break\n",
    "                target.append( reference )    \n",
    "                   # 認識誤りを計算\n",
    "                (error, substitute, \n",
    "                    delete, insert, ref_length) = \\\n",
    "                       levenshtein.calculate_error(hypo,\n",
    "                                               reference)\n",
    "                    \n",
    "                # 誤り文字数を累積する\n",
    "                total_error += error\n",
    "                # 文字の総数を累積する\n",
    "                total_token_length += ref_length                \n",
    "            if True:\n",
    "                #print( \"pred_label_id:\", pred_label_id )\n",
    "                for i4, (pred, tar) in enumerate( zip( predict, target ) ):\n",
    "                    if i4 >= 2:\n",
    "                        break\n",
    "                    if display:\n",
    "                        print( \"test pred:\", ' '.join(pred) )\n",
    "                        print( \"test targ:\", ' '.join(tar)  )\n",
    "\n",
    "            del input_x, input_y, input_x_len, input_y_len, fast_weights\n",
    "            torch.cuda.empty_cache()  \n",
    "        \n",
    "        avg_error = total_error / total_token_length\n",
    "\n",
    "        return avg_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "88539768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device( \"cpu\")\n",
    "epoch_num = 3\n",
    "dim_hidden = 768\n",
    "dim_feedforward = dim_hidden * 4\n",
    "heads = 12\n",
    "layers =8\n",
    "dropout = 0.1\n",
    "derivative_change_rate = 0.01\n",
    "#dim_hidden = 256\n",
    "#dim_feedforward = dim_hidden * 4\n",
    "#heads = 4\n",
    "#layers =4\n",
    "#dropout = 0.0\n",
    "model = Transformer(dim_hidden, dim_feedforward, heads, layers, enc_vocab_size, dec_vocab_size, idx_list['<pad>'], idx_list_en['<pad>'], dropout = dropout ).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=idx_list_en['<pad>'])\n",
    "loss_fn = criterion\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "#optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "fine_tuning = FineTuning( model = model, loss_fn = loss_fn, device = device, id_to_word = token_list_en )\n",
    "# 全ステップ数\n",
    "#num_global_steps = len( ob_train ) * epoch_num\n",
    "#print( \"num_global_steps:\", num_global_steps )\n",
    "#num_warmup_steps = num_global_steps * 0.1\n",
    "#print( \"num_warmup_steps:\", num_warmup_steps )\n",
    "#derivative_change_step = num_global_steps * derivative_change_rate\n",
    "#print( \"derivative_change_step:\", derivative_change_step )\n",
    "#スケジューラーの定義\n",
    "#scheduler = get_linear_schedule_with_warmup( optimizer, num_warmup_steps, num_global_steps )\n",
    "#use_param_file = False\n",
    "PATH = \"./model_NTT_auto_curr3.pt\"\n",
    "print_coef = 1000\n",
    "save_file_coef = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "763ea208-7ed2-4b7b-9ccf-0228c3aa86b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs1 size: torch.Size([8, 120, 20556])\n",
      "tensor(-0.1181, grad_fn=<SelectBackward0>)\n",
      "outputs2 size: torch.Size([8, 120, 20556])\n",
      "tensor(-0.1181, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#seed = 0\n",
    "\n",
    "#random.seed(seed)\n",
    "#np.random.seed(seed)\n",
    "#torch.manual_seed(seed)\n",
    "#torch.backends.cudnn.benchmark = False\n",
    "#torch.backends.cudnn.deterministic = True\n",
    "\n",
    "text = torch.randint( 0, enc_vocab_size, size=(8, 100 ))\n",
    "dec_input = torch.randint( 0, dec_vocab_size, size=(1,120))\n",
    "\n",
    "seq_len_src = text.shape[1]\n",
    "seq_len_tgt = dec_input.shape[1]\n",
    "\n",
    "mask_tgt = nn.Transformer.generate_square_subsequent_mask( seq_len_tgt, dtype=bool ).to(device)\n",
    "mask_src = torch.zeros((seq_len_src, seq_len_src), device=device).type(torch.bool)\n",
    "\n",
    "padding_mask_src = (text == idx_list['<pad>'])\n",
    "padding_mask_tgt = (dec_input == idx_list_en['<pad>'])\n",
    "\n",
    "weights = OrderedDict(model.named_parameters())\n",
    "\n",
    "model.eval()\n",
    "outputs1 = model( text.to(device), dec_input.to(device) )\n",
    "#outputs1 = model.encoder( text.to(device), None, None )\n",
    "#outputs1 = model.encoder( text.to(device), dec_input None, None )\n",
    "print( \"outputs1 size:\", outputs1.size())\n",
    "print( outputs1[0][0][0])\n",
    "\n",
    "#for name in weights:\n",
    "#    print( name )\n",
    "model.eval()\n",
    "outputs2 = model.adaptation( text.to(device), dec_input.to(device), weights )\n",
    "#outputs2 = model.fencoder( text.to(device), None, None,  weights )\n",
    "print( \"outputs2 size:\", outputs2.size() )\n",
    "print( outputs2[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "0f4215f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# モデルの読み込み\n",
    "\n",
    "print( device )\n",
    "\n",
    "#if torch.cuda.is_available():\n",
    "if device != torch.device(\"cpu\"):\n",
    "    checkpoint = torch.load(PATH)\n",
    "else:\n",
    "    checkpoint = torch.load(PATH, map_location=torch.device('cpu'))\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device( \"cpu\")\n",
    "\n",
    "# optimizerのstateを現在のdeviceに移す。これをしないと、保存前後でdeviceの不整合が起こる可能性がある。\n",
    "#for state in optimizer.state.values():\n",
    "#    for k, v in state.items():\n",
    "#        if isinstance(v, torch.Tensor):\n",
    "#            state[k] = v.to(device)\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with open('history_train_NTT_auto_curr3.pkl', 'rb') as f:\n",
    "    history_train = pickle.load(f)\n",
    "\n",
    "with open('history_val_NTT_auto_curr3.pkl', 'rb') as f:\n",
    "    history_val = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "f7db97d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#x = []\n",
    "#print( len( history[\"train_loss\"] ))\n",
    "#for i in range( len( history[\"train_loss\"] )):\n",
    "#    x.append( i )\n",
    "    \n",
    "#v_x = []\n",
    "#print( len( history[\"val_loss\"]) )\n",
    "#for i in range( len( history[\"val_loss\"]) ):\n",
    "#    v_x.append( (i )  * 100 )\n",
    "\n",
    "#print( history_train )\n",
    "#print( history_val )\n",
    "\n",
    "for i, step in enumerate( history_val[\"step\"] ):\n",
    "    step = step\n",
    "    history_val[\"step\"][i] = step\n",
    "\n",
    "plt.title( \"Loss\")\n",
    "plt.xlabel( \"Epochs\")\n",
    "plt.ylabel( \"Loss\")\n",
    "plt.plot( history_train[\"step\"], history_train[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot( history_val[\"step\"], history_val[\"val_loss\"], label=\"val_loss\")\n",
    "plt.ylim( 1, 3 )\n",
    "plt.xlim( 194000, 210000)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.title( \"WER\")\n",
    "plt.xlabel( \"Epochs\")\n",
    "plt.ylabel( \"WER\")\n",
    "plt.plot( history_train[\"step\"], history_train[\"train_wer\"], label=\"train_wer\")\n",
    "plt.plot( history_val[\"step\"], history_val[\"val_wer\"], label=\"val_wer\")\n",
    "plt.ylim( 0.3, 0.6 )\n",
    "plt.xlim( 194000, 210000)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "c9e1f36d-0457-41ad-9c4b-e41b638f18f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.8164182662963868\n",
      "test pred: <sos> we need a consensus in the eu on the prevention of strategies that jeopardise sensible spatial planning and the environment <eos>\n",
      "test targ: <sos> a consensus is necessary in the eu in order to avoid policies that threaten rational european planning and the environment <eos>\n",
      "test pred: <sos> if however the commission repeatedly introduces new <unk> such a policy is created in silence which is unacceptable <eos>\n",
      "test targ: <sos> if the commission then repeatedly introduces new monitoring tools into this area a policy is created in secrecy and that is unacceptable <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.525683045387268\n",
      "test pred: <sos> president-in-office of the council (de) mr posselt we share your concern about the human rights situation and fundamental freedoms in the <unk> and in russia in general and that is\n",
      "test targ: <sos> mr posselt we do share the concern evident in your question on the position concerning human rights and fundamental freedoms in the northern caucasus and russia as a whole and for that reason too we are keeping developments under continual observation <eos>\n",
      "test pred: <sos> i fully agree with the importance of the social dialogue and the tripartite summit and this is why we put this topic on the agenda for yesterday's council meeting in\n",
      "test targ: <sos> i agree completely that the social dialogue and the tripartite summit are important which is why we scheduled these matters for discussion over lunch at yesterday's council meeting where it was agreed that we should establish a tripartite social dialogue summit <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.0994650840759277\n",
      "test pred: <sos> this sentence contains nothing new but it is essential for the sake of the inspectors to give the regime in baghdad a credible <unk> to open doors without hide so\n",
      "test targ: <sos> this sentence does not contain anything new but it is essential because it gives credibility to the inspectors urging the baghdad regime to open its doors to them without concealing anything so that it could avoid the situation subsequently deteriorating still further <eos>\n",
      "test pred: <sos> i therefore urge you to accept this request as a matter of urgency <eos>\n",
      "test targ: <sos> i would therefore make an urgent appeal to you now to honour the request <eos>\n",
      "Step:1  WER:0.5892857143\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.25527822971344\n",
      "test pred: <sos> combating piracy in the field of <unk> <eos>\n",
      "test targ: <sos> combating piracy in the criminal field <eos>\n",
      "test pred: <sos> the most effective way of combating this scourge is to work closely together between the administrative authorities of the member states <eos>\n",
      "test targ: <sos> the most effective way of preventing this type of fraud is through close administrative cooperation between the member states <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.1886623620986938\n",
      "test pred: <sos> accepting community cofinancing for a longer period would significantly reduce the availability of community funds for the cofinancing of <unk> which is clearly the priority objective in this case <eos>\n",
      "test targ: <sos> to accept community co-financing for a longer period would significantly reduce the availability of community funds for co-financing of scrapping which is clearly the priority objective in this case <eos>\n",
      "test pred: <sos> at the beginning of the parliamentary debate it was already clear that only cooperation with the council and the commission would make possible a proposal that was broadly approved <eos>\n",
      "test targ: <sos> at the start of the parliamentary discussion it was already clear that only cooperation with the council and the commission could secure the proposal wide support <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.3192968606948852\n",
      "test pred: <sos> good governance is so crucial as stability <eos>\n",
      "test targ: <sos> good governance is just as vital as stability <eos>\n",
      "test pred: <sos> will the democratic parties of europe which will in future be like <unk> will only be funded and only if they are integrated into the founding values of europe? <eos>\n",
      "test targ: <sos> will the european democratic parties which in future appoint themselves as european parties only be supported if they stand for the values on which europe is <unk> <eos>\n",
      "Step:2  WER:0.6017130621\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.528366255760193\n",
      "test pred: <sos> i am thinking in particular of the european added value that will bring us to the effect that we can increase the exchange rates at eu level by means of\n",
      "test targ: <sos> i am thinking in particular of the european added value which we will find if there is more interaction between the countries of the european union namely through grants provided to young people so that they can go and carry out research and work in other member states and discover more about themselves <eos>\n",
      "test pred: <sos> we are trying with great political proposals and <unk> to find a response to problems of the elderly and difficulties in health care <eos>\n",
      "test targ: <sos> by means of fine policy proposals and election rhetoric we try to address the problems of the elderly in health care <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.1427428483963014\n",
      "test pred: <sos> the latter is of course based on the fundamental value of the human being <eos>\n",
      "test targ: <sos> the latter is of course based on the value of the fundamental entity namely the human being <eos>\n",
      "test pred: <sos> the report proposes a working party to which representatives of civil society should be involved <eos>\n",
      "test targ: <sos> the report proposes a working group in which representatives of civil society should be involved <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.9429402351379395\n",
      "test pred: <sos> if we take that as a whole we will have the right road ahead <eos>\n",
      "test targ: <sos> we will be on the right track if we grasp that fact <eos>\n",
      "test pred: <sos> but the political programme demonstrates that it should have been the legislative programme with the aim of implementing the policies and that is what is lacking <eos>\n",
      "test targ: <sos> however the political programme needs to be underpinned by the legislative programme required to implement the political objectives and that is what is missing <eos>\n",
      "Step:3  WER:0.6176470588\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.796740746498108\n",
      "test pred: <sos> that is why we are increasingly convinced that immigration must be a union of the union of tomorrow <eos>\n",
      "test targ: <sos> this is why we are increasingly convinced that immigration ought to be one of the european union’s responsibilities in the future <eos>\n",
      "test pred: <sos> the prospect of membership alone promotes democracy human rights and a better life for the citizens <eos>\n",
      "test targ: <sos> the very perspective of becoming a member state encourages the development of democracy human rights and a better life for citizens <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.6125488758087159\n",
      "test pred: <sos> as a member of the committee on constitutional affairs i have to say on behalf of lord inglewood that a period of almost a year is enough for the powers\n",
      "test targ: <sos> as a member of the constitutional affairs committee speaking on behalf of lord inglewood i must say that it seems very nearly a year is sufficient time for the powers that be to have considered this matter <eos>\n",
      "test pred: <sos> finally my group firmly believes - mr duff will return to this point in a moment - that the union and its institutions must put their own house in order\n",
      "test targ: <sos> finally my group feels very strongly - and mr duff will expand on this in a moment - that the union and its institutions should put its own house in order even before the next enlargement round gets underway <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  2.54532790184021\n",
      "test pred: <sos> it does not seem appropriate to extend the health insurance scheme to former members and my group is not in favour of this <eos>\n",
      "test targ: <sos> extending the health insurance scheme to apply to former members does not seem justified and my group is not in favour of this <eos>\n",
      "test pred: <sos> however significant progress was made at the conference on technical issues in particular the mechanisms for ensuring clean development on the reporting and revision procedures <eos>\n",
      "test targ: <sos> significant progress on technical matters was made at the <unk> however these related in particular to mechanisms for clean development reporting and review procedures <eos>\n",
      "Step:4  WER:0.5661605206\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.7665024280548096\n",
      "test pred: <sos> what are human rights in <unk> <eos>\n",
      "test targ: <sos> what are human rights in russia? <eos>\n",
      "test pred: <sos> the member states are of the opinion that the threat posed by the spread of weapons of mass destruction must be supported by the member states and that the need\n",
      "test targ: <sos> member states agree about the threat of proliferation of weapons of mass <unk> they agree on the need for full and effective disarmament of <unk> they agree that the united nations must remain at the centre of the international order; they agree indeed that force should only be used as a last resort <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.9037625432014466\n",
      "test pred: <sos> 4 <eos>\n",
      "test targ: <sos> 4 <eos>\n",
      "test pred: <sos> madam president mr president-in-office of the council mr vice-president of the commission i have here the draft list of votes on the wortmann-kool report on heavy goods vehicles <eos>\n",
      "test targ: <sos> madam president mr president-in-office of the council mr vice-president of the commission i have here the draft voting list for the wortmann-kool report on the charging of heavy goods vehicles <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.0982770204544068\n",
      "test pred: <sos> the next item is the report <unk> by mr casaca on behalf of the committee on budgetary control on the proposal for a council regulation amending regulation (ec) no <unk>\n",
      "test targ: <sos> the next item is the report by mr casaca <unk> on behalf of the committee on budgetary control on the proposal for a council regulation on amending regulation (eec) <unk> establishing an integrated administration and control system for certain community aid schemes <unk> <unk> - <unk> - <unk> <unk> <eos>\n",
      "test pred: <sos> this increase has mainly benefited road transport especially freight transport <eos>\n",
      "test targ: <sos> this increase has mainly benefited the road sector particular as regards freight movement <eos>\n",
      "Step:5  WER:0.5476839237\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.0763424396514893\n",
      "test pred: <sos> that is why finland will shortly be holding an extraordinary meeting of the heads of state and of government in lahti on these issues <eos>\n",
      "test targ: <sos> finland will therefore host an extraordinary social summit just prior to the lahti meeting of heads of state and government where these issues will be discussed <eos>\n",
      "test pred: <sos> there is a great deal of concern that the restrictions imposed by the european commission on the risks of excessive costs and major damage have been identified <eos>\n",
      "test targ: <sos> there is cross-party concern that the current restrictions imposed by the european commission are disproportionately costly and disruptive <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  2.0608239650726317\n",
      "test pred: <sos> i will endeavour to react and i will certainly try to respond by coordinating and warning mechanisms and by storing appropriate reserves and equipment internationally <eos>\n",
      "test targ: <sos> the readiness and ability to react will certainly follow on from improvements in coordination and early warning mechanisms as well as stores of appropriate materials and reserves on the international level <eos>\n",
      "test pred: <sos> there are clearly lessons to be drawn but we must not forget that india was recently plagued by a series of appalling acts of terror perpetrated by the extremist islamic\n",
      "test targ: <sos> clearly lessons have to be learned but we should not forget that india is currently <unk> from a series of atrocious terrorist attacks by islamic <unk> extremists - most recently the killing of 30 people including women and children and scores of wounded on the outskirts of jammu <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.6092727661132813\n",
      "test pred: <sos> thirdly no consideration has been given to areas such as maritime pollution or nuclear contamination as the argument is that there are international conventions in this regard <eos>\n",
      "test targ: <sos> thirdly oil pollution and nuclear damage are not covered with the argument that they are covered by international conventions however these generally cover traditional damage and not environmental damage <eos>\n",
      "test pred: <sos> three of them have been <unk> the level of primary <unk> that is to say the <unk> of payments the nominal rate of nominal growth and the additional factors that\n",
      "test targ: <sos> three factors have influenced this development: the level of primary balance that is excluding financial payments the nominal rate of growth of gdp and other autonomous or residual factors <eos>\n",
      "Step:6  WER:0.6394557823\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  2.013778638839722\n",
      "test pred: <sos> on behalf of the pse group - (fr) madam president commissioner ladies and gentlemen we are dealing here with a very important area of the water framework directive <eos>\n",
      "test targ: <sos> on behalf of the pse group - (fr) madam president commissioner ladies and gentlemen we have here an extremely important daughter directive of the water framework directive <eos>\n",
      "test pred: <sos> we are giving this objective a broad political and financial support but - and i would like to stress this - we are keeping a close eye on human rights\n",
      "test targ: <sos> we are extending substantial political and financial support to that goal while - and i would like to underline this - remaining very vigilant on human rights and other concerns <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.4960009098052978\n",
      "test pred: <sos> regions with low population density are hopelessly short of opportunities and as it is practically unthinkable for sparsely populated or <unk> regions to build <unk> schools we must look for\n",
      "test targ: <sos> as it would in desert or other <unk> areas be unrealistic to build a large number of schools near to where people live we have to take a good look at other solutions to reduce the educational deficit <eos>\n",
      "test pred: <sos> i had hoped that the initiatives that were put in place to obtain their support for the so-called proposal would be fruitful and that at the end of the new\n",
      "test targ: <sos> it is a key country in the united nations and i hoped that the initiatives to obtain their support for mr <unk> proposal would succeed and that in the end the new council would be established with <unk> support <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.5080880880355836\n",
      "test pred: <sos> the reform is increasing <eos>\n",
      "test targ: <sos> the momentum for reform is mounting <eos>\n",
      "test pred: <sos> for ten years since the start of globalisation the rate of growth in india has been 6% <eos>\n",
      "test targ: <sos> over the last ten years since globalisation began india has had a rate of growth of 6 to 7% <eos>\n",
      "Step:7  WER:0.6441102757\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  2.4097840785980225\n",
      "test pred: <sos> finally i would like to thank the local authorities for their help in the field <eos>\n",
      "test targ: <sos> in conclusion let me express my gratitude to the aid workers on the ground for aid has to be delivered to the local people <eos>\n",
      "test pred: <sos> firstly the position of the <unk> and their concern for privatisation <eos>\n",
      "test targ: <sos> first of all i should like to mention the position of air traffic controllers and their concerns surrounding privatisation <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.926246953010559\n",
      "test pred: <sos> guinea on the other hand was the example of ghana a former british colony which had acquired its independence a year earlier and wanted to develop an african model of\n",
      "test targ: <sos> guinea on the other hand followed the example of the former british colony ghana which had gained independence one year earlier and which wanted to develop an african model of socialism with strong participation of the own population <eos>\n",
      "test pred: <sos> the best way to guarantee security of supply is certainly to have our own supply sources and it is therefore to be welcomed that work is being done gradually and\n",
      "test targ: <sos> the best form of security of supply is surely to have our own supplies and we must therefore welcome the fact that work is gradually being done (a) to produce high-quality gas ourselves inside the european union and (b) to feed that gas into our own available networks <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  2.082711410522461\n",
      "test pred: <sos> mr president mr blokland this report calls for freedom of movement in europe for waste – urban waste industrial waste – and i voted in favour of this report <eos>\n",
      "test targ: <sos> mr president with this report mr blokland calls for freedom of movement in europe for waste – urban waste industrial waste – and i voted for the motion <eos>\n",
      "test pred: <sos> why then can we not be equally credible in terms of the <unk> <eos>\n",
      "test targ: <sos> why then can we not be equally credible in the political <unk> <eos>\n",
      "Step:8  WER:0.5783410138\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  2.3947747230529783\n",
      "test pred: <sos> this creates a serious threat to the continuity of traditional farming families in europe through the eu and the member states that are combating unemployment and the protection of traditional\n",
      "test targ: <sos> with the availability of employment outside agriculture and the advances made by the eu and member states in reducing unemployment there is now a serious threat to the sustainability of europe's traditional family farm <eos>\n",
      "test pred: <sos> thirdly the commission considers that the need to take prompt action and to reinforce the use of the preventive measures provided for in the stability and growth pact must be\n",
      "test targ: <sos> thirdly the commission believes that the debate must deal with the need to apply measures quickly to increase the use of the preventive measures of the stability and growth pact <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.450268530845642\n",
      "test pred: <sos> thus in a <unk> on 19 april mr balfe was given an <unk> we thought that a certain day was limited and that the meetings scheduled for that day were\n",
      "test targ: <sos> for example on 19 april i received an e-mail from the <unk> mr richard balfe which stated that that day was a restricted day and that the meetings planned for that day were cancelled <eos>\n",
      "test pred: <sos> although i am not really convinced by everything that has been said in this context it is not so true that the cultural dimension of the european project contains many\n",
      "test targ: <sos> whilst i was not actually persuaded by everything that was stated in this context it remains true that the cultural dimension of the european project contains many buried treasures and still has a lot of untold reserves <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.2495561122894288\n",
      "test pred: <sos> how can we ignore the fact that the pact establishes a useful discipline for the member states in terms of deficit but does it not define the policies necessary to\n",
      "test targ: <sos> who can fail to see that the pact while providing member states with a useful discipline as far as public deficits are concerned does not begin to define the policies that are required if the development targets set in lisbon are to be <unk> <eos>\n",
      "test pred: <sos> let me turn though to the subject under discussion which is so serious and which i know has stirred up a huge amount of energy over the years namely the\n",
      "test targ: <sos> let me turn to this very serious subject which i know has raised great passions and has produced a great deal of intellectual energy over the years: the question of the systems of fixed book prices in the member states which as honourable members have said has been around for some time <eos>\n",
      "Step:9  WER:0.6201232033\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.9704419136047364\n",
      "test pred: <sos> it has the right of initiative and it can make a decision in any case <eos>\n",
      "test targ: <sos> it has the right of initiative and can decide anyway <eos>\n",
      "test pred: <sos> even though the profits were not as good as the lisbon strategy remained the lisbon strategy as our leading politicians they were looking for a sort of <unk> <unk> to\n",
      "test targ: <sos> but even when the profits flooded in the lisbon strategy as our leaders chose to understand it remained an excuse – a kind of mantra – to spend the new and plentiful money on anything but the people who needed it most <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.5449148893356324\n",
      "test pred: <sos> the <unk> thinks it can supply food to 110 000 people only 10% of the population in the most affected areas to <unk> million people <eos>\n",
      "test targ: <sos> the <unk> only expects to deliver food <unk> to 110 000 people 10% of the <unk> area's <unk> million <eos>\n",
      "test pred: <sos> i would point out as the commissioner did that this proposal deals with animal health <eos>\n",
      "test targ: <sos> i would stress as the commissioner did that this proposal deals with animal health <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.1575901508331299\n",
      "test pred: <sos> but to go further i urge the governments of the member states and the council to take the courageous decisions that are required <eos>\n",
      "test targ: <sos> in order to make progress however i invite above all the governments of member states and the council to take the courageous decisions that are necessary <eos>\n",
      "test pred: <sos> i would like to point out in passing that we have been working with you for four years and working with a spirit of transparency <eos>\n",
      "test targ: <sos> i should like to emphasise in passing the spirit in which we are working and in which i have been working with you for the last four years a spirit which is inspired by a desire for transparency <eos>\n",
      "Step:10  WER:0.5975308642\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.517533302307129\n",
      "test pred: <sos> i congratulate the commission on this initiative which seeks to give fresh impetus to the legislation in this field at european level and to these realities <eos>\n",
      "test targ: <sos> i congratulate the commission on its initiative in adapting the legislation in this area to these developments at european level and also in giving new impetus <eos>\n",
      "test pred: <sos> we cannot once again share europe in a europe of <unk> and <unk> <eos>\n",
      "test targ: <sos> we should not divide europe once again into the <unk> and the <unk> <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.8852279543876648\n",
      "test pred: <sos> i hope that the united kingdom will do that but it is very likely that the accession countries in eastern and central europe will do so and so we must\n",
      "test targ: <sos> i hope that great britain will be one of them but some countries from the central and eastern european accession candidates will undoubtedly join too which means that it is essential to consider how these voting modalities can be amended and adjusted to the new dimension given that in future some 15 20 or 25 countries may be part of the <unk> <eos>\n",
      "test pred: <sos> the eu should also make a major contribution to this <eos>\n",
      "test targ: <sos> the eu should also make a major contribution to this shortfall <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.509321117401123\n",
      "test pred: <sos> it is important that the same rules should apply to meat imported from third countries as well <eos>\n",
      "test targ: <sos> it is essential that identical labelling rules are applied to beef imported from third countries <eos>\n",
      "test pred: <sos> it would however be appropriate for the president of a parliamentary group and also for those who are responsible in the european institutions - and i am referring to minister\n",
      "test targ: <sos> it would in any case be appropriate for the chairman of a parliamentary group and also for those holding positions of responsibility in the european institutions - i refer to the absent mr michel - not to make statements either of approval or condemnation in this chamber on the internal affairs of a member state <eos>\n",
      "Step:11  WER:0.6157517900\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.583290958404541\n",
      "test pred: <sos> rarely have i been so proud of this office as it is today following the work of the committee on petitions <eos>\n",
      "test targ: <sos> rarely have i felt as proud of my position as i am today in relation to this report due to the work that has been carried out in committee over the last year <eos>\n",
      "test pred: <sos> when he was removed from the right of residence in slovenia he was returned to germany where he was twelve years old and <unk> <eos>\n",
      "test targ: <sos> when he lost his rights of <unk> in slovenia he settled in germany where he lived and worked for 12 years <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.8558761835098267\n",
      "test pred: <sos> unfortunately the governments appear to have forgotten them <eos>\n",
      "test targ: <sos> unfortunately governments appear to have lost sight of this goal <eos>\n",
      "test pred: <sos> the rapporteur proposes that the common position which only has been added to an article on <unk> already accepted by the council should be approved <eos>\n",
      "test targ: <sos> the rapporteur proposes that the common position be approved with just one additional article on the <unk> <unk> already agreed with the council <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.7447455167770385\n",
      "test pred: <sos> you have not chosen to adopt a <unk> model on adaptation and a <unk> which has in fact proved to be a <unk> framework of <unk> which has <unk> the\n",
      "test targ: <sos> you did not choose the ambition of a model but settled for an accommodation and for staying afloat which in practice has proved to be a framework of shifting sands which has <unk> the power struggle between powers and between countries <eos>\n",
      "test pred: <sos> (fr) doha was supposed to be the development round <eos>\n",
      "test targ: <sos> doha should have been the development cycle <eos>\n",
      "Step:12  WER:0.5865168539\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  2.0751210689544677\n",
      "test pred: <sos> it is not only the business that has a responsibility <eos>\n",
      "test targ: <sos> of course it is not just the company that has a responsibility <eos>\n",
      "test pred: <sos> thanks to this directive we will be able to protect the health and safety of people from the harmful effects of high blood alcohol products from exceptions that are proven\n",
      "test targ: <sos> thanks to this directive we shall be able to protect vegetation and people' s health from the damaging effects of unduly high concentrations of ozone except in those cases where that cannot be brought about by proportionate measures <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.5855833053588868\n",
      "test pred: <sos> during the enquiry by the echelon committee we received no evidence that people are consciously being damaged to the private lives of citizens or that the systems of large-scale international\n",
      "test targ: <sos> during the investigation by the echelon committee we have not obtained any evidence that citizens' privacy is being intentionally violated or that systems designed for large-scale tapping operations involving international telecommunications traffic are deliberately being used for large-scale and direct industrial espionage <eos>\n",
      "test pred: <sos> the <unk> used part of the <unk> <unk> which amounted to several million for the purchase of weapons <eos>\n",
      "test targ: <sos> <unk> used part of the oil <unk> s advance millions to acquire weapons <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.2946118593215943\n",
      "test pred: <sos> secondly that the committee did not properly apply the rules on reimbursement of travel expenses and other financial rules <eos>\n",
      "test targ: <sos> second that the cor failed to properly apply the rules on the reimbursement of travel expenses and other financial rules <eos>\n",
      "test pred: <sos> according to the council of fisheries ministers the limits imposed on cod stocks are used to protect cod stocks instead of saying that they have never given their opinion and\n",
      "test targ: <sos> this is a restriction which according to the european fisheries council is required in order to protect cod whilst <unk> claim that such recommendations were never made and that this was a political decision <eos>\n",
      "Step:13  WER:0.5959885387\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.570985770225525\n",
      "test pred: <sos> what has the finnish presidency of the european council done to prevent us from developing in this direction either because we will end up with a democratic europe or no\n",
      "test targ: <sos> what has the finnish presidency done at european level to prevent things moving in that direction here as well in view of the fact that we will in the end have either a democratic europe or no europe? <eos>\n",
      "test pred: <sos> i would like to hear mrs schroedter accept this amendment particularly in view of the subsidiarity aspect of its positive recitals <eos>\n",
      "test targ: <sos> i should be very grateful mrs schroedter if you would actually include this proposed amendment in the part relating to subsidiarity in your positive deliberations <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.2135340452194214\n",
      "test pred: <sos> therefore as we have experienced with great concern the recent days of 9 april and 28 may it is obvious that we can only do things properly by means of\n",
      "test targ: <sos> therefore since two recent days 9 april and 28 may have caused us great concern it is clear that we will only be able to act appropriately if we send observers which we hope will be possible and who we hope will be able to act effectively <eos>\n",
      "test pred: <sos> it is not only passengers but also all <unk> that will benefit from uniform standards in all european countries even if the number of people travelling by air is constantly\n",
      "test targ: <sos> it is not only the passengers but also the whole crew who stand to benefit from the establishment of uniform standards in all european countries which will make it possible even though the number of people who travel by air is constantly increasing for the same safety standards to apply wherever they do so <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.7913449287414551\n",
      "test pred: <sos> after years of seveso disaster to name but two european directives to give the member states a harmonised policy on the management of industrial risks which are more serious they\n",
      "test targ: <sos> it is 33 years since the seveso disaster not to mention the other disasters that have occurred and two european directives seeking to provide the member states with a harmonised policy in managing major industrial risks will not have been enough to avoid another human tragedy this is a stark reality that illustrates the limits of a law even where this is binding and transnational <eos>\n",
      "test pred: <sos> in its communication of january 2000 the commission emphasised that a genuine european research area can only be achieved if we also promote a european area of shared ethical values\n",
      "test targ: <sos> in its statement of january 2000 the commission highlighted the fact that a genuine european research area can only become a reality if we also promote a european area of ethical values which are shared throughout europe <eos>\n",
      "Step:14  WER:0.6800000000\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  2.177171230316162\n",
      "test pred: <sos> i am very pleased that we are in favour of the european union adopting a position on the future development of montenegro <eos>\n",
      "test targ: <sos> i am very glad that we see eye to eye on the future development of montenegro on its road towards the european union <eos>\n",
      "test pred: <sos> i would be grateful if you could do that <eos>\n",
      "test targ: <sos> i would be obliged if you could do so <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.8061707973480225\n",
      "test pred: <sos> the final draft agenda for this part-session as set by the conference of presidents at its meeting of thursday 8 november pursuant to rules 130 and 131 of the rules\n",
      "test targ: <sos> the final draft of the agenda for this part-session as laid down by the conference of presidents at its meeting of thursday 8 november pursuant to rules 130 and 131 of the rules of procedure has been distributed to you <eos>\n",
      "test pred: <sos> this <unk> petition shows how complex the problem is and the need to strike a balance as the commissioner said between the right of asylum and the fact that citizens\n",
      "test targ: <sos> this request which is unusual demonstrates the complexity of the problem and the need to find a balance as the commissioner said between the right to asylum and the citizens feeling secure <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.3880544185638428\n",
      "test pred: <sos> the evans report rightly emphasises the importance of legal certainty for companies <eos>\n",
      "test targ: <sos> the evans report rightly stresses the importance of legal certainty for companies <eos>\n",
      "test pred: <sos> as regards the new agencies this provision will be included in the proposed <unk> <unk> <eos>\n",
      "test targ: <sos> an additional clause will be included in the instruments of incorporation of new agencies <eos>\n",
      "Step:15  WER:0.5064935065\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.4283892631530761\n",
      "test pred: <sos> although the report contains some <unk> it does not now mean that i believe it is a rare occasion to <unk> <eos>\n",
      "test targ: <sos> the policy is implicitly included in the report but i still think that in a sense not dealing with it now is a missed opportunity <eos>\n",
      "test pred: <sos> many improvements to the text of the amended proposal by the commission will be based on this first opinion of parliament which voted for the amended proposal on 2 december\n",
      "test targ: <sos> this initial position taken by parliament resulted in numerous improvements to the text of the commission proposal as later amended which parliament approved on 2 december 1993 <unk> that approval on 27 october 1999 <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.3173580169677734\n",
      "test pred: <sos> therefore their consolidation in a single general clause is clear simple and straightforward <eos>\n",
      "test targ: <sos> therefore their consolidation into a single general clause is clear simple and straightforward <eos>\n",
      "test pred: <sos> we cannot let competition benefit only a few because ultimately it is a controlled society that is now waiting for us for the digital dictatorship <eos>\n",
      "test targ: <sos> competition must not be allowed to merely become a benefit for the few because at the end of that road awaits a controlled society currently the <unk> control dictatorship <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.0748875617980957\n",
      "test pred: <sos> two states peace security <eos>\n",
      "test targ: <sos> two states both have peace both have security <eos>\n",
      "test pred: <sos> i think that we need to work on these two aspects in general but i think the report is very valuable <eos>\n",
      "test targ: <sos> i am keen to encourage both of those aspects but overall this report is extremely worthwhile <eos>\n",
      "Step:16  WER:0.5171339564\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.3130847215652466\n",
      "test pred: <sos> the debate is closed <eos>\n",
      "test targ: <sos> the debate is closed <eos>\n",
      "test pred: <sos> madam president i would like to say something about a point of order on wednesday which you have not mentioned <eos>\n",
      "test targ: <sos> madam president i would like to talk about an item on the agenda for wednesday which you have not yet announced <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.4683019876480103\n",
      "test pred: <sos> what can be seen now however is that the old member states are trying to solve their old problems at the expense of the new member states <eos>\n",
      "test targ: <sos> however what we can now see is that the old member states intend to solve their long-standing unsolved problems to the detriment of the new member states <eos>\n",
      "test pred: <sos> as you said commissioner the main issue is to develop effective technical standards for products <eos>\n",
      "test targ: <sos> as you said commissioner it is largely about the development of effective technical standards for products <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.653783392906189\n",
      "test pred: <sos> this creates a disparity which does not benefit the clarity of the legislative text and certainly does not help the passenger to understand exactly what they are entitled to <eos>\n",
      "test targ: <sos> the disparity introduced does not serve to make the legislative text clearer and certainly does not help passengers to understand their exact rights <eos>\n",
      "test pred: <sos> i have the impression that mrs wallström is paying more attention to mr barón crespo than you mr fatuzzo <eos>\n",
      "test targ: <sos> i am afraid mrs wallström seems to be paying more attention to mr barón crespo than to you mr fatuzzo <eos>\n",
      "Step:17  WER:0.4955489614\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.6668556928634644\n",
      "test pred: <sos> first of all we must remember that people are at stake and on this basis we must develop a system that allows adequate pensions to be provided with the full\n",
      "test targ: <sos> first and foremost we need to remember that this is about people and based on this we must develop a system which makes sound pension schemes possible and we must as far as we can leave this task to the member states <eos>\n",
      "test pred: <sos> there can therefore be no proportionality and furthermore every single person is dying <eos>\n",
      "test targ: <sos> there is therefore a lack of proportionality the fact is however that each of these deaths is pointless <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.2420868158340455\n",
      "test pred: <sos> in the baltic sea however the consequences would be much more devastating and sustainable than in the atlantic <eos>\n",
      "test targ: <sos> in the baltic sea however the consequences would be vastly more disastrous and longer-term than in the atlantic <eos>\n",
      "test pred: <sos> article <unk> of council directive <unk> on the framework agreement on fixed-term work concluded by the european trade union confederation the employers' and employers' associations and the european employers' federation\n",
      "test targ: <sos> article 2 paragraph 1 of council directive <unk> concerning the framework agreement on fixed-term work concluded by the european trade union confederation the union of industrial and employers' <unk> of europe and the european centre of enterprises with public participation obliges the member states to comply with the directive in question by 10 july 2001 <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.633069396018982\n",
      "test pred: <sos> this should enable the member states to contact the states to transmit information but also to carry out their work in order to dismantle the networks <eos>\n",
      "test targ: <sos> this instrument must be such as to make it possible to address states directly in order to urge them to pass on information but also to issue orders so that the networks can be dismantled <eos>\n",
      "test pred: <sos> this is why given such a large number of unresolved issues small farmers cannot plan their investment and their future <eos>\n",
      "test targ: <sos> that is why when so many questions remain unanswered our small farmers cannot plan their investments and their future <eos>\n",
      "Step:18  WER:0.6164383562\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.8611624002456666\n",
      "test pred: <sos> these are the values that the european parliament has been <unk> throughout its relationship with the <unk> <eos>\n",
      "test targ: <sos> these are values that have been defended by the european parliament throughout its relationship with mercosur <eos>\n",
      "test pred: <sos> we respect the experience and wisdom of parliament and we are sure that parliament the commission and the council will be able to shape a policy that will in future\n",
      "test targ: <sos> we recognise the experience and the wisdom of parliament and we are sure that together parliament commission and council will be able to frame a policy which will not only continue to command the attention of the whole house but will actually lead to the sort of beneficial consequences that we wish to see <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.2647359132766725\n",
      "test pred: <sos> we fully agree with you that we should encourage <unk> and therefore the departure of some fishermen from the fisheries sector <eos>\n",
      "test targ: <sos> we even agree on providing incentives for scrapping and therefore encouraging some fishermen to leave the fisheries sector <eos>\n",
      "test pred: <sos> the european parliament must accept the basis of this decision and extend the directive to cover the environmental damage caused <eos>\n",
      "test targ: <sos> parliament should accept this proposal as a basis for a decision and extend the directive to environmental damage caused by nuclear activities <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.7359447240829469\n",
      "test pred: <sos> when we talk about quality we are talking about more of an <unk> tax policy to increase employment to promote research and innovation and to encourage investment in human capital\n",
      "test targ: <sos> when i say quality i mean especially a fiscal policy aimed more towards creating jobs supporting research and innovation and increasing investment in human resources <eos>\n",
      "test pred: <sos> parliamentary debate at the moment of <unk> or almost <eos>\n",
      "test targ: <sos> a parliamentary debate at the <unk> hour just before the <unk> hour <eos>\n",
      "Step:19  WER:0.6186868687\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.7826440811157227\n",
      "test pred: <sos> unfortunately it chose to buy sugar for no account taken of consumers or other eu countries <eos>\n",
      "test targ: <sos> however it has chosen to go down an imposed route without consideration for customers or indeed for other eu countries <eos>\n",
      "test pred: <sos> it is of course the commission' s task to ensure that community law is strictly adhered to and in so cases where inappropriate measures have been taken that do not\n",
      "test targ: <sos> it is of course the commission's job to guarantee that european community law is strictly complied with and in cases where inappropriate measures have been adopted leading not to the protection of workers but rather to restrictions on the freedom to provide services in the internal market the commission has to take action as enshrined in the treaty <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.0836546421051025\n",
      "test pred: <sos> only agreements at eu level can be envisaged <eos>\n",
      "test targ: <sos> only agreements at eu level can be considered <eos>\n",
      "test pred: <sos> the eu is consulting the six which makes the best possible contribution to this process while remaining in favour of the implementation of security council resolution <unk> <eos>\n",
      "test targ: <sos> the eu is consulting with the six offering the best means by which we can assist in this process while remaining committed to the implementation of security council resolution <unk> <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.5218717336654664\n",
      "test pred: <sos> mr president in finland a group of experts is currently in operation on border control <eos>\n",
      "test targ: <sos> – mr president at present a team of experts in the field of border control operate in finland <eos>\n",
      "test pred: <sos> as regards the procedures for family reunification the requirements for the release of material and health resources for the disease will remain <unk> <eos>\n",
      "test targ: <sos> in matters of family reunification procedures requirements concerning accommodation material resources and sickness insurance will remain optional <eos>\n",
      "Step:20  WER:0.5914786967\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.506671380996704\n",
      "test pred: <sos> however we were told that they would have to rely on us for some time <eos>\n",
      "test targ: <sos> however we were informed that owing to constraints of time you had to leave earlier <eos>\n",
      "test pred: <sos> i should like to say once again to the members that i was delighted with the conciliation procedure to ensure that services for the search and call for tenders in\n",
      "test targ: <sos> i should like to repeat to the house that i am delighted that during the conciliation procedure we managed to ensure that in general services for searching and consulting spatial information must be made available free of charge <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.7476980209350585\n",
      "test pred: <sos> nevertheless we broadly agree with this proposal and i should like to thank the rapporteur mr sacconi for his commitment and for the work he has done <eos>\n",
      "test targ: <sos> nevertheless we are generally in agreement regarding this project and i should to thank the rapporteur mr sacconi for his commitment and for the work he has done <eos>\n",
      "test pred: <sos> in the joint resolution we call on the council and the member states to redouble their efforts to ensure that minorities are respected and that democratic structures are established and\n",
      "test targ: <sos> in the joint resolution we ask the council and the member states to double their efforts to ensure respect for minority groups and to work towards the creation of democratic structures giving a voice to all those forces which still believe in peaceful coexistence <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.0992210388183594\n",
      "test pred: <sos> thirdly it is also important that minors be sold <eos>\n",
      "test targ: <sos> thirdly the issue of selling to under-age people is also an important one <eos>\n",
      "test pred: <sos> this means that there is a greater risk on the one hand in relation to the budget on the other hand in relation to the environment we must not forget\n",
      "test targ: <sos> this poses new risks both to the budget and to the environment for we must not forget that cotton planting is a very intensive form of crop farming and involves the highly concentrated use of fertilisers and pesticides <eos>\n",
      "Step:21  WER:0.6066945607\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.686539626121521\n",
      "test pred: <sos> if that is the case we can withdraw the request for a split vote <eos>\n",
      "test targ: <sos> if so we can withdraw the attempt at a split vote <eos>\n",
      "test pred: <sos> in this way most of what parliament has wanted and we would be better off with a comparison <eos>\n",
      "test targ: <sos> we should in that way obtain most of what parliament has wanted at the same time as being able to avoid conciliation <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  2.008935236930847\n",
      "test pred: <sos> in this respect we need more transparency and more information on company rights and obligations that are practised on the posting of workers <eos>\n",
      "test targ: <sos> what is needed therefore is greater transparency and more information on the rights and obligations to be complied with when businesses become involved in the posting of workers <eos>\n",
      "test pred: <sos> at the same time as these trade measures are needed the european union must draw up a specific transitional aid plan for the restructuring and modification of the entire sector\n",
      "test targ: <sos> in parallel with these trade measures the european union should implement a transitional practical plan to assist restructuring and retraining for the entire sector with a view to helping <unk> regions and safeguarding the future and competitiveness of the sector on international markets <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.765856385231018\n",
      "test pred: <sos> finally we must ensure that the rights of the people who live near areas that are at risk of disasters are protected <eos>\n",
      "test targ: <sos> finally we also need to ensure that the rights of people living near areas vulnerable to disasters are protected <eos>\n",
      "test pred: <sos> our group agrees with the developing countries that they are putting the agenda on new issues because a <unk> agenda would put a strain on the ability of many developing\n",
      "test targ: <sos> our group agrees with developing countries when they oppose new issues on the agenda on the grounds that expanding that agenda would over stretch the capacity of many developing countries which are already struggling to participate effectively in the wto process <eos>\n",
      "Step:22  WER:0.5991091314\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.4914455175399781\n",
      "test pred: <sos> i would like to thank you for giving me the opportunity to give me at least a little more detailed consideration than ever allowed me in this house <eos>\n",
      "test targ: <sos> i should like to thank you for giving me the opportunity at least to stretch my thoughts a little bit further than i have ever been allowed to do in this house before <eos>\n",
      "test pred: <sos> i therefore ask you to ensure that it is not only examined but that the americans clearly ban the international convention banning the use of cluster munitions <eos>\n",
      "test targ: <sos> it is for that reason that i call upon you to ensure that there are not just investigations but that the americans also sign the international convention specifically outlawing the use of white <unk> <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.5616605043411256\n",
      "test pred: <sos> this point is already settled at the moment by which i mean the special characteristics of products manufactured from aquaculture <eos>\n",
      "test targ: <sos> it is already included now: <unk> of the particular characteristics due to the manufacturing <unk> <eos>\n",
      "test pred: <sos> a <unk> as it was heard that not only the multilingual version remains in place but the simultaneous interpretation of all official languages should be made available to all meps\n",
      "test targ: <sos> this is a fiction to the extent that it has been agreed that not only will the multilingual version be retained but the simultaneous translation into all the official languages should also be made available on request to all the members of the house as well as to the public at large <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.219566011428833\n",
      "test pred: <sos> council elections on 17 november will certainly be an important event for the future of kosovo and will increase the direct participation of the people and local political leaders in\n",
      "test targ: <sos> the elections on 17 november will certainly be a key event for the future of kosovo and will facilitate a greater level of direct participation by the population and by local political leaders in the operation of the provisional constitutional framework established by unmik <eos>\n",
      "test pred: <sos> <unk> <eos>\n",
      "test targ: <sos> <unk> <eos>\n",
      "Step:23  WER:0.5634328358\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.6905551910400392\n",
      "test pred: <sos> it is also extremely important that europol should be allowed to cooperate better between law enforcement agencies in the member states <eos>\n",
      "test targ: <sos> in addition it is very important to organise better cooperation between member states police forces through europol <eos>\n",
      "test pred: <sos> it is crucial that recognition - not only the systems and procedures of third countries but also the <unk> of compliance with these requirements - be strictly monitored at european\n",
      "test targ: <sos> it is essential that recognition not only of third countries' systems and procedures but also of their continued compliance be for a substantial initial period of time strictly monitored at european level as such recognition will be global in nature <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.8135313153266907\n",
      "test pred: <sos> although the commission initiatives particularly awareness actions are not to underestimate they are unfortunately insufficient since the sexual exploitation of children and <unk> is increasingly gaining dimensions not only in\n",
      "test targ: <sos> of course the commission' s initiatives especially the activities to increase awareness should not be underestimated but unfortunately they are proving to be inadequate since the sexual exploitation of children and child tourism are continually assuming greater dimensions not only in <unk> underdeveloped countries but within the european union itself <eos>\n",
      "test pred: <sos> what would happen in 20 years when their grandchildren are sitting on their <unk> <unk> <unk> was the right way to respond to this great disaster when europe has not\n",
      "test targ: <sos> what will they do in 20 years' time when they are sitting with their grandchildren on their knees and their grandchildren say: 'what was your real role in the great disaster when europe failed to grasp its own <unk> <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  2.018646001815796\n",
      "test pred: <sos> finally in reply to mrs van lancker on the intentions and will of the swedish presidency on the issue of discrimination i shall reply as representative of the council and\n",
      "test targ: <sos> let me finally say the following in answer to mrs van <unk> s question on what the swedish presidency intends or would like to do to tackle the problems of discrimination <eos>\n",
      "test pred: <sos> there is the nice system which is based on a weighting of votes and there is the system of a constitution based on a double majority of states and a\n",
      "test targ: <sos> there is the nice system based on a scale and there is the draft constitution system based on a double <unk> a majority of the member states and a majority of the represented population which we have set at 60% <eos>\n",
      "Step:24  WER:0.6918918919\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.3279040575027465\n",
      "test pred: <sos> you know i come from the <unk> and know the situation there very well <eos>\n",
      "test targ: <sos> you know i am from a border region and i know exactly what the situation is on the ground <eos>\n",
      "test pred: <sos> the candidate countries can also participate in this programme <eos>\n",
      "test targ: <sos> the countries seeking to join the union can also join this programme <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.8712781310081482\n",
      "test pred: <sos> i can only support it <eos>\n",
      "test targ: <sos> i very much support this <eos>\n",
      "test pred: <sos> this <unk> attitude is very serious and unfair to those who live in countries that have chosen to provide robust civil protection <eos>\n",
      "test targ: <sos> this problem of <unk> is very serious and is unfair towards people in those countries that choose to have strong civil protection <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.9565104722976685\n",
      "test pred: <sos> thank you commissioner byrne <eos>\n",
      "test targ: <sos> thank you commissioner <eos>\n",
      "test pred: <sos> thank you commissioner nielson <eos>\n",
      "test targ: <sos> thank you very much commissioner <eos>\n",
      "Step:25  WER:0.4268774704\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.4556040525436402\n",
      "test pred: <sos> mr president waste management and the whole of the eu waste management is an ever more important component of eu environmental policy <eos>\n",
      "test targ: <sos> mr president the treatment of waste and the whole policy on waste is becoming an ever more important part of eu environmental policy <eos>\n",
      "test pred: <sos> in the review ten years later the commission is still proposing that it should remain in opt-out <eos>\n",
      "test targ: <sos> in the review now 10 years on the commission is still proposing that the opt-out should continue to exist <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.5638753414154052\n",
      "test pred: <sos> environmental pollution by cadmium means a very important health and economic cost in particular of water usage <eos>\n",
      "test targ: <sos> cadmium pollution of the environment carries very high health and economic costs including water purification costs <eos>\n",
      "test pred: <sos> finally mr president i really do think that the paper should be included in the definition of packaging <eos>\n",
      "test targ: <sos> finally mr president i really do think that gift wrap should be covered by the definition of packaging <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.4443696737289429\n",
      "test pred: <sos> the reality is that under the institutional solutions we adopt on the ecb we are fearful of some because they are afraid that other institutional mechanisms will be able to\n",
      "test targ: <sos> the truth is that in fact the institutional solutions we adopt with regard to the central bank are sometimes frightening because we sometimes have the impression that they could be imposed on other institutional mechanisms or even the union itself <eos>\n",
      "test pred: <sos> six of the 12 candidate countries have a rate of unemployment above 10% which leads to social exclusion and poverty <eos>\n",
      "test targ: <sos> in six of the 12 candidate countries unemployment exceeds 10% leading to social exclusion and poverty <eos>\n",
      "Step:26  WER:0.5260416667\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.7875572681427\n",
      "test pred: <sos> we as parliamentarians cannot be allowed to get on with it <eos>\n",
      "test targ: <sos> we as parliamentarians cannot be treated in this way <eos>\n",
      "test pred: <sos> the european parliament spoke at the negotiating table with the council and the commission with a single voice despite the political <unk> and the countries of origin so that it\n",
      "test targ: <sos> the european parliament spoke with one voice at the negotiating table with the council and the commission going beyond political affiliations and countries of origin and thus succeeded in having a greater impact on the results of the negotiation helping to significantly improve the initial structure of the provisions <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.3393200635910034\n",
      "test pred: <sos> curiously we always remember that we have the best consumer protection system in the world <eos>\n",
      "test targ: <sos> the funny thing is that we always claim that we have the best consumer protection in the world <eos>\n",
      "test pred: <sos> mr president commissioner ladies and gentlemen the world economic forum in switzerland was held the world social forum in brazil <eos>\n",
      "test targ: <sos> mr president commissioner ladies and gentlemen the world economic forum met in switzerland and the world social forum in brazil <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.0946935653686523\n",
      "test pred: <sos> the majority of the <unk> million people who live in the least developed countries are now over a tenth of the entire world's population fighting to survive on less than\n",
      "test targ: <sos> most of the <unk> million people who live in the least developed countries just over <unk> of the world's entire population are struggling to survive on less than two dollars a day <eos>\n",
      "test pred: <sos> the directive also lays down that if broadcasting bodies are allowed in future to provide online access to their own audiovisual productions within the framework of services or if necessary\n",
      "test targ: <sos> finally the directive also regulates when broadcasting organisations are entitled to use their archive productions for <unk> services and when they must first sign a contract with all the rightholders involved in the tv production giving them the right to do so <eos>\n",
      "Step:27  WER:0.6449579832\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.5056395053863525\n",
      "test pred: <sos> mr president political realism has led the commission to conclude that it would be inappropriate to amend the 1997 proposal at this time <eos>\n",
      "test targ: <sos> mr president political realism has led the commission to conclude that it is not appropriate at this stage to modify its proposal of 1997 <eos>\n",
      "test pred: <sos> we all know that the problem is caused by the increase in trade especially the <unk> <unk> between the eu and russia and on the other hand the general <unk>\n",
      "test targ: <sos> we all noted that the origin of this problem was the growth in trade namely the increase in trade between the eu and russia on the one hand and the overall increase of trade with russia including <unk> on the other <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.201897883415222\n",
      "test pred: <sos> this is the kind of thing we can expect from a failure to reach an agreement which is therefore preferable to an unfortunate agreement <eos>\n",
      "test targ: <sos> this is the sort of thing we can expect of a negotiator so it is better not to have an agreement at all than to have a flawed one <eos>\n",
      "test pred: <sos> the commission communication before us concerns as everyone knows the commitment made by the european car industry to increase the chances of survival of pedestrians and cyclists by a vehicle\n",
      "test targ: <sos> as we all know the commission communication we are examining today is about a commitment by the european automobile industry designed to increase the chances of survival of pedestrians and cyclists involved in collisions with motor vehicles <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.929662823677063\n",
      "test pred: <sos> the regional government estimates that its inclusion in the european high-speed network and more specifically in the high-speed train <unk> <unk> <unk> <unk> <unk> will make a significant contribution to\n",
      "test targ: <sos> the regional government considers that including it in the european high-speed train routes and more specifically in the route for the high-speed train from madrid to <unk> would bring it closer to the axis of the mediterranean basin considerably helping its development and its recovery from the shortfall that has accumulated in recent decades in terms of communications <eos>\n",
      "test pred: <sos> for example the total emission reduction required by japan is 14% and canada is about 15% and this is a matter of figures which will require a real commitment from\n",
      "test targ: <sos> the total emission reduction required for example for japan is 14% and for canada around 15% so there is still a real effort to be made for these countries <eos>\n",
      "Step:28  WER:0.6333333333\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.236770462989807\n",
      "test pred: <sos> this <unk> must be based on the european parliament's proposal on the individual legislative powers <eos>\n",
      "test targ: <sos> in accordance with the european parliament's proposal individual responsibility must form the basis for this collection target <eos>\n",
      "test pred: <sos> it is not a very simple question if we bear in mind that the free movement of persons and goods is a fundamental pillar of the european union <eos>\n",
      "test targ: <sos> it is not a very simple issue when we consider that the european union's basic pillars incorporate the notion of the free movement of people and goods <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.3329872131347655\n",
      "test pred: <sos> 'a little less <unk> <unk> <eos>\n",
      "test targ: <sos> 'not so <unk> says the italian <eos>\n",
      "test pred: <sos> we are not completely reassured especially as as i said to you mr bolkestein's intentions are very familiar with them <eos>\n",
      "test targ: <sos> we are certainly not reassured by this particularly since as i have just said to you we are perfectly well aware of mr bolkestein's intentions <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.631760263442993\n",
      "test pred: <sos> that is why last month we proposed changes to be made to the pact in order to adopt more flexible criteria so that it becomes an increasingly intelligent instrument for\n",
      "test targ: <sos> this is why last month we floated ideas for making adjustments and introducing more sophisticated criteria for applying the pact with a view to making it an increasingly valuable instrument for promoting stability and growth <eos>\n",
      "test pred: <sos> this is therefore a simple formal competence that is fully regulated <eos>\n",
      "test targ: <sos> we are therefore talking about a formal and fully regulated competence <eos>\n",
      "Step:29  WER:0.6063432836\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.4224931955337525\n",
      "test pred: <sos> this process was already completed at the end of 1993 <eos>\n",
      "test targ: <sos> this process was concluded as early as the end of 1993 <eos>\n",
      "test pred: <sos> another issue concerned internal conflicts which at least in two of the countries affected by the disaster will be dealt with <eos>\n",
      "test targ: <sos> another question touched upon the internal conflicts raging in at least two of the countries affected by the disaster <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.258276915550232\n",
      "test pred: <sos> as a result we have not yet achieved what we wanted namely a commission in which officials are really responsible and able to take responsibility for this <eos>\n",
      "test targ: <sos> as a result we still have not got what we set out to achieve namely a commission in which officials are really responsible and can also shoulder that responsibility <eos>\n",
      "test pred: <sos> (the sitting was suspended at <unk> <unk> and resumed at 9 <unk> <eos>\n",
      "test targ: <sos> (the sitting was adjourned at <unk> <unk> and resumed at 9 <unk> <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.2354121446609496\n",
      "test pred: <sos> commissioner you are right when you ask for more particularly on the subject of human rights and the issue of tibet but we cannot accept the <unk> steps that turkey\n",
      "test targ: <sos> they must be fully <unk> commissioner you are right to demand more particularly regarding political rights and cyprus <eos>\n",
      "test pred: <sos> my group also wants to strengthen the provisions on family reunification even further <eos>\n",
      "test targ: <sos> my group also aims to further strengthen the provisions on family unity <eos>\n",
      "Step:30  WER:0.6561844864\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.5971133232116699\n",
      "test pred: <sos> i know that we have found solutions to the many discussions we have had in a council and commission meeting but i believe that we have found solutions which of\n",
      "test targ: <sos> i know that this puts us into a conflict situation with the council and the commission but i believe that our numerous discussions have brought solutions which of course ensure that the commission's responsibility is made clear that it will continue so to speak to supervise the aviation safety agency but that it is clear at the same time that the aviation safety agency must also take responsibility for itself in its area of expertise <eos>\n",
      "test pred: <sos> the institutions of the european union and as president-in-office of the council cannot ignore this situation <eos>\n",
      "test targ: <sos> the institutions of the european union and you as president-in-office of the council cannot tolerate this situation <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.4494539737701415\n",
      "test pred: <sos> if china does not do that then china must be allowed to <unk> <eos>\n",
      "test targ: <sos> if china does not accept this then we must expel china from the <unk> <eos>\n",
      "test pred: <sos> europe can do a great deal but the members who say that it should carry out a thorough analysis may perhaps be right not only in terms of what it\n",
      "test targ: <sos> europe has the potential to do a great deal but those members may be right who say that it needs to carry out a considerable amount of <unk> not only in terms of what it fails to give as regards development aid but what it fails to give by refusing to change its policies too <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.4800952672958374\n",
      "test pred: <sos> in fact laboratories scientists responsible scientists and not <unk> as some people think they are making millions and millions of human beings out of a dreadful fate today to return\n",
      "test targ: <sos> they hope very soon to make it possible for untold millions of human beings to escape a dreadful fate these people would then be able to resume a normal life and regain their dignity as human beings <eos>\n",
      "test pred: <sos> i do not understand why the president was able to reject my question because when i spoke about mr aznar i made it quite clear that these were the intentions\n",
      "test targ: <sos> i fail to understand on what grounds the president has decided to reject my question because when i spoke about mr aznar i did so objectively and clearly: this was mr <unk> clear intention in threatening chile and mexico countries which furthermore are <unk> which makes the situation all the more unacceptable <eos>\n",
      "Step:31  WER:0.7098646035\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  2.3868921756744386\n",
      "test pred: <sos> mr president i believe we can be satisfied - and my group is certainly not only because this debate with an important event which has been held with the international\n",
      "test targ: <sos> mr president i feel that we can be satisfied - at least my group is satisfied - that not only is this debate being held at an important juncture when a process is under way to mobilise the international press a process which has currently been suspended in south africa but it also has a symbolic <unk> do poor communities have priority access to medication which can relieve their suffering and delay their death or not? <eos>\n",
      "test pred: <sos> mr corbett's proposal is therefore to restrict the access of european citizens to the work they have elected in order to represent them in the european union and to defend\n",
      "test targ: <sos> what mr corbett is proposing is therefore to limit the access of european citizens to the work of those whom they have elected to represent them and to defend their interests within the european union <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.3059341192245484\n",
      "test pred: <sos> mr president commissioner ladies and gentlemen there are still two opposing views on migration <eos>\n",
      "test targ: <sos> – mr president commissioner ladies and gentlemen two points of view are always <unk> with as regards the topic of migration <eos>\n",
      "test pred: <sos> the united nations is best placed to find a lasting solution to the conflict in the middle east <eos>\n",
      "test targ: <sos> the un is the international organisation best placed to bring about a lasting solution to the conflict in the middle east <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.7457357168197631\n",
      "test pred: <sos> the crucial factor was that these sectors would at last have legislation and would also be covered by the directive <eos>\n",
      "test targ: <sos> it is now crucial that these sectors can at long last look forward to a regulation so that they too come under the scope of the directive <eos>\n",
      "test pred: <sos> i hope that we will be able to count on the support of other groups on these aspects and other important amendments <eos>\n",
      "test targ: <sos> i hope that we are successful in winning the support of other groups for this and other important amendments <eos>\n",
      "Step:32  WER:0.5343137255\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.9960562229156493\n",
      "test pred: <sos> more legislation means no more consumer protection <eos>\n",
      "test targ: <sos> more statutory provisions do not mean greater protection of consumers <eos>\n",
      "test pred: <sos> in my country the republic of ireland eight days a <unk> <unk> <unk> is being held in the 15 member states on the nice treaty <eos>\n",
      "test targ: <sos> in my country the republic of ireland in eight days time there is a referendum on the treaty of nice unique among the 15 member states <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.7928044319152832\n",
      "test pred: <sos> we could do much more given the great commitments we have made to reduce co2 and greenhouse gas emissions and reduce dependence on imports of energy <eos>\n",
      "test targ: <sos> we could do rather more considering that we have made great commitments in reducing co2 and greenhouse gases and to reduce dependence on energy imports <eos>\n",
      "test pred: <sos> in this context it is important that khartoum and rebels should contribute to the work of the international criminal court for darfur and that the donor community is prepared to\n",
      "test targ: <sos> in this context it is important that khartoum and the rebels contribute and assist the work of the international criminal court on darfur and that the donor community is ready to provide an immediate peace dividend once a positive outcome emerges from abuja <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.2699551105499267\n",
      "test pred: <sos> women must of course have the same opportunities the same conditions as pension programmes for men and women must also be made easier for both men to reconcile work and\n",
      "test targ: <sos> women must of course be given the same job opportunities working conditions and pension schemes as men and we must make it easier for both sexes to reconcile work and family life especially if we want to encourage people to have children <eos>\n",
      "test pred: <sos> i am very pleased that in the three years and over which i have been in kosovo we have not only experienced a transformation of the economic and social situation\n",
      "test targ: <sos> i am very pleased that in the three years and more that i have been visiting kosovo we have seen not only a transformation of the economic and social situation on the ground but the establishment of the provisional <unk> the political structure to reflect the wishes of the people of kosovo themselves <eos>\n",
      "Step:33  WER:0.5569358178\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.7739843606948853\n",
      "test pred: <sos> i should like to take this opportunity to point out that we in the european parliament have a consultative body which is the body that we have in the european\n",
      "test targ: <sos> i should also like to remind you at this juncture as a member of the scientific and technological options assessment <unk> that we have a novel type of body available to us in the european parliament the stoa panel whose function is to provide members of this house with aspects of scientific assessment that they may need in order to clarify the positions that our various committees are required to take <eos>\n",
      "test pred: <sos> the world of niger delta has certainly finally reached its conclusion <eos>\n",
      "test targ: <sos> admittedly the world has finally turned its attention to the fate of niger <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.348304319381714\n",
      "test pred: <sos> as a mayor of <unk> <unk> i was directly struck by the floods that hit the czech republic on that occasion several of our cities were devastated and many people\n",
      "test targ: <sos> as mayor of <unk> <unk> i had first-hand experience of the floods which hit the czech republic and which led to a number of our towns being inundated and many people being left homeless <eos>\n",
      "test pred: <sos> we must avoid a civil war <unk> a crisis scenario in <unk> which would be extremely dangerous and regrettable for the <unk> and all its inhabitants <eos>\n",
      "test targ: <sos> we must avoid being caught up in a civil war between ethnic groups a crisis situation similar to that in the congo which would be extremely dangerous and regrettable for côte d'ivoire and all its inhabitants <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.1314925670623779\n",
      "test pred: <sos> mr president the european commission shares the concerns about child labour in general and in particular the production of sports goods <eos>\n",
      "test targ: <sos> mr president the european commission shares the concerns expressed about the use of child labour in general and particularly in the production of sports equipment <eos>\n",
      "test pred: <sos> it is not by chance that we should have as many bus drivers as possible but in order to be able to develop <unk> both in rural and concentrated areas\n",
      "test targ: <sos> it was not created in order to employ as many bus drivers as possible; on the contrary it was created so that attractive facilities can be offered in rural and more densely populated areas to ensure the mobility of all <eos>\n",
      "Step:34  WER:0.6536964981\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.506272053718567\n",
      "test pred: <sos> as you know this summit was negotiated with one side and the other <eos>\n",
      "test targ: <sos> as you will be aware this summit was negotiated with another <unk> it was not imposed on them nor could it be <eos>\n",
      "test pred: <sos> the next item is voting time <eos>\n",
      "test targ: <sos> – the next item is voting time <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.8148116111755371\n",
      "test pred: <sos> i commend the report however and hope that the commissioner will be able to make a legislative instrument <eos>\n",
      "test targ: <sos> but i commend the report and hope that the commissioner will be able to take it forward to a legislative instrument <eos>\n",
      "test pred: <sos> at european development days this issue was dealt with in detail by kofi annan who created a foundation for assisting rural areas and developing agriculture <eos>\n",
      "test targ: <sos> during the european development days this question was dealt with in some detail by kofi annan who has set up a foundation to aid rural regions and develop agriculture <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  2.4039947986602783\n",
      "test pred: <sos> the reasons for pilotage excluded from the scope of the directive in my view are applied to all technical services provided that the quality of the service and safety of\n",
      "test targ: <sos> as i see it the reasons for excluding pilotage services from the scope of the directive apply to all technical nautical services quality of service and the safety of operations are part and parcel of all these services <eos>\n",
      "test pred: <sos> these are critical remarks that indicate what still remains to be done by the candidate countries <eos>\n",
      "test targ: <sos> these are quite specific points that show where individual candidate countries still have to make enormous efforts <eos>\n",
      "Step:35  WER:0.5375000000\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.5364004135131837\n",
      "test pred: <sos> unfortunately our economies are still developing in a way that does not meet the needs of children <eos>\n",
      "test targ: <sos> unfortunately our economies still operate in a way that fails to take account of children’s needs <eos>\n",
      "test pred: <sos> for all these reasons i am very much in favour of your being a party to the drafting of the report <eos>\n",
      "test targ: <sos> for all these reasons as someone who has been involved in working on the report i wholeheartedly commend the report to you for adoption <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.5990901947021485\n",
      "test pred: <sos> the emphasis must be placed on reducing environmental dangers with a view to causing human health or causing diseases whose economic and social cost is considerable <eos>\n",
      "test targ: <sos> emphasis must be placed on reducing the environmental dangers which have repercussions on human health or cause diseases at huge economic and social cost <eos>\n",
      "test pred: <sos> in addition over recent years there has been an interesting and instructive development in solvit including the fact that the resolution of problems by several solvit centres is more structured\n",
      "test targ: <sos> moreover over the past few years we have seen interesting spontaneous developments such as the more structural resolution of problems by several solvit centres even though this is not part of the solvit mandate as described in the commission recommendation <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.3058943033218384\n",
      "test pred: <sos> this proposal is destined to <unk> the european union <eos>\n",
      "test targ: <sos> this proposal is designed to <unk> the european union <eos>\n",
      "test pred: <sos> we must anticipate measures which protect the rights of suspects because ultimately they are the very people who are not convicted and therefore their rights must be protected <eos>\n",
      "test targ: <sos> something needs to be put in there to guarantee the rights of suspects because at the end of the day they are <unk> they have not been convicted and therefore their rights should be protected <eos>\n",
      "Step:36  WER:0.6423982869\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.828316903114319\n",
      "test pred: <sos> mr president once again we are confronted with an ethical minefield with the progress of biotechnology on human beings <eos>\n",
      "test targ: <sos> mr president once again we face a fundamental ethical debate on developments in biotechnology as applied to humans <eos>\n",
      "test pred: <sos> the june movement believes that we should make a start on the <unk> <eos>\n",
      "test targ: <sos> the committee on agriculture does in fact believe that we should have the courage to begin to move out of it <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.0797261953353883\n",
      "test pred: <sos> the most important issue even if we are now beginning to realise that this will not be the focus of the work to be done over the next commitment period\n",
      "test targ: <sos> the most important issue even if we are now beginning to understand that it will probably not be possible to concentrate on it is about what will happen during the next commitment period <eos>\n",
      "test pred: <sos> i am talking about new substantial aid which the european union is preparing to grant <eos>\n",
      "test targ: <sos> i hear talk of very large sums of money that the european union is about to appropriate in further aid <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.4853457689285279\n",
      "test pred: <sos> at this point i must say that the meetings organised by the european parliament in cancún were particularly intense and that we received strong support from some developing countries <eos>\n",
      "test targ: <sos> at this point i must say how well the european parliament meetings in cancún were received and how well we were supported by some of the developing countries <eos>\n",
      "test pred: <sos> i congratulate mr watson and all those who have participated in the drafting of the report <eos>\n",
      "test targ: <sos> i congratulate mr watson and all concerned in putting together this report <eos>\n",
      "Step:37  WER:0.6070588235\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  2.2074437618255613\n",
      "test pred: <sos> the unanimity principle means paralysis <eos>\n",
      "test targ: <sos> unanimity is the system that allows a few to impose their will on everyone else <eos>\n",
      "test pred: <sos> in february 2007 we started negotiations on a new partnership and cooperation agreement with the eu <eos>\n",
      "test targ: <sos> in february 2007 we began negotiations on a new partnership and association agreement between ukraine and the eu <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  2.1809457302093507\n",
      "test pred: <sos> if though a international criminal court <unk> the trial of a few <unk> who bear a great responsibility for the massacres in the former yugoslavia if anything else for genocide\n",
      "test targ: <sos> even with one international criminal court conducting an investigation into a few <unk> who bear a large part of the responsibility for the massacres in the former yugoslavia and another conducting an investigation into the genocide in rwanda however there is absolutely nothing to suggest that all those guilty of offences which are just as serious will be prosecuted <eos>\n",
      "test pred: <sos> we also welcome the introduction by uzbekistan of a <unk> and the abolition of the death penalty which is due to come into force in january 2008 <eos>\n",
      "test targ: <sos> we also welcome <unk> introduction of habeas corpus and the abolition of the death penalty which should come into effect in january 2008 <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.3280039787292481\n",
      "test pred: <sos> it is time to ensure a future for the stability and growth pact <eos>\n",
      "test targ: <sos> now is also the time to ensure that the stability and growth pact will have a future <eos>\n",
      "test pred: <sos> i am sorry that the ppe-de group has taken rule 115 because the last version of the report in french yesterday evening was removed from the <unk> <eos>\n",
      "test targ: <sos> i regret that the group of the european people' s party (christian democrats) and european democrats have quoted rule 115 because the last version of the text which was the french one was actually issued at <unk> <unk> <eos>\n",
      "Step:38  WER:0.6363636364\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.6604788780212403\n",
      "test pred: <sos> in coal the challenges are even greater in terms of the use of energy production and its role in achieving our aims to combat climate change <eos>\n",
      "test targ: <sos> with regard to coal the challenges are even greater due to its implications for energy production and for achieving our objectives for combating climate change <eos>\n",
      "test pred: <sos> it would have been good to know who these candidates are <eos>\n",
      "test targ: <sos> it would have been good to know who the candidates are <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.375168800354004\n",
      "test pred: <sos> the andersson report looks to future generations but it also aims to modernise the system so that social security can be achieved and our aims and are fighting against poverty\n",
      "test targ: <sos> the andersson report is thinking about the next generations but is also thinking about bringing things up to date so that social security pensions fulfil our objectives and combat poverty <eos>\n",
      "test pred: <sos> the united nations security council remains the main forum for resolving the iraq issue <eos>\n",
      "test targ: <sos> the un security council is still the main forum for resolving the iraqi issue <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  0.7555806875228882\n",
      "test pred: <sos> the fact is however that the proposal does not introduce software patents and not all the dreadful effects that the <unk> of <unk> want to make them believe <eos>\n",
      "test targ: <sos> but the fact is that the proposal does not introduce software patents and will not have all the terrible effects that the <unk> of doom would have you believe <eos>\n",
      "test pred: <sos> consequently the primary objective of this technology appears to be another of speeding up the liberalisation of international trade and increasing the <unk> of the production and distribution of the\n",
      "test targ: <sos> consequently the overriding aim of this technology appears to be something else - speeding up the liberalisation of international trade and increasing the <unk> of the mass production and distribution of foods throughout the world with consequences for the <unk> of agri-food production and for the decline of small-scale local farmers thereby calling sustainable development into question <eos>\n",
      "Step:39  WER:0.4797687861\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.3616084098815917\n",
      "test pred: <sos> the decisions that were taken during the irish presidency – and are – are a good basis for our efforts to prevent <unk> <eos>\n",
      "test targ: <sos> the decisions taken by the council during the irish presidency were – and continue to be – a good start in our efforts to prevent cardiovascular diseases <eos>\n",
      "test pred: <sos> first of all it is true that the soil is <unk> international as a air and water which are always in the process of being promoted <eos>\n",
      "test targ: <sos> the issue of land use was outside the scope of that report <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.8244190454483032\n",
      "test pred: <sos> it is quite simply a matter of saying that mobile phones cause our brains <eos>\n",
      "test targ: <sos> however it is easier to claim that mobile phones <unk> our brains <eos>\n",
      "test pred: <sos> the same principle applies to bananas but we agreed that there was a need for further assessment especially in order to take account of the eu's outermost regions and so\n",
      "test targ: <sos> the same principle applies on bananas but we agreed on additional evaluation notably to take into account the eu's outermost regions and that will happen <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.7205453157424926\n",
      "test pred: <sos> we hope that the wording adopted by the commission will make it possible for the speedy adoption of a european party statute not to give too much space to the\n",
      "test targ: <sos> we hope that the wording selected will allow the commission to finalise a european party statute quickly not set the majority requirements in the council too high and respect full co-decision for the european parliament <eos>\n",
      "test pred: <sos> the ad hoc committee on human genetics and other new technologies in modern medicine have not achieved the goal i am trying to achieve <eos>\n",
      "test targ: <sos> the temporary committee on human genetics and other new technologies in modern medicine did not reach the destination i expected <eos>\n",
      "Step:40  WER:0.6706443914\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.5674760580062865\n",
      "test pred: <sos> i said president abbas that we welcome this initiative but i have no doubt that such an agreement is not the real goal but merely the beginning of a process\n",
      "test targ: <sos> i have commended president abbas for taking this initiative but at the same time i have been clear that it is not an end in itself but only the start of a process <eos>\n",
      "test pred: <sos> we have finally ensured our representation and i thank you for pointing that out <eos>\n",
      "test targ: <sos> we have at last secured our representative and thank you for pointing that out <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.7576526880264283\n",
      "test pred: <sos> madam president ladies and gentlemen this is an ambitious proposal to improve road safety to ensure free movement to combat fraud and driving licences <eos>\n",
      "test targ: <sos> madam president ladies and gentlemen here is an ambitious proposal for improving road safety ensuring freedom of movement and combating driving licence fraud <eos>\n",
      "test pred: <sos> it is inadequate <eos>\n",
      "test targ: <sos> it is inadequate <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.6170607805252075\n",
      "test pred: <sos> the vote will take place tomorrow at 12 noon <eos>\n",
      "test targ: <sos> the vote will take place tomorrow at 12 noon <eos>\n",
      "test pred: <sos> mr president ladies and gentlemen commissioners i believe that i have fought a kind of record today because i have been allocated 15 minutes of speaking time as rapporteur on\n",
      "test targ: <sos> mr president ladies and gentlemen commissioners i have probably set some kind of record here today for i have been given a whole 15 minutes in which to speak because i am rapporteur for three different but quite similar matters <eos>\n",
      "Step:41  WER:0.6036446469\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.1589208126068116\n",
      "test pred: <sos> i therefore believe mr president that the commission is very fearful of proposing a <unk> <unk> which parliament is merely a small <unk> <eos>\n",
      "test targ: <sos> thus i am very much afraid mr president that when the commission proposed a <unk> <unk> parliament gave it only a small <unk> <unk> <eos>\n",
      "test pred: <sos> the netherlands has made a clear submission and tabled a substantial number of amendments <eos>\n",
      "test targ: <sos> the netherlands has clearly formulated a request and tabled quite a few amendments <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  0.9758808255195618\n",
      "test pred: <sos> if this house adopts resolutions that reflect the thinking of social democrats we are in favour of them <eos>\n",
      "test targ: <sos> if this house adopts resolutions reflecting what we social democrats think then they will enjoy our support <eos>\n",
      "test pred: <sos> this right to develop national competences in different sectors will then have to be adapted <eos>\n",
      "test targ: <sos> this right to skill development must then be adapted at national level within different sectors <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.763839888572693\n",
      "test pred: <sos> tariff barriers also constitute serious obstacles to access to the eu market <eos>\n",
      "test targ: <sos> non-tariff barriers also constitute major obstacles to accessing the european union market <eos>\n",
      "test pred: <sos> women have only the task of bringing the water <eos>\n",
      "test targ: <sos> they are the water carriers <eos>\n",
      "Step:42  WER:0.5327102804\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.9270899057388307\n",
      "test pred: <sos> despite some understandable political and philosophical differences all those who commit to these principles must build a genuine european partnership <eos>\n",
      "test targ: <sos> despite some natural political and ideological differences those who are committed to these principles should build a truly european partnership <eos>\n",
      "test pred: <sos> your energy your enthusiasm and not least your never <unk> <unk> in dealing with difficult issues <eos>\n",
      "test targ: <sos> indeed their vigour their enthusiasm and most importantly their all-round good humour in dealing with difficult issues should be recognised <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.1719024181365967\n",
      "test pred: <sos> in fact the conclusions do not seem to me to be equal to the needs and possibilities of the moment <eos>\n",
      "test targ: <sos> indeed the conclusions do not strike me as being adequate to address current needs and opportunities <eos>\n",
      "test pred: <sos> all this should make it possible to improve controls make them effective and ensure better cooperation <eos>\n",
      "test targ: <sos> all of this should enable us to improve the checks make them effective and guarantee greater cooperation <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.5459996938705445\n",
      "test pred: <sos> on the basis of the commission's report i can only conclude that the member states that intend to join the european union still have to resolve an awful lot of\n",
      "test targ: <sos> i have no option but to conclude from the commission report that the member states wishing to accede still have to sort out an awful lot of things <eos>\n",
      "test pred: <sos> well then this proposal for a regulation will enable us to define the legal basis which is essential for the implementation of a accession partnership so that it will facilitate\n",
      "test targ: <sos> this proposal for a regulation will now enable us to create the necessary legal basis to establish an accession partnership in order to help turkey fulfil the copenhagen criteria <eos>\n",
      "Step:43  WER:0.6279069767\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  2.0469493865966797\n",
      "test pred: <sos> it is our taxpayers' money that we must ensure that it is used for the objectives of the international community and the european union <eos>\n",
      "test targ: <sos> we are talking about taxpayers' money here and we must be sure that it is used to achieve the goals set by the international community and the european union <eos>\n",
      "test pred: <sos> for this reason we are very much in favour of a compromise amendment and hope that we will support it <eos>\n",
      "test targ: <sos> for this reason we are asking most emphatically for an evaluation proposal and we hope that you will support the amendment <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.2927233219146728\n",
      "test pred: <sos> mr president i would just like to make a brief comment on the santini report which the pse members voted for the santini report and the amendments which were also\n",
      "test targ: <sos> mr president i just want to make a brief comment on the santini report which along with the substantive amendments from the committee on citizens' freedoms and rights justice and home affairs the members belonging to the group of the party of european socialists have voted for even though it is clear to us that there are elements in this report about which the council will have serious misgivings some of them originating from the german government <eos>\n",
      "test pred: <sos> according to the european commission's communication the gap between total public and private research expenditure in the usa and the eu is steadily increasing from eur 12 billion in 1992\n",
      "test targ: <sos> according to the commission communication the gap between total public and private expenditure on research in the usa and in the eu is continuing to widen having grown from eur 12 billion in 1992 to some eur 60 billion in 1998 <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.4995037078857423\n",
      "test pred: <sos> i should like to point out that the french agriculture minister mr <unk> announced recently that he would undertake to carry out approximately <unk> of the type of tests we\n",
      "test targ: <sos> i would also draw your attention to the fact that recently mr <unk> the french minister of agriculture announced his intention of putting in place approximately 40 000 tests of the type that i have been discussing with you here this afternoon to determine the presence of bse and the levels of <unk> in france <eos>\n",
      "test pred: <sos> why are we giving these refugees back in the hands of the regime that we want to <unk> <eos>\n",
      "test targ: <sos> why do we throw those refugees back into the hands of the regime we want to change? <eos>\n",
      "Step:44  WER:0.6476190476\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.242613959312439\n",
      "test pred: <sos> they claim to be <unk> they want to see the first step in the european economic <unk> to overcome this deficit but the big companies are no longer allowed to\n",
      "test targ: <sos> you claim that you want to overcome and i quote 'the democratic deficit in european economic policy' but the first step you must take in order to overcome this deficit would be to deprive large employers of their divine right to get rid of all the people who work to improve their existence and you would do this by banning employers from making collective redundancies and forcing them to deduct from profits and from dividends stored up by shareholders whatever is necessary to safeguard all the jobs that are at risk <eos>\n",
      "test pred: <sos> let us therefore move forward and deal with these issues in a special directive <eos>\n",
      "test targ: <sos> <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.0856666564941406\n",
      "test pred: <sos> mr president on monday when i was opening this debate i asked members to be aware of the correspondence between mr bush and mr prodi <eos>\n",
      "test targ: <sos> mr president on monday when the sitting opened i asked for members to be provided with a copy of the letters exchanged between mr bush and mr prodi <eos>\n",
      "test pred: <sos> i have already consulted and already consulted the people who tabled the resolution and who are in favour of it <eos>\n",
      "test targ: <sos> i have discussed it with those who tabled the motion and they have no objection <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  0.8848182916641235\n",
      "test pred: <sos> we live with the <unk> they form part of our soul and we cannot live without them <eos>\n",
      "test targ: <sos> we live with <unk> they are part of our soul and we cannot live without them <eos>\n",
      "test pred: <sos> it has become clear that people are being wrongly considered as a candidate country and in view of what was said during the dutch government crisis <eos>\n",
      "test targ: <sos> there it emerged that people felt very unfairly identified as an unreliable candidate country in connection with what had been said during the dutch government crisis <eos>\n",
      "Step:45  WER:0.6923076923\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.4151681423187257\n",
      "test pred: <sos> if you believe that legislation is not enough you are presenting us with a review <eos>\n",
      "test targ: <sos> if you think the legislation is no good let us revise it <eos>\n",
      "test pred: <sos> in this respect effective enforcement of national regulations on issues ranging from tax to the attention of wealthy <unk> could make a decisive contribution <eos>\n",
      "test targ: <sos> effective enforcement of existing national legislation on issues ranging from <unk> to age limits could make a significant difference <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.385317897796631\n",
      "test pred: <sos> i hope that the expected expertise regarding the <unk> of soft pvc will be achieved <eos>\n",
      "test targ: <sos> i hope that the last outstanding report will be included in the assessment of soft pvc in the green paper <eos>\n",
      "test pred: <sos> i have no information on the advisory group of legal services created pursuant to the interinstitutional agreement of 20 december 1994 on a fast-track working method for formal codification of\n",
      "test targ: <sos> i do not know whether this was referred to the consultative working party of the legal services set up under the interinstitutional agreement of 20 december 1994 on an accelerated working method for official codification of legislative texts <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.3732892751693726\n",
      "test pred: <sos> finally i should like to assure parliament that the search for a peaceful solution to the conflict based on the positions of the european union is a priority of the\n",
      "test targ: <sos> finally i should like to assure this house that the search for a peaceful solution to the <unk> conflict based on the well-established positions of the european union is a priority of the irish presidency and that we shall make every attempt to take the roadmap forward and to convince the parties to the conflict to make the necessary efforts to achieve a comprehensive and lasting peace for the benefit of all the peoples and states of the region <eos>\n",
      "test pred: <sos> but pvc also has other properties including the right to refuse to be particularly hazardous <eos>\n",
      "test targ: <sos> however pvc also has other qualities being the source of unusually hazardous waste <eos>\n",
      "Step:46  WER:0.5838926174\n",
      "\n",
      "-----------------Test Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  1.7212472438812256\n",
      "test pred: <sos> this has contributed to reducing environmental pollution and to strengthening the internal market in packaging and packaging <eos>\n",
      "test targ: <sos> in this way the directive has helped to reduce environmental impacts and to strengthen the internal market for packaging and packaging waste <eos>\n",
      "test pred: <sos> this is the council regulation on the criteria for determining the member state responsible for examining an asylum application which was the subject of the marinho report <eos>\n",
      "test targ: <sos> i refer to the council regulation establishing the criteria for determining the member states responsible for examining asylum applications which was the subject of the marinho report <eos>\n",
      "----Task 1 ----\n",
      "Inner Loss:  1.16028151512146\n",
      "test pred: <sos> the recitals of the draft resolution which state that we must take firm steps to oppose trafficking in human beings and the references to the un texts seem to us\n",
      "test targ: <sos> those recitals in the draft which state that there is a need to take strong measures to halt trafficking in human beings and the references to the un texts appear to us to be broadly correct <eos>\n",
      "test pred: <sos> i have had the opportunity to see the scenario of these manoeuvres and they have indeed been with the <unk> of refugees <eos>\n",
      "test targ: <sos> i have had sight of the scenario for these manoeuvres which do indeed have something to do with <unk> refugees <eos>\n",
      "----Task 2 ----\n",
      "Inner Loss:  1.6135246992111205\n",
      "test pred: <sos> i have the impression - and this should not be reduced to your <unk> mr byrne - that whenever the codecision procedure is applied the commission proposals are more appropriate\n",
      "test targ: <sos> i have the impression - and this is not meant to detract from your invaluable contribution mr byrne - that whenever the codecision procedure is applied the commission' s proposals are more concrete more <unk> and better adapted from the outset to the wishes and the working methods of the european parliament; these proposals serve as an example of the way we should be working here all the time <eos>\n",
      "test pred: <sos> i would add one thing to this: there is a list of projects to be drawn up <eos>\n",
      "test targ: <sos> there is something else i would <unk> we have produced a list known as the quick start list which provides for a series of projects <eos>\n",
      "Step:47  WER:0.5492957746\n",
      "Step:47  avg_WER:0.5967420680\n"
     ]
    }
   ],
   "source": [
    "test_print_coef = 10\n",
    "\n",
    "history = {\"test_loss\": [], \"test_wer\": [] }\n",
    "\n",
    "n = 0\n",
    "\n",
    "f_test = open( \"test.log\", mode=\"w\", encoding = \"UTF-8\" )\n",
    "\n",
    "tes_global_step = 0\n",
    "\n",
    "print( \"Test\")\n",
    "test_wer = 0\n",
    "n = 0\n",
    "model.eval()\n",
    "for i, test_task in enumerate(db_test):\n",
    "    tes_global_step += 1\n",
    "    display = True\n",
    "    print(\"\\n-----------------Test Mode-----------------\\n\")\n",
    "    wer = fine_tuning( batch = test_task, train_step = 5, lr1 = 1e-4, display = display )\n",
    "    test_wer += wer\n",
    "    n += 1\n",
    "    history[\"test_wer\"].append( test_wer / n )\n",
    "    print(f\"Step:{tes_global_step}  WER:{ wer:.10f}\")\n",
    "    f_test.write( \"Epoch: \" + str(epoch) + \", Step: \" + str(tes_global_step) + \", WER: \" + str(test_wer / n ) + \"\\n\" )\n",
    "    f_test.flush()\n",
    "\n",
    "f_test.close()\n",
    "print(f\"Step:{tes_global_step}  avg_WER:{ test_wer / n :.10f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "a758d471-3194-407c-a9a8-b4cb7abb7469",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"best_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "77ac70b1-d9ba-4ffd-ad05-234f8ea3a61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.to('cpu'), \"best_model_cpu.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "04c8f963-4254-4313-8c7b-46dbf5510e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = torch.load( \"best_model_cpu.pth\", weights_only = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4970d79-2b46-4f25-9803-d1f7d9323fb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
