{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8bf21a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install janome\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "#from torchtext.vocab import vocab\n",
    "#import torchtext.transforms as T\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#from torchvision import transforms\n",
    "import numpy as np\n",
    "import math\n",
    "import janome\n",
    "from janome.tokenizer import Tokenizer\n",
    "#import spacy\n",
    "from collections import Counter\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import time\n",
    "#from torchtext.vocab import build_vocab_from_iterator\n",
    "import levenshtein\n",
    "import json\n",
    "import pickle\n",
    "from timm.scheduler import CosineLRScheduler\n",
    "import nltk\n",
    "from nltk import bleu_score\n",
    "from torch.nn.init import constant_, xavier_uniform_\n",
    "from torch.nn.parameter import Parameter\n",
    "from transformers import  get_linear_schedule_with_warmup\n",
    "from collections import OrderedDict\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "346dde95-1503-4243-9963-31dcdce0e2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device( \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d745f5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49528 20556\n"
     ]
    }
   ],
   "source": [
    "with open( \"corpus/id_to_word_s.pkl\", \"rb\" ) as f:\n",
    "    token_list = pickle.load(f)\n",
    "with open( \"corpus/id_to_word_t.pkl\", \"rb\" ) as f:\n",
    "    token_list_en = pickle.load(f)\n",
    "with open( \"corpus/word_to_id_s.pkl\", \"rb\" ) as f:\n",
    "    idx_list = pickle.load(f)\n",
    "with open( \"corpus/word_to_id_t.pkl\", \"rb\" ) as f:\n",
    "    idx_list_en = pickle.load(f)\n",
    "\n",
    "pad_idx_s = idx_list['<pad>']\n",
    "pad_idx_t = idx_list_en['<pad>']\n",
    "\n",
    "enc_vocab_size, dec_vocab_size = len(token_list), len(token_list_en)\n",
    "print(enc_vocab_size, dec_vocab_size)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86f1161a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<sos>', '<eos>', '<unk>', '<blank>', '<mask>', 'der']\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([ 0, 1,2,3,4,5,6 ])\n",
    "\n",
    "#print( token_list[2] )\n",
    "\n",
    "#n = 0\n",
    "\n",
    "#ii = [ i for i in a ]\n",
    "\n",
    "#print( ii )\n",
    "\n",
    "b = [ token_list[i.item()] for i in a ]\n",
    "\n",
    "print( b )\n",
    "\n",
    "\n",
    "d = idx_list['<pad>']\n",
    "\n",
    "print( d )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "649837a5-2937-4e81-880b-a57f2532b967",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    ''' ミニバッチデータを作成するクラス\n",
    "        torch.utils.data.Datasetクラスを継承し，\n",
    "        以下の関数を定義する\n",
    "        __len__: 総サンプル数を出力する関数\n",
    "        __getitem__: 1サンプルのデータを出力する関数\n",
    "    feat_scp:  特徴量リストファイル\n",
    "    label_scp: ラベルファイル\n",
    "    feat_mean: 特徴量の平均値ベクトル\n",
    "    feat_std:  特徴量の次元毎の標準偏差を並べたベクトル \n",
    "    pad_index: バッチ化の際にフレーム数を合わせる\n",
    "               ためにpaddingする整数値\n",
    "    splice:    前後(splice)フレームを特徴量を結合する\n",
    "               splice=1とすると，前後1フレーム分結合\n",
    "               するので次元数は3倍になる．\n",
    "               splice=0の場合は何もしない\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 et,\n",
    "                 pad_index_s,\n",
    "                 pad_index_t\n",
    "                 ):\n",
    "\n",
    "        # 読み込みながら情報を取得する\n",
    "        self.pad_index_s = pad_index_s\n",
    "        self.pad_index_t = pad_index_t\n",
    "        print( \"pad_index_s:\", self.pad_index_s )\n",
    "        print( \"pad_index_t:\", self.pad_index_t )\n",
    "        self.en_list = []\n",
    "        self.en_lens = []\n",
    "        self.et_list = []\n",
    "        self.et_lens = []\n",
    "        self.num_data = 0\n",
    "        for line in et:\n",
    "            # 各行をスペースで区切り，\n",
    "            # リスト型の変数にする\n",
    "            self.en_list.append( line['target'] )\n",
    "            self.en_lens.append( len( line['target']) )\n",
    "            #self.en_att_list.append( line['target']['attention_mask'] )\n",
    "\n",
    "            self.et_list.append( line['source'] )\n",
    "            self.et_lens.append( len( line['source']) )\n",
    "            #self.et_att_list.append( line['source']['attention_mask'] )\n",
    "            self.num_data += 1\n",
    "\n",
    "        #self.en_list = np.int64( np.array( self.en_list ) )\n",
    "        #self.et_list = np.int64( np.array( self.et_list ) )\n",
    "        self.en_lens = np.int64( np.array( self.en_lens ) )\n",
    "        self.et_lens = np.int64( np.array( self.et_lens ) )\n",
    "\n",
    "        # フレーム数の最大値を得る\n",
    "        self.max_en_len = np.max(self.en_lens)\n",
    "        # ラベル長の最大値を得る\n",
    "        self.max_et_len = np.max(self.et_lens)\n",
    "\n",
    "        for n in range(self.num_data):\n",
    "            if n % 10000 == 0:\n",
    "                print( \"n:\", n )\n",
    "            # 埋めるフレームの数\n",
    "            # = 最大フレーム数 - 自分のフレーム数\n",
    "            pad_len = self.max_en_len - self.en_lens[n]\n",
    "            # pad_indexの値で埋める\n",
    "            tmp = self.en_list[n]\n",
    "            self.en_list[n] = np.pad( tmp, (0, pad_len), mode='constant', constant_values=(self.pad_index_t, self.pad_index_t ))\n",
    "            pad_len = self.max_et_len - self.et_lens[n]\n",
    "            # pad_indexの値で埋める\n",
    "            self.et_list[n] = np.pad(self.et_list[n],[0, pad_len],mode='constant', constant_values=self.pad_index_s)\n",
    "\n",
    "        self.en_list = np.int64( np.array( self.en_list ) )\n",
    "        self.et_list = np.int64( np.array( self.et_list ) )\n",
    "\n",
    "    def __len__(self):\n",
    "        ''' 学習データの総サンプル数を返す関数\n",
    "        本実装では発話単位でバッチを作成するため，\n",
    "        総サンプル数=発話数である．\n",
    "        '''\n",
    "        return self.num_data\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ''' サンプルデータを返す関数\n",
    "        本実装では発話単位でバッチを作成するため，\n",
    "        idx=発話番号である．\n",
    "        '''\n",
    "\n",
    "        # ラベル\n",
    "        et = self.et_list[idx]\n",
    "        et_len = self.et_lens[idx]\n",
    "\n",
    "        # 発話ID\n",
    "        en = self.en_list[idx]\n",
    "        en_len = self.en_lens[idx]\n",
    "\n",
    "        # 特徴量，ラベル，フレーム数，\n",
    "        # ラベル長，発話IDを返す\n",
    "        #return (jps, jp_lens, ens,  en_lens)\n",
    "        return (et, et_len, en, en_len )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa12965b-1beb-4be9-967f-7c533486462d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it_train: 499000\n",
      "it_val: 500\n",
      "pad_index_s: 0\n",
      "pad_index_t: 0\n",
      "n: 0\n",
      "n: 10000\n",
      "n: 20000\n",
      "n: 30000\n",
      "n: 40000\n",
      "n: 50000\n",
      "n: 60000\n",
      "n: 70000\n",
      "n: 80000\n",
      "n: 90000\n",
      "n: 100000\n",
      "n: 110000\n",
      "n: 120000\n",
      "n: 130000\n",
      "n: 140000\n",
      "n: 150000\n",
      "n: 160000\n",
      "n: 170000\n",
      "n: 180000\n",
      "n: 190000\n",
      "n: 200000\n",
      "n: 210000\n",
      "n: 220000\n",
      "n: 230000\n",
      "n: 240000\n",
      "n: 250000\n",
      "n: 260000\n",
      "n: 270000\n",
      "n: 280000\n",
      "n: 290000\n",
      "n: 300000\n",
      "n: 310000\n",
      "n: 320000\n",
      "n: 330000\n",
      "n: 340000\n",
      "n: 350000\n",
      "n: 360000\n",
      "n: 370000\n",
      "n: 380000\n",
      "n: 390000\n",
      "n: 400000\n",
      "n: 410000\n",
      "n: 420000\n",
      "n: 430000\n",
      "n: 440000\n",
      "n: 450000\n",
      "n: 460000\n",
      "n: 470000\n",
      "n: 480000\n",
      "n: 490000\n",
      "pad_index_s: 0\n",
      "pad_index_t: 0\n",
      "n: 0\n"
     ]
    }
   ],
   "source": [
    "with open(\"data_train_it.pkl\", mode=\"rb\") as f:\n",
    "    it_train = pickle.load(f)\n",
    "with open(\"data_val_it.pkl\", mode=\"rb\") as f:\n",
    "    it_val = pickle.load(f)\n",
    "\n",
    "#it_train = it_train[:10000]\n",
    "\n",
    "print( \"it_train:\", len( it_train ) )\n",
    "print( \"it_val:\", len( it_val ) )\n",
    "\n",
    "train_dataset = SequenceDataset( it_train, pad_idx_s, pad_idx_t )\n",
    "val_dataset = SequenceDataset( it_val, pad_idx_s, pad_idx_t  )\n",
    "    \n",
    "batch_size = 20\n",
    "#num_workers = 4 if torch.cuda.is_available() else 0\n",
    "num_workers = 0 if device == torch.device( 'cpu' ) else 4\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers)\n",
    "# 開発データのDataLoaderを呼び出す\n",
    "# 開発データはデータはシャッフルしない\n",
    "val_loader = DataLoader(val_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers)    \n",
    "\n",
    "#Iter = iter(train_loader)\n",
    "#xdata, xatt, ydata, yatt = next(Iter) #教師データ、ラベルデータ\n",
    "#print(xdata, xatt, ydata, yatt)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b825cab-27ba-4da2-8050-2631036aa9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        #self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        #return self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66322069-a9f2-458e-af00-45953e75b074",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    '''\n",
    "    位置埋め込み （Positional embedding）\n",
    "    dim_embedding: 埋込み次元\n",
    "    max_len      : 入力の最大系列長\n",
    "    '''\n",
    "    def __init__(self, dim_embedding: int, max_len: int=5000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pos_emb = nn.Embedding(max_len, dim_embedding)\n",
    "\n",
    "    '''\n",
    "    位置エンコーディングの順伝播\n",
    "    x: 位置エンコーディングを埋め込む対象のテンソル,\n",
    "       [バッチサイズ, 系列長, 埋め込み次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        seq = x.shape[1]\n",
    "        positions = torch.arange(start=0, end=seq, step=1, device=x.device).to(torch.long)\n",
    "        positions = self.pos_emb(positions)[:seq,:]\n",
    "        \n",
    "        return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cea6ff9-e8c6-436a-941b-34a4e2e4fced",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fPositionalEmbedding(nn.Module):\n",
    "    '''\n",
    "    位置埋め込み （Positional embedding）\n",
    "    dim_embedding: 埋込み次元\n",
    "    max_len      : 入力の最大系列長\n",
    "    '''\n",
    "    def __init__(self, dim_embedding: int, name, max_len: int=5000):\n",
    "        super().__init__()\n",
    "\n",
    "        #self.pos_emb = nn.Embedding(max_len, dim_embedding)\n",
    "        self.name = name\n",
    "        \n",
    "    '''\n",
    "    位置エンコーディングの順伝播\n",
    "    x: 位置エンコーディングを埋め込む対象のテンソル,\n",
    "       [バッチサイズ, 系列長, 埋め込み次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor, weights):\n",
    "        seq = x.shape[1]\n",
    "        positions = torch.arange(start=0, end=seq, step=1, device=x.device).to(torch.long)\n",
    "        #positions = self.pos_emb(positions)[:seq,:]\n",
    "        positions = F.embedding( positions, weights[self.name] )[:seq,:]\n",
    "        \n",
    "        return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e05abc0d-48d7-4e58-8bc4-0a5310e59330",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "    '''\n",
    "    自己アテンション\n",
    "    dim_hidden: 入力特徴量の次元\n",
    "    num_heads : マルチヘッドアテンションのヘッド数\n",
    "    qkv_bias  : クエリなどを生成する全結合層のバイアスの有無\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 qkv_bias: bool=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # 特徴量を各ヘッドのために分割するので、\n",
    "        # 特徴量次元をヘッド数で割り切れるか検証\n",
    "        assert dim_hidden % num_heads == 0\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # ヘッド毎の特徴量次元\n",
    "        dim_head = dim_hidden // num_heads\n",
    "\n",
    "        # ソフトマックスのスケール値\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        # ヘッド毎にクエリ、キーおよびバリューを生成するための全結合層\n",
    "        self.proj_in_q = nn.Linear(\n",
    "            dim_hidden, dim_hidden, bias=qkv_bias)\n",
    "        self.proj_in_k = nn.Linear(\n",
    "            dim_hidden, dim_hidden, bias=qkv_bias)\n",
    "        self.proj_in_v = nn.Linear(\n",
    "            dim_hidden, dim_hidden, bias=qkv_bias)\n",
    "\n",
    "        #self.in_proj_weight = nn.Parameter( torch.randn( dim_hidden * 3, dim_hidden ) )\n",
    "        #self.in_proj_bias = nn.Parameter( torch.randn( dim_hidden * 3 ) )\n",
    "        #self.in_proj_weight = Parameter( torch.empty( dim_hidden * 3, dim_hidden ) )\n",
    "        #self.in_proj_bias = Parameter( torch.empty( dim_hidden * 3 ) )\n",
    "\n",
    "        # 各ヘッドから得られた特徴量を一つにまとめる全結合層\n",
    "        self.proj_out = nn.Linear(dim_hidden, dim_hidden)\n",
    "        \n",
    "        #self._reset_parameters()\n",
    "        \n",
    "    #def _reset_parameters(self):\n",
    "    #    xavier_uniform_( self.in_proj_weight )\n",
    "    #    constant_( self.in_proj_bias, 0.0 )\n",
    "\n",
    "    def split_head(self, x):\n",
    "        x = torch.tensor_split(x, self.num_heads, dim = 2)\n",
    "        x = torch.stack(x, dim = 1)\n",
    "        return x\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, mask: torch.Tensor):\n",
    "\n",
    "        #bs_q, ns_q, ds_q = q.size()\n",
    "        #bs_k, ns_k, ds_k = k.size()\n",
    "        \n",
    "        ##  k = v assumpotion\n",
    "        #if q is k:\n",
    "        #    qkv = q @ self.in_proj_weight.transpose(-2,-1) + self.in_proj_bias\n",
    "        #    qkv = qkv.view( bs_q, ns_q, 3, ds_q )\n",
    "        #    q, k, v = torch.unbind( qkv, dim = 2 )\n",
    "        #else:\n",
    "        #    W_q, W_kv = self.in_proj_weight.split([ds_q, ds_q * 2])\n",
    "        #    b_q, b_kv = self.in_proj_bias.split([ds_q, ds_q * 2])\n",
    "        #    q = q @ W_q.transpose(-2,-1) + b_q\n",
    "        #    kv =  k @ W_kv.transpose(-2,-1) + b_kv\n",
    "        #    kv = kv.view( bs_k, ns_k, 2, ds_k )\n",
    "        #    k, v = torch.unbind( kv, dim = 2 )\n",
    "        \n",
    "        q = self.proj_in_q(q)\n",
    "        k = self.proj_in_k(k)\n",
    "        v = self.proj_in_v(v)\n",
    "        \n",
    "        q = self.split_head(q)\n",
    "        k = self.split_head(k)\n",
    "        v = self.split_head(v)\n",
    "\n",
    "        # クエリとキーの行列積とアテンションの計算(今回マスクは不使用)\n",
    "        # attnは[バッチサイズ, ヘッド数, 特徴量数, 特徴量数]\n",
    "        attn = q.matmul(k.transpose(-2, -1))\n",
    "        #print( \"attn size:\", attn.size() )\n",
    "        #print( \"mask size:\", mask.size() )\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill_(mask, -torch.finfo(torch.float).max)\n",
    "        attn = (attn * self.scale).softmax(dim=-1)\n",
    "\n",
    "        # アテンションとバリューの行列積によりバリューを収集\n",
    "        # xは[バッチサイズ, ヘッド数, 特徴量数, ヘッドの特徴量次元]\n",
    "        x = attn.matmul(v)\n",
    "\n",
    "        # permute関数により\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数, ヘッドの特徴量次元]\n",
    "        # flatten関数により全てのヘッドから得られる特徴量を連結して、\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数 * ヘッドの特徴量次元]\n",
    "        x = x.permute(0, 2, 1, 3).flatten(2)\n",
    "        x = self.proj_out(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a51e5fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class feMHA(nn.Module):\n",
    "    '''\n",
    "    自己アテンション\n",
    "    dim_hidden: 入力特徴量の次元\n",
    "    num_heads : マルチヘッドアテンションのヘッド数\n",
    "    qkv_bias  : クエリなどを生成する全結合層のバイアスの有無\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 qkv_bias: bool=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # 特徴量を各ヘッドのために分割するので、\n",
    "        # 特徴量次元をヘッド数で割り切れるか検証\n",
    "        assert dim_hidden % num_heads == 0\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # ヘッド毎の特徴量次元\n",
    "        dim_head = dim_hidden // num_heads\n",
    "\n",
    "        # ソフトマックスのスケール値\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        # ヘッド毎にクエリ、キーおよびバリューを生成するための全結合層\n",
    "        #self.proj_in = nn.Linear(\n",
    "        #    dim_hidden, dim_hidden * 3, bias=qkv_bias)\n",
    "\n",
    "        # 各ヘッドから得られた特徴量を一つにまとめる全結合層\n",
    "        #self.proj_out = nn.Linear(dim_hidden, dim_hidden)\n",
    "\n",
    "    def split_head(self, x):\n",
    "        x = torch.tensor_split(x, self.num_heads, dim = 2)\n",
    "        x = torch.stack(x, dim = 1)\n",
    "        return x\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor,  v: torch.Tensor, mask: torch.Tensor, i, weights ):\n",
    "        bs, ns = q.shape[:2]\n",
    "\n",
    "        #qkv = self.proj_in(x)\n",
    "        q = F.linear(q, weights['encoder.encoder_layers.' + str(i) + '.attention.proj_in_q.weight'], weights['encoder.encoder_layers.' + str(i) + '.attention.proj_in_q.bias'])\n",
    "        k = F.linear(k, weights['encoder.encoder_layers.' + str(i) + '.attention.proj_in_k.weight'], weights['encoder.encoder_layers.' + str(i) + '.attention.proj_in_k.bias'])\n",
    "        v = F.linear(v, weights['encoder.encoder_layers.' + str(i) + '.attention.proj_in_v.weight'], weights['encoder.encoder_layers.' + str(i) + '.attention.proj_in_v.bias'])\n",
    "\n",
    "        q = self.split_head( q )\n",
    "        k = self.split_head( k )\n",
    "        v = self.split_head( v )\n",
    "\n",
    "        # クエリとキーの行列積とアテンションの計算(今回マスクは不使用)\n",
    "        # attnは[バッチサイズ, ヘッド数, 特徴量数, 特徴量数]\n",
    "        attn = q.matmul(k.transpose(-2, -1))\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill_(mask, -torch.finfo(torch.float).max)\n",
    "        attn = (attn * self.scale).softmax(dim=-1)\n",
    "\n",
    "        # アテンションとバリューの行列積によりバリューを収集\n",
    "        # xは[バッチサイズ, ヘッド数, 特徴量数, ヘッドの特徴量次元]\n",
    "        x = attn.matmul(v)\n",
    "\n",
    "        # permute関数により\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数, ヘッドの特徴量次元]\n",
    "        # flatten関数により全てのヘッドから得られる特徴量を連結して、\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数 * ヘッドの特徴量次元]\n",
    "        x = x.permute(0, 2, 1, 3).flatten(2)\n",
    "        #x = self.proj_out(x)\n",
    "        x = F.linear(x, weights['encoder.encoder_layers.' + str(i) + '.attention.proj_out.weight'], weights['encoder.encoder_layers.' + str(i) + '.attention.proj_out.bias'])\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5839e53-a27a-4be2-8e24-b5a5b0deec8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fdsMHA(nn.Module):\n",
    "    '''\n",
    "    自己アテンション\n",
    "    dim_hidden: 入力特徴量の次元\n",
    "    num_heads : マルチヘッドアテンションのヘッド数\n",
    "    qkv_bias  : クエリなどを生成する全結合層のバイアスの有無\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 qkv_bias: bool=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # 特徴量を各ヘッドのために分割するので、\n",
    "        # 特徴量次元をヘッド数で割り切れるか検証\n",
    "        assert dim_hidden % num_heads == 0\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # ヘッド毎の特徴量次元\n",
    "        dim_head = dim_hidden // num_heads\n",
    "\n",
    "        # ソフトマックスのスケール値\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        # ヘッド毎にクエリ、キーおよびバリューを生成するための全結合層\n",
    "        #self.proj_in = nn.Linear(\n",
    "        #    dim_hidden, dim_hidden * 3, bias=qkv_bias)\n",
    "\n",
    "        # 各ヘッドから得られた特徴量を一つにまとめる全結合層\n",
    "        #self.proj_out = nn.Linear(dim_hidden, dim_hidden)\n",
    "\n",
    "    def split_head(self, x):\n",
    "        x = torch.tensor_split(x, self.num_heads, dim = 2)\n",
    "        x = torch.stack(x, dim = 1)\n",
    "        return x\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor,  v: torch.Tensor, mask: torch.Tensor, i, weights ):\n",
    "        bs, ns = q.shape[:2]\n",
    "\n",
    "        #qkv = self.proj_in(x)\n",
    "        #print( \"size q:\", q.size() )\n",
    "        #print( \"size weigths q:\", weights['d_layers.' + str(i) + '.crossattn.proj_in_q.weight'].size() )\n",
    "        #print( \"size k:\", k.size() )\n",
    "        #print( \"size weights k:\", weights['d_layers.' + str(i) + '.crossattn.proj_in_k.weight'].size() )\n",
    "        q = F.linear(q, weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_in_q.weight'], weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_in_q.bias'])\n",
    "        k = F.linear(k, weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_in_k.weight'], weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_in_k.bias'])\n",
    "        v = F.linear(v, weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_in_v.weight'], weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_in_v.bias'])\n",
    "\n",
    "        q = self.split_head( q )\n",
    "        k = self.split_head( k )\n",
    "        v = self.split_head( v )\n",
    "\n",
    "        # view関数により\n",
    "        # [バッチサイズ, 特徴量数, QKV, ヘッド数, ヘッドの特徴量次元]\n",
    "        # permute関数により\n",
    "        # [QKV, バッチサイズ, ヘッド数, 特徴量数, ヘッドの特徴量次元]\n",
    "        #qkv = qkv.view(\n",
    "        #    bs, ns, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        # クエリ、キーおよびバリューに分解\n",
    "        #q, k, v = qkv.unbind(0)\n",
    "\n",
    "        # クエリとキーの行列積とアテンションの計算(今回マスクは不使用)\n",
    "        # attnは[バッチサイズ, ヘッド数, 特徴量数, 特徴量数]\n",
    "        attn = q.matmul(k.transpose(-2, -1))\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill_(mask, -torch.finfo(torch.float).max)      \n",
    "        attn = (attn * self.scale).softmax(dim=-1)\n",
    "\n",
    "        # アテンションとバリューの行列積によりバリューを収集\n",
    "        # xは[バッチサイズ, ヘッド数, 特徴量数, ヘッドの特徴量次元]\n",
    "        x = attn.matmul(v)\n",
    "\n",
    "        # permute関数により\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数, ヘッドの特徴量次元]\n",
    "        # flatten関数により全てのヘッドから得られる特徴量を連結して、\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数 * ヘッドの特徴量次元]\n",
    "        x = x.permute(0, 2, 1, 3).flatten(2)\n",
    "        #x = self.proj_out(x)\n",
    "        x = F.linear(x, weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_out.weight'], weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_out.bias'])\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0464026d-c2fd-456a-89dc-e70cd632fbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fdcMHA(nn.Module):\n",
    "    '''\n",
    "    自己アテンション\n",
    "    dim_hidden: 入力特徴量の次元\n",
    "    num_heads : マルチヘッドアテンションのヘッド数\n",
    "    qkv_bias  : クエリなどを生成する全結合層のバイアスの有無\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 qkv_bias: bool=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # 特徴量を各ヘッドのために分割するので、\n",
    "        # 特徴量次元をヘッド数で割り切れるか検証\n",
    "        assert dim_hidden % num_heads == 0\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # ヘッド毎の特徴量次元\n",
    "        dim_head = dim_hidden // num_heads\n",
    "\n",
    "        # ソフトマックスのスケール値\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        # ヘッド毎にクエリ、キーおよびバリューを生成するための全結合層\n",
    "        #self.proj_in = nn.Linear(\n",
    "        #    dim_hidden, dim_hidden * 3, bias=qkv_bias)\n",
    "\n",
    "        # 各ヘッドから得られた特徴量を一つにまとめる全結合層\n",
    "        #self.proj_out = nn.Linear(dim_hidden, dim_hidden)\n",
    "\n",
    "    def split_head(self, x):\n",
    "        x = torch.tensor_split(x, self.num_heads, dim = 2)\n",
    "        x = torch.stack(x, dim = 1)\n",
    "        return x\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor,  v: torch.Tensor, mask: torch.Tensor, i, weights ):\n",
    "        bs, ns = q.shape[:2]\n",
    "\n",
    "        #qkv = self.proj_in(x)\n",
    "        #print( \"size q:\", q.size() )\n",
    "        #print( \"size weigths q:\", weights['d_layers.' + str(i) + '.crossattn.proj_in_q.weight'].size() )\n",
    "        #print( \"size k:\", k.size() )\n",
    "        #print( \"size weights k:\", weights['d_layers.' + str(i) + '.crossattn.proj_in_k.weight'].size() )\n",
    "        q = F.linear(q, weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_in_q.weight'], weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_in_q.bias'])\n",
    "        k = F.linear(k, weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_in_k.weight'], weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_in_k.bias'])\n",
    "        v = F.linear(v, weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_in_v.weight'], weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_in_v.bias'])\n",
    "\n",
    "        q = self.split_head( q )\n",
    "        k = self.split_head( k )\n",
    "        v = self.split_head( v )\n",
    "\n",
    "        # view関数により\n",
    "        # [バッチサイズ, 特徴量数, QKV, ヘッド数, ヘッドの特徴量次元]\n",
    "        # permute関数により\n",
    "        # [QKV, バッチサイズ, ヘッド数, 特徴量数, ヘッドの特徴量次元]\n",
    "        #qkv = qkv.view(\n",
    "        #    bs, ns, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        # クエリ、キーおよびバリューに分解\n",
    "        #q, k, v = qkv.unbind(0)\n",
    "\n",
    "        # クエリとキーの行列積とアテンションの計算(今回マスクは不使用)\n",
    "        # attnは[バッチサイズ, ヘッド数, 特徴量数, 特徴量数]\n",
    "        attn = q.matmul(k.transpose(-2, -1))\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill_(mask, -torch.finfo(torch.float).max)      \n",
    "        attn = (attn * self.scale).softmax(dim=-1)\n",
    "\n",
    "        # アテンションとバリューの行列積によりバリューを収集\n",
    "        # xは[バッチサイズ, ヘッド数, 特徴量数, ヘッドの特徴量次元]\n",
    "        x = attn.matmul(v)\n",
    "\n",
    "        # permute関数により\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数, ヘッドの特徴量次元]\n",
    "        # flatten関数により全てのヘッドから得られる特徴量を連結して、\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数 * ヘッドの特徴量次元]\n",
    "        x = x.permute(0, 2, 1, 3).flatten(2)\n",
    "        #x = self.proj_out(x)\n",
    "        x = F.linear(x, weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_out.weight'], weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_out.bias'])\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88160e0b-1bca-44a6-88d3-22f706a89b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "    '''\n",
    "    Transformerエンコーダ内の順伝播型ニューラルネットワーク\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    dim_feedforward: 中間特徴量の次元\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, dim_feedforward: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(dim_hidden, dim_feedforward)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, dim_hidden)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbd75eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class feFNN(nn.Module):\n",
    "    '''\n",
    "    Transformerエンコーダ内の順伝播型ニューラルネットワーク\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    dim_feedforward: 中間特徴量の次元\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, dim_feedforward: int):\n",
    "        super().__init__()\n",
    "\n",
    "        #self.linear1 = nn.Linear(dim_hidden, dim_feedforward)\n",
    "        #self.linear2 = nn.Linear(dim_feedforward, dim_hidden)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor, i, weights ):\n",
    "        #x = self.linear1(x)\n",
    "        x = F.linear(x, weights['encoder.encoder_layers.' + str(i) + '.fnn.linear1.weight'], weights['encoder.encoder_layers.' + str(i) + '.fnn.linear1.bias'])\n",
    "        x = self.activation(x)\n",
    "        #x = self.linear2(x)\n",
    "        x = F.linear(x, weights['encoder.encoder_layers.' + str(i) + '.fnn.linear2.weight'], weights['encoder.encoder_layers.' + str(i) + '.fnn.linear2.bias'])\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1830c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fdFNN(nn.Module):\n",
    "    '''\n",
    "    Transformerエンコーダ内の順伝播型ニューラルネットワーク\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    dim_feedforward: 中間特徴量の次元\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, dim_feedforward: int):\n",
    "        super().__init__()\n",
    "\n",
    "        #self.linear1 = nn.Linear(dim_hidden, dim_feedforward)\n",
    "        #self.linear2 = nn.Linear(dim_feedforward, dim_hidden)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor, i, weights ):\n",
    "        #x = self.linear1(x)\n",
    "        x = F.linear(x, weights['decoder.decoder_layers.' + str(i) + '.fnn.linear1.weight'], weights['decoder.decoder_layers.' + str(i) + '.fnn.linear1.bias'])\n",
    "        x = self.activation(x)\n",
    "        #x = self.linear2(x)\n",
    "        x = F.linear(x, weights['decoder.decoder_layers.' + str(i) + '.fnn.linear2.weight'], weights['decoder.decoder_layers.' + str(i) + '.fnn.linear2.bias'])\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "caaa86ef-4575-4c9c-9265-a337fb7a9734",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    '''\n",
    "    Transformerエンコーダ層\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    num_heads      : ヘッド数\n",
    "    dim_feedforward: 中間特徴量の次元\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 dim_feedforward: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = MHA(dim_hidden, num_heads, qkv_bias = True)\n",
    "        self.fnn = FNN(dim_hidden, dim_feedforward)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(dim_hidden)\n",
    "        self.norm2 = nn.LayerNorm(dim_hidden)\n",
    "        \n",
    "        self.dropout = nn.Dropout( dropout )\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor):\n",
    "        ''' B2T\n",
    "        x0 = x\n",
    "        x = self.attention( x, x, x, mask )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x = self.norm1(x)\n",
    "        x1 = x\n",
    "        x = self.fnn( x )\n",
    "        x = self.dropout( x )\n",
    "        x =  x + x1 + x0\n",
    "        x = self.norm2( x )\n",
    "        '''\n",
    "        #pre-LN\n",
    "        x0 = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attention( x, x, x, mask )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x1 = x\n",
    "        x = self.norm2( x )\n",
    "        x = self.fnn( x )\n",
    "        x = self.dropout( x )\n",
    "        x =  x + x1\n",
    "        \n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a2533c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fTransformerEncoderLayer(nn.Module):\n",
    "    '''\n",
    "    Transformerエンコーダ層\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    num_heads      : ヘッド数\n",
    "    dim_feedforward: 中間特徴量の次元\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 dim_feedforward: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fattention = feMHA(dim_hidden, num_heads)\n",
    "        self.fffn = feFNN(dim_hidden, dim_feedforward)\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.dropout = nn.Dropout( dropout )\n",
    "        \n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor, i, weights ):\n",
    "        '''B2T\n",
    "        x0 = x\n",
    "        x = self.fattention( x, x, x, mask, i, weights )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['encoder.encoder_layers.' + str(i) + '.norm1.weight'], bias=weights['encoder.encoder_layers.' + str(i) + '.norm1.bias'], eps=1e-05)\n",
    "        x1 = x\n",
    "        x = self.fffn( x, i, weights ) \n",
    "        x = self.dropout( x )\n",
    "        x = x + x1 + x0\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['encoder.encoder_layers.' + str(i) + '.norm2.weight'], bias=weights['encoder.encoder_layers.' + str(i) + '.norm2.bias'], eps=1e-05)\n",
    "        '''\n",
    "        #pre-LN\n",
    "        x0 = x\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['encoder.encoder_layers.' + str(i) + '.norm1.weight'], bias=weights['encoder.encoder_layers.' + str(i) + '.norm1.bias'], eps=1e-05)\n",
    "        x = self.fattention( x, x, x, mask, i, weights )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x1 = x\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['encoder.encoder_layers.' + str(i) + '.norm2.weight'], bias=weights['encoder.encoder_layers.' + str(i) + '.norm2.bias'], eps=1e-05)\n",
    "        x = self.fffn( x, i, weights ) \n",
    "        x = self.dropout( x )\n",
    "        x = x + x1\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "387ffc10-2b89-4039-8a6a-4b4a8fbde394",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    '''\n",
    "    Transformerエンコーダ層\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    num_heads      : ヘッド数\n",
    "    dim_feedforward: 中間特徴量の次元\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 dim_feedforward: int, dropout: float=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.selfattn = MHA(dim_hidden, num_heads, qkv_bias = True)\n",
    "        self.crossattn = MHA(dim_hidden, num_heads, qkv_bias = True)\n",
    "        self.fnn = FNN(dim_hidden, dim_feedforward)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(dim_hidden)\n",
    "        self.norm2 = nn.LayerNorm(dim_hidden)\n",
    "        self.norm3 = nn.LayerNorm(dim_hidden)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor,  self_mask: torch.Tensor,  cross_mask: torch.Tensor):\n",
    "        '''B2T\n",
    "        x0 = x\n",
    "        x = self.selfattn( x, x, x, self_mask )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x = self.norm1( x )\n",
    "        x1 = x\n",
    "        x = self.crossattn( x, y, y, cross_mask )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x1\n",
    "        x = self.norm2( x )\n",
    "        x2 = x\n",
    "        x = self.fnn( x )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x2 + x0\n",
    "        x = self.norm3( x )\n",
    "        '''\n",
    "        #pre-LN\n",
    "        x0 = x\n",
    "        x = self.norm1( x )\n",
    "        x = self.selfattn( x, x, x, self_mask )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x1 = x\n",
    "        x = self.norm2( x )\n",
    "        x = self.crossattn( x, y, y, cross_mask )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x1\n",
    "        x2 = x\n",
    "        x = self.norm3( x )\n",
    "        x = self.fnn( x )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x2\n",
    "        \n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "063637b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fTransformerDecoderLayer(nn.Module):\n",
    "    '''\n",
    "    Transformerエンコーダ層\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    num_heads      : ヘッド数\n",
    "    dim_feedforward: 中間特徴量の次元\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 dim_feedforward: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fselfattn = fdsMHA(dim_hidden, num_heads)\n",
    "        self.fcrossattn = fdcMHA(dim_hidden, num_heads)\n",
    "        self.fffn = fdFNN(dim_hidden, dim_feedforward)\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        #self.norm1 = nn.LayerNorm(dim_hidden)\n",
    "        #self.norm2 = nn.LayerNorm(dim_hidden)\n",
    "        #self.norm3 = nn.LayerNorm(dim_hidden)\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor,  self_mask: torch.Tensor,  cross_mask: torch.Tensor, i, weights):\n",
    "        '''B2T\n",
    "        x0 = x\n",
    "        x = self.fselfattn( x, x, x, self_mask, i, weights )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['decoder.decoder_layers.' + str(i) + '.norm1.weight'], bias=weights['decoder.decoder_layers.' + str(i) + '.norm1.bias'], eps=1e-05)\n",
    "        x1 = x\n",
    "        x = self.fcrossattn( x, y, y, cross_mask, i, weights )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x1\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['decoder.decoder_layers.' + str(i) + '.norm2.weight'], bias=weights['decoder.decoder_layers.' + str(i) + '.norm2.bias'], eps=1e-05)\n",
    "        x2 = x\n",
    "        x = self.fffn( x, i, weights )\n",
    "        x = self.dropout( x ) \n",
    "        x = x + x2 + x0\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['decoder.decoder_layers.' + str(i) + '.norm3.weight'], bias=weights['decoder.decoder_layers.' + str(i) + '.norm3.bias'], eps=1e-05)\n",
    "        '''\n",
    "        #pre-LN\n",
    "        x0 = x\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['decoder.decoder_layers.' + str(i) + '.norm1.weight'], bias=weights['decoder.decoder_layers.' + str(i) + '.norm1.bias'], eps=1e-05)\n",
    "        x = self.fselfattn( x, x, x, self_mask, i, weights )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x1 = x\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['decoder.decoder_layers.' + str(i) + '.norm2.weight'], bias=weights['decoder.decoder_layers.' + str(i) + '.norm2.bias'], eps=1e-05)\n",
    "        x = self.fcrossattn( x, y, y, cross_mask, i, weights )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x1\n",
    "        x2 = x\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['decoder.decoder_layers.' + str(i) + '.norm3.weight'], bias=weights['decoder.decoder_layers.' + str(i) + '.norm3.bias'], eps=1e-05)\n",
    "        x = self.fffn( x, i, weights )\n",
    "        x = self.dropout( x ) \n",
    "        x = x + x2\n",
    "        \n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0509fcd8-bf62-4006-a5ca-94f2fe331e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    '''\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    dim_feedforward: FNNにおける中間特徴量の次元\n",
    "    num_heads      : マルチヘッドアテンションのヘッド数\n",
    "    num_layers     : Transformerエンコーダの層数\n",
    "    '''\n",
    "    def __init__(self, text_vocab_size: int, dim_embedding: int, dim_feedforward: int,\n",
    "                 num_heads: int,  num_layers: int, pad_index:int, dropout: float = 0.1 ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 単語埋め込み\n",
    "        self.embed = nn.Embedding(\n",
    "            text_vocab_size, dim_embedding, padding_idx=pad_index)        \n",
    "        \n",
    "        # 位置エンコーディング\n",
    "        #self.pos_enc = PositionalEncoding(dim_embedding)\n",
    "        self.pos_emb = PositionalEmbedding(dim_embedding)\n",
    "        #self.dropout = nn.Dropout( dropout )\n",
    "        \n",
    "        # Transformerエンコーダ層\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(dim_hidden=dim_embedding, num_heads=num_heads, dim_feedforward = dim_feedforward, dropout = dropout )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(dim_embedding)\n",
    "       \n",
    "        self.pad_index = pad_index\n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x           : 入力, [バッチサイズ, 入力チャネル数, 高さ, 幅]\n",
    "    return_embed: 特徴量を返すかロジットを返すかを選択する真偽値\n",
    "    '''\n",
    "    def forward(self, src: torch.Tensor, src_mask: torch.Tensor=None, src_padding_mask: torch.Tensor=None ):\n",
    "\n",
    "        if src_padding_mask is not None and src_mask is None:\n",
    "            mask = src_padding_mask[:,None,:,None]\n",
    "            mask = mask.expand( (-1, self.num_heads, -1, src.size(1)))\n",
    "        elif src_padding_mask is not None and src_mask is not None:\n",
    "            mask1 = src_padding_mask[:,None,:,None]\n",
    "            mask1 = mask1.expand( (-1, self.num_heads, -1, src.size(1)))\n",
    "            mask2 = src_mask[None,None,:,:]\n",
    "            mask2 = mask2.expand( ( src.size(0), self.num_heads, -1, -1 ) )\n",
    "            mask = torch.logical_or(mask1, mask2 )\n",
    "        elif src_padding_mask is None and src_mask is not None:\n",
    "            mask = src_mask[None,None,:,:]\n",
    "            mask = mask.padding( ( src.size(0), src.size(1), -1, -1 ) )\n",
    "        else:\n",
    "            mask = None\n",
    "\n",
    "        x = self.embed( src ) * math.sqrt( self.dim_embedding )\n",
    "\n",
    "        #x = self.pos_enc( x )\n",
    "        position = self.pos_emb( x )\n",
    "        x = x + position\n",
    "        #x = self.dropout( x )\n",
    "        #x = self.norm( x )\n",
    "        \n",
    "        # Transformerエンコーダ層を適用\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer( x, mask )\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd45f25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fTransformerEncoder(nn.Module):\n",
    "    '''\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    dim_feedforward: FNNにおける中間特徴量の次元\n",
    "    num_heads      : マルチヘッドアテンションのヘッド数\n",
    "    num_layers     : Transformerエンコーダの層数\n",
    "    '''\n",
    "    def __init__(self, text_vocab_size: int, dim_embedding: int, dim_feedforward: int,\n",
    "                 num_heads: int,  num_layers: int, pad_idx:int, dropout: float = 0.1 ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 単語埋め込み\n",
    "        #self.embed = nn.Embedding(\n",
    "        #    text_vocab_size, dim_embedding, padding_idx=pad_index)        \n",
    "        \n",
    "        # 位置エンコーディング\n",
    "        #self.pos_enc = PositionalEncoding(dim_embedding)\n",
    "        self.fpos_emb = fPositionalEmbedding(dim_embedding, \"encoder.pos_emb.pos_emb.weight\" )\n",
    "        #self.dropout = nn.Dropout( dropout )\n",
    "        \n",
    "        # Transformerエンコーダ層\n",
    "        self.fenclayer = fTransformerEncoderLayer(dim_hidden=dim_embedding, num_heads=num_heads, dim_feedforward = dim_feedforward, dropout = dropout )\n",
    "        \n",
    "        # ロジットを生成する前のレイヤー正規化と全結合\n",
    "        #self.norm = nn.LayerNorm(dim_embedding)\n",
    "        \n",
    "        self.pad_idx = pad_idx\n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x           : 入力, [バッチサイズ, 入力チャネル数, 高さ, 幅]\n",
    "    return_embed: 特徴量を返すかロジットを返すかを選択する真偽値\n",
    "    '''\n",
    "    def forward(self, src: torch.Tensor, src_mask: torch.Tensor=None, src_padding_mask: torch.Tensor=None, weights = None ):\n",
    "\n",
    "        if src_padding_mask is not None and src_mask is None:\n",
    "            mask = src_padding_mask[:,None,:,None]\n",
    "            mask = mask.expand( (-1, self.num_heads, -1, src.size(1)))\n",
    "        elif src_padding_mask is not None and src_mask is not None:\n",
    "            mask1 = src_padding_mask[:,None,:,None]\n",
    "            mask1 = mask1.expand( (-1, self.num_heads, -1, src.size(1)))\n",
    "            mask2 = src_mask[None,None,:,:]\n",
    "            mask2 = mask2.expand( ( src.size(0), self.num_heads, -1, -1 ) )\n",
    "            mask = torch.logical_or(mask1, mask2 )\n",
    "        elif src_padding_mask is None and src_mask is not None:\n",
    "            mask = src_mask[None,None,:,:]\n",
    "            mask = mask.padding( ( src.size(0), src.size(1), -1, -1 ) )\n",
    "        else:\n",
    "            mask = None\n",
    "\n",
    "        #x = self.embed( src ) * math.sqrt( self.dim_embedding )\n",
    "        x = F.embedding( src, weights['encoder.embed.weight'], padding_idx = self.pad_idx )  * math.sqrt( self.dim_embedding )\n",
    "        \n",
    "        #x = self.pos_enc( x )\n",
    "        position = self.fpos_emb( x, weights )\n",
    "        x = x + position\n",
    "        #x = self.dropout( x )\n",
    "        ##x = self.norm(x)\n",
    "        #x = F.layer_norm(x, (self.dim_embedding,), weight=weights['encoder.norm.weight'], bias=weights['encoder.norm.bias'], eps=1e-05)\n",
    "    \n",
    "        # Transformerエンコーダ層を適用\n",
    "        #for layer in self.encoder_layers:\n",
    "        #    x = layer( x, mask )\n",
    "        for block in range( self.num_layers ):\n",
    "            x = self.fenclayer(x, mask, block, weights )\n",
    "\n",
    "        #x = self.norm(x)\n",
    "        x = F.layer_norm(x, (self.dim_embedding,), weight=weights['encoder.norm.weight'], bias=weights['encoder.norm.bias'], eps=1e-05)\n",
    "        \n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15582995-ff08-4fbe-9f54-9e61e13f89e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    '''\n",
    "    CaptioningTransformerのコンストラクタ\n",
    "    dim_embedding  : 埋め込み次元\n",
    "    dim_feedforward: FNNの中間特徴次元\n",
    "    num_heads      : マルチヘッドアテンションのヘッド数\n",
    "    num_layers     : Transformerデコーダ層の数\n",
    "    vocab_size     : 辞書の次元\n",
    "    pad_index     : NULLのID\n",
    "    dropout        : ドロップアウト確率\n",
    "    '''\n",
    "    def __init__(self, vocab_size: int, dim_embedding: int, dim_feedforward: int,\n",
    "                 num_heads: int, num_layers: int, \n",
    "                 pad_index: int, dropout: float=0.1, ds_rate: float=0.1 ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 単語埋め込み\n",
    "        self.embed = nn.Embedding(\n",
    "            vocab_size, dim_embedding, padding_idx=pad_index)\n",
    "        \n",
    "        # 位置エンコーディング\n",
    "        #self.pos_enc = PositionalEncoding(dim_embedding)\n",
    "        self.pos_emb = PositionalEmbedding(dim_embedding)\n",
    "        #self.dropout = nn.Dropout( dropout )\n",
    "\n",
    "        # Transformerデコーダ\n",
    "        #self.decoder_layers = nn.ModuleList([\n",
    "        #    TransformerDecoderLayer(\n",
    "        #        dim_embedding, num_heads, dim_feedforward, dropout)\n",
    "        #    for _ in range(num_layers)\n",
    "        #])\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer(dim_hidden=dim_embedding, num_heads=num_heads, dim_feedforward = dim_feedforward, dropout = dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # ロジットを生成する前のレイヤー正規化と全結合\n",
    "        #self.norm = nn.LayerNorm(dim_embedding)\n",
    "        \n",
    "        # 単語出力分布計算\n",
    "        self.linear = nn.Linear(dim_embedding, vocab_size)\n",
    "        \n",
    "        self.pad_index = pad_index\n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "\n",
    "\n",
    "    ''' CaptioningTransformerの順伝播処理\n",
    "    features: 画像特徴量 [バッチサイズ, 埋め込み次元]\n",
    "    captions: 正解キャプション [バッチサイズ, 系列長]\n",
    "\n",
    "    '''\n",
    "    #def forward(self, features: torch.Tensor, caption_lengths: torch.Tensor):\n",
    "    #def forward(self, features: torch.Tensor, captions: torch.Tensor, padding_mask_src: torch.Tensor=None, \\\n",
    "    #            padding_mask_tgt: torch.Tensor=None, mask_tgt: torch.Tensor=None ):\n",
    "    def forward(self, features: torch.Tensor, captions: torch.Tensor, memory_padding_mask: torch.Tensor=None, \\\n",
    "                tgt_padding_mask: torch.Tensor=None, tgt_mask: torch.Tensor=None ):\n",
    "\n",
    "        #feature_lengths = torch.ones( (features.size(0) ), device=features.device ) * features.size(1)\n",
    "\n",
    "        tgt = captions\n",
    "        if tgt_padding_mask is not None and tgt_mask is not None:\n",
    "            self_mask1 = tgt_padding_mask[:,None,:,None]\n",
    "            self_mask1 = self_mask1.expand( (-1, self.num_heads, -1, tgt.size(1)))\n",
    "            #print( \"self_mask1:\", self_mask1.size() )\n",
    "            self_mask2 = tgt_mask[None,None,:,:]\n",
    "            self_mask2 = self_mask2.expand( (tgt.size(0), self.num_heads, -1, -1 ))\n",
    "            #print( \"self_mask2:\", self_mask2.size() )\n",
    "            self_mask = torch.logical_or(self_mask1, self_mask2 )\n",
    "            #print( \"0 self_mask:\", self_mask.size() )\n",
    "        elif tgt_padding_mask is not None and tgt_mask is None:\n",
    "            self_mask = tgt_padding_mask[:,None,:,None]\n",
    "            self_mask = self_mask.expand( (-1, self.num_heads, -1, tgt.size(1)))\n",
    "        elif tgt_padding_mask is None and tgt_mask is not None:\n",
    "            #print(\"tgt_padding_mask is None\")\n",
    "            self_mask = tgt_mask[None,None,:,:]\n",
    "            self_mask = self_mask.expand( (tgt.size(0), self.num_heads, -1, -1 ))\n",
    "            #print( \"0 self_mask size():\", self_mask.size() )\n",
    "        elif tgt_padding_mask is None and tgt_mask is None:\n",
    "            self_mask = None\n",
    "            \n",
    "        if memory_padding_mask is not None:\n",
    "            cross_mask = memory_padding_mask[:,None,None,:]\n",
    "            cross_mask = cross_mask.expand((-1,self.num_heads, tgt.size(1), -1))\n",
    "        else:\n",
    "            cross_mask = None\n",
    "        \n",
    "        # 単語埋め込み [バッチサイズ, 系列長]\n",
    "        # -> [バッチサイズ, 系列長, 埋め込み次元]\n",
    "        embeddings = self.embed(captions) * math.sqrt( self.dim_embedding )\n",
    "        seq = embeddings.shape[1]\n",
    "        \n",
    "        # 位置エンコーディング\n",
    "        #embeddings = self.pos_enc( embeddings )\n",
    "        positions = self.pos_emb(embeddings)\n",
    "        embeddings = embeddings + positions\n",
    "        #embeddings = self.dropout( embeddings )\n",
    "        #embeddings = self.norm(embeddings)\n",
    "        \n",
    "        # Transformerデコーダでキャプション生成\n",
    "        # 画像の特徴も入力する\n",
    "        for layer in self.decoder_layers:\n",
    "            embeddings = layer( embeddings, features, self_mask, cross_mask )\n",
    "\n",
    "        \n",
    "        # [バッチサイズ, 系列長, 埋め込み次元]\n",
    "        # -> [バッチサイズ, 系列長, 辞書の次元]\n",
    "        preds = self.linear(embeddings)\n",
    "        #print( \"argmax of preds:\", torch.argmax( preds, dim = 2 ))\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa9c54fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fTransformerDecoder(nn.Module):\n",
    "    '''\n",
    "    CaptioningTransformerのコンストラクタ\n",
    "    dim_embedding  : 埋め込み次元\n",
    "    dim_feedforward: FNNの中間特徴次元\n",
    "    num_heads      : マルチヘッドアテンションのヘッド数\n",
    "    num_layers     : Transformerデコーダ層の数\n",
    "    vocab_size     : 辞書の次元\n",
    "    pad_index     : NULLのID\n",
    "    dropout        : ドロップアウト確率\n",
    "    '''\n",
    "    def __init__(self, vocab_size: int, dim_embedding: int, dim_feedforward: int,\n",
    "                 num_heads: int, num_layers: int, \n",
    "                 pad_idx: int, dropout: float=0.1, ds_rate: float=0.1 ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 単語埋め込み\n",
    "        #self.embed = nn.Embedding(\n",
    "        #    vocab_size, dim_embedding, padding_idx=pad_index)\n",
    "        \n",
    "        # 位置エンコーディング\n",
    "        #self.pos_enc = PositionalEncoding(dim_embedding)\n",
    "        self.fpos_emb = fPositionalEmbedding(dim_embedding, \"decoder.pos_emb.pos_emb.weight\" )\n",
    "        #self.dropout = nn.Dropout( dropout )\n",
    "        \n",
    "        # Transformerデコーダ\n",
    "        self.fdeclayer = fTransformerDecoderLayer(dim_hidden=dim_embedding, num_heads=num_heads, dim_feedforward = dim_feedforward, dropout = dropout )\n",
    "        \n",
    "        # 単語出力分布計算\n",
    "        #self.linear = nn.Linear(dim_embedding, vocab_size)\n",
    "        \n",
    "        self.pad_idx = pad_idx\n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    ''' CaptioningTransformerの順伝播処理\n",
    "    features: 画像特徴量 [バッチサイズ, 埋め込み次元]\n",
    "    captions: 正解キャプション [バッチサイズ, 系列長]\n",
    "\n",
    "    '''\n",
    "    def forward(self, features: torch.Tensor, captions: torch.Tensor, memory_padding_mask: torch.Tensor=None, \\\n",
    "                tgt_padding_mask: torch.Tensor=None, tgt_mask: torch.Tensor=None, weights = None ):\n",
    "\n",
    "        tgt = captions\n",
    "        if tgt_padding_mask is not None and tgt_mask is not None:\n",
    "            self_mask1 = tgt_padding_mask[:,None,:,None]\n",
    "            self_mask1 = self_mask1.expand( (-1, self.num_heads, -1, tgt.size(1)))\n",
    "            #print( \"self_mask1:\", self_mask1.size() )\n",
    "            self_mask2 = tgt_mask[None,None,:,:]\n",
    "            self_mask2 = self_mask2.expand( (tgt.size(0), self.num_heads, -1, -1 ))\n",
    "            #print( \"self_mask2:\", self_mask2.size() )\n",
    "            self_mask = torch.logical_or(self_mask1, self_mask2 )\n",
    "            #print( \"0 self_mask:\", self_mask.size() )\n",
    "        elif tgt_padding_mask is not None and tgt_mask is None:\n",
    "            self_mask = tgt_padding_mask[:,None,:,None]\n",
    "            self_mask = self_mask.expand( (-1, self.num_heads, -1, tgt.size(1)))\n",
    "        elif tgt_padding_mask is None and tgt_mask is not None:\n",
    "            #print(\"tgt_padding_mask is None\")\n",
    "            self_mask = tgt_mask[None,None,:,:]\n",
    "            self_mask = self_mask.expand( (tgt.size(0), self.num_heads, -1, -1 ))\n",
    "            #print( \"0 self_mask size():\", self_mask.size() )\n",
    "        elif tgt_padding_mask is None and tgt_mask is None:\n",
    "            self_mask = None\n",
    "            \n",
    "        if memory_padding_mask is not None:\n",
    "            cross_mask = memory_padding_mask[:,None,None,:]\n",
    "            cross_mask = cross_mask.expand((-1,self.num_heads, tgt.size(1), -1))\n",
    "        else:\n",
    "            cross_mask = None\n",
    "        \n",
    "        # 単語埋め込み [バッチサイズ, 系列長]\n",
    "        # -> [バッチサイズ, 系列長, 埋め込み次元]\n",
    "        embeddings = F.embedding( captions, weights['decoder.embed.weight'], padding_idx = self.pad_idx )  * math.sqrt( self.dim_embedding )\n",
    "        seq = embeddings.shape[1]\n",
    "        \n",
    "        # 位置エンコーディング\n",
    "        #embeddings = self.pos_enc(embeddings)\n",
    "        positions = self.fpos_emb( embeddings, weights )\n",
    "        embeddings = embeddings + positions\n",
    "        #embeddings = self.dropout( embeddings )\n",
    "        ##embeddings = self.norm(embeddings)\n",
    "        #embeddings = F.layer_norm(embeddings, (self.dim_embedding,), weight=weights['decoder.norm.weight'], bias=weights['decoder.norm.bias'], eps=1e-05)\n",
    "        \n",
    "        # Transformerデコーダでキャプション生成\n",
    "        # 画像の特徴も入力する\n",
    "        #for layer in self.decoder_layers:\n",
    "        #    #embeddings = layer( embeddings, features, tgt_key_padding_mask = padding_mask_tgt, \\\n",
    "        #    #                                memory_key_padding_mask = padding_mask_src, tgt_is_causal = True, tgt_mask = mask_tgt )\n",
    "        for block in range( self.num_layers ):\n",
    "            embeddings = self.fdeclayer(embeddings, features, self_mask, cross_mask, block, weights )\n",
    "  \n",
    "        # [バッチサイズ, 系列長, 埋め込み次元]\n",
    "        # -> [バッチサイズ, 系列長, 辞書の次元]\n",
    "        #preds = self.linear(embeddings)\n",
    "        preds = F.linear( embeddings, weights['decoder.linear.weight'], weights['decoder.linear.bias'] )\n",
    "        #print( \"argmax of preds:\", torch.argmax( preds, dim = 2 ))\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14d1d088-4485-4752-b85b-25197f08824b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim_embedding: int, dim_feedforward: int,\n",
    "                 num_heads: int, num_layers: int, enc_vocab_size: int, dec_vocab_size: int,\n",
    "                 j_pad_index: int,e_pad_index: int, dropout: float=0.1, ds_rate: float=0.1 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = TransformerEncoder(enc_vocab_size, dim_embedding, dim_feedforward, num_heads, num_layers, j_pad_index, dropout )\n",
    "        self.decoder = TransformerDecoder(dec_vocab_size, dim_embedding, dim_feedforward, num_heads, num_layers, e_pad_index, dropout )\n",
    "        self.fencoder = fTransformerEncoder(enc_vocab_size, dim_embedding, dim_feedforward, num_heads, num_layers, j_pad_index, dropout )\n",
    "        self.fdecoder = fTransformerDecoder(dec_vocab_size, dim_embedding, dim_feedforward, num_heads, num_layers, e_pad_index, dropout )\n",
    "        self.j_pad_index = j_pad_index\n",
    "        self.e_pad_index = e_pad_index\n",
    "\n",
    "        self._reset_parameters()\n",
    "        #self._reset_parameters2()\n",
    "    \n",
    "    def _reset_parameters(self):\n",
    "        r\"\"\"Initiate parameters in the transformer model.\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                xavier_uniform_(p)\n",
    "                #xavier_normal_(p)\n",
    "                #kaiming_uniform_(p)\n",
    "                #kaiming_normal_(p)\n",
    "\n",
    "    #def _reset_parameters2(self):\n",
    "    #    for module in self.modules():\n",
    "    #        if isinstance(module, nn.Linear):\n",
    "    #            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    #            if module.bias is not None:\n",
    "    #                nn.init.zeros_(module.bias)\n",
    "    #        elif isinstance(module, nn.Embedding):\n",
    "    #            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    #        elif isinstance(module, nn.LayerNorm):\n",
    "    #            nn.init.zeros_(module.bias)\n",
    "    #            nn.init.ones_(module.weight)\n",
    "        \n",
    "    ''' CaptioningTransformerの順伝播処理\n",
    "    features: 画像特徴量 [バッチサイズ, 埋め込み次元]\n",
    "    captions: 正解キャプション [バッチサイズ, 系列長]\n",
    "\n",
    "    '''\n",
    "    def forward(self, text, dec_input):\n",
    "\n",
    "        seq_len_src = text.shape[1]\n",
    "        seq_len_tgt = dec_input.shape[1]\n",
    "\n",
    "        mask_tgt = nn.Transformer.generate_square_subsequent_mask( seq_len_tgt, dtype=bool ).to(device)\n",
    "        mask_src = torch.zeros((seq_len_src, seq_len_src), device=device).type(torch.bool)\n",
    "\n",
    "        padding_mask_src = (text == idx_list['<pad>'])\n",
    "        padding_mask_tgt = (dec_input == idx_list_en['<pad>'])\n",
    "    \n",
    "        x = self.encoder( text, mask_src, padding_mask_src )\n",
    "        preds = self.decoder(x,dec_input, padding_mask_src, padding_mask_tgt, mask_tgt )\n",
    "\n",
    "        return preds\n",
    "    \n",
    "    def adaptation(self, text, dec_input, weights):\n",
    "\n",
    "        seq_len_src = text.shape[1]\n",
    "        seq_len_tgt = dec_input.shape[1]\n",
    "\n",
    "        mask_tgt = nn.Transformer.generate_square_subsequent_mask( seq_len_tgt, dtype=bool ).to(device)\n",
    "        mask_src = torch.zeros((seq_len_src, seq_len_src), device=device).type(torch.bool)\n",
    "\n",
    "        padding_mask_src = (text == idx_list['<pad>']).to(text.device )\n",
    "        padding_mask_tgt = (dec_input == idx_list_en['<pad>']).to(text.device )\n",
    "        \n",
    "        x = self.fencoder( text, mask_src, padding_mask_src, weights)\n",
    "        preds = self.fdecoder( x, dec_input, padding_mask_src, padding_mask_tgt, mask_tgt, weights )\n",
    "        \n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "88539768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Transformer(512, 2048, 8, 6, enc_vocab_size, dec_vocab_size, idx_list['<pad>'], idx_list_en['<pad>'] ).to(device)\n",
    "dim_hidden = 768\n",
    "dim_feedforward = dim_hidden * 4\n",
    "heads = 12\n",
    "layers =8\n",
    "dropout = 0.1\n",
    "#dim_hidden = 256\n",
    "#dim_feedforward = dim_hidden * 4\n",
    "#heads = 4\n",
    "#layers =4\n",
    "#dropout = 0.0\n",
    "model = Transformer(dim_hidden, dim_feedforward, heads, layers, enc_vocab_size, dec_vocab_size, idx_list['<pad>'], idx_list_en['<pad>'], dropout = dropout ).to(device)\n",
    "model = torch.load( \"best_model.pth\", weights_only = False )\n",
    "model = model.to(device)\n",
    "#model = MyTransformer(768, 768 * 4, 12, 12, enc_vocab_size, dec_vocab_size, idx_list['<pad>'], idx_list_en['<pad>'], start_idx = start_idx_t, max_seq = 200  ).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=idx_list_en['<pad>'])\n",
    "loss_fn = criterion\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b845bbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "outputs1 size: torch.Size([8, 120, 20556])\n",
      "tensor(-10.4474, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "outputs2 size: torch.Size([8, 120, 20556])\n",
      "tensor(-10.4474, device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): TransformerEncoder(\n",
       "    (embed): Embedding(49528, 768, padding_idx=0)\n",
       "    (pos_emb): PositionalEmbedding(\n",
       "      (pos_emb): Embedding(5000, 768)\n",
       "    )\n",
       "    (encoder_layers): ModuleList(\n",
       "      (0-7): 8 x TransformerEncoderLayer(\n",
       "        (attention): MHA(\n",
       "          (proj_in_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_in_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_in_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_out): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (fnn): FNN(\n",
       "          (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELU(approximate='none')\n",
       "        )\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (embed): Embedding(20556, 768, padding_idx=0)\n",
       "    (pos_emb): PositionalEmbedding(\n",
       "      (pos_emb): Embedding(5000, 768)\n",
       "    )\n",
       "    (decoder_layers): ModuleList(\n",
       "      (0-7): 8 x TransformerDecoderLayer(\n",
       "        (selfattn): MHA(\n",
       "          (proj_in_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_in_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_in_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_out): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (crossattn): MHA(\n",
       "          (proj_in_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_in_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_in_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_out): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (fnn): FNN(\n",
       "          (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELU(approximate='none')\n",
       "        )\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (linear): Linear(in_features=768, out_features=20556, bias=True)\n",
       "  )\n",
       "  (fencoder): fTransformerEncoder(\n",
       "    (fpos_emb): fPositionalEmbedding()\n",
       "    (fenclayer): fTransformerEncoderLayer(\n",
       "      (fattention): feMHA()\n",
       "      (fffn): feFNN(\n",
       "        (activation): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (fdecoder): fTransformerDecoder(\n",
       "    (fpos_emb): fPositionalEmbedding()\n",
       "    (fdeclayer): fTransformerDecoderLayer(\n",
       "      (fselfattn): fdsMHA()\n",
       "      (fcrossattn): fdcMHA()\n",
       "      (fffn): fdFNN(\n",
       "        (activation): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#seed = 0\n",
    "\n",
    "#random.seed(seed)\n",
    "#np.random.seed(seed)\n",
    "#torch.manual_seed(seed)\n",
    "#torch.backends.cudnn.benchmark = False\n",
    "#torch.backends.cudnn.deterministic = True\n",
    "\n",
    "text = torch.randint( 0, enc_vocab_size, size=(8, 100 ))\n",
    "dec_input = torch.randint( 0, dec_vocab_size, size=(1,120))\n",
    "\n",
    "seq_len_src = text.shape[1]\n",
    "seq_len_tgt = dec_input.shape[1]\n",
    "\n",
    "mask_tgt = nn.Transformer.generate_square_subsequent_mask( seq_len_tgt, dtype=bool ).to(device)\n",
    "mask_src = torch.zeros((seq_len_src, seq_len_src), device=device).type(torch.bool)\n",
    "\n",
    "padding_mask_src = (text == idx_list['<pad>'])\n",
    "padding_mask_tgt = (dec_input == idx_list_en['<pad>'])\n",
    "\n",
    "weights = OrderedDict(model.named_parameters())\n",
    "model.eval()\n",
    "print( device )\n",
    "outputs1 = model( text.to(device), dec_input.to(device) )\n",
    "print( \"outputs1 size:\", outputs1.size())\n",
    "print( outputs1[0][0][0])\n",
    "\n",
    "#for name in weights:\n",
    "#    print( name )\n",
    "model.eval()\n",
    "outputs2 = model.adaptation( text.to(device), dec_input.to(device), weights )\n",
    "print( \"outputs2 size:\", outputs2.size() )\n",
    "print( outputs2[0][0][0])\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5b01dfbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_global_steps: 2495000\n",
      "num_warmup_steps: 249500.0\n",
      "Train\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 8.012024048096193e-08\n",
      "pred: this is in any case about living gmos which part in the research stage and for not guaranteed by be safe <eos>\n",
      "tar:  this is in any case about living gmos as yet at the research stage and thus not guaranteed to be safe <eos>\n",
      "pred: please would wondering much to say you say that you is be no pressure pressure imposed that ask ask you to oppose any pressure from any country it which part <eos>\n",
      "tar:  i was very glad to hear you say that there must be no new conditions and i would ask you to resist any pressure from whichever country for our side to impose them <eos>\n",
      "epoch:1  Step:2000  loss:1.9840067625  WER:0.3989169675\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this sector also creates many jobs jobs on the processing industry or in the <unk> industry industry of <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:25  loss:1.5095516825  WER:0.3515615441\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 1.6028056112224448e-07\n",
      "pred: special least regions must be given special attention in view of the impact that total decoupling of aid will have on their <eos>\n",
      "tar:  the least-favoured areas must be given special intention in view of the impact the total decoupling of aid will have on them <eos>\n",
      "pred: today have now it thing quite today that do not want to from of a laboratory of do not want to caught <unk> <unk> or <unk> <unk> <unk> of the laboratory <eos>\n",
      "tar:  we have made one thing clear today: we do not want fish out of a <unk> we do not want fish only from aquaculture or a standardised packet off a supermarket shelf <eos>\n",
      "epoch:1  Step:4000  loss:1.7995390892  WER:0.4007858546\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this sector also creates many jobs jobs in the processing industry or in the shipyards industry industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:50  loss:1.5023275375  WER:0.3500645245\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 2.4044088176352706e-07\n",
      "pred: the date the measures taken has not produced successful and the commission continues deeply concerned about the current <eos>\n",
      "tar:  to date the action taken has not been satisfactory and the commission remains very concerned about the situation <eos>\n",
      "pred: we fact we are cooperating with the indonesian government and not only to push but also to work it to a cooperative spirit to behave what a government has to do in the these similar of this type <eos>\n",
      "tar:  in fact we are cooperating with the indonesian government trying not only to insist but also to urge them in a cooperative spirit to do what a government ought to do in handling a problem of this character <eos>\n",
      "epoch:1  Step:6000  loss:1.6264637709  WER:0.3512014787\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore also also creates many jobs jobs on the processing industry or in the shipyards industry industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:75  loss:1.4968359852  WER:0.3488426159\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 3.206012024048096e-07\n",
      "pred: but then can it explain that press that the press and the european parliament are about the eurostat scandal at year year before the were the commission to act under under <eos>\n",
      "tar:  how then will he explain the fact that the press and the european parliament knew about the eurostat scandal a good year before meps forced the commission to act <eos>\n",
      "pred: he has the 15 member states of the european union to take a commitment to their taking they financial to development will will be at of gdp as member states are currently contributing considerably less <eos>\n",
      "tar:  he persuaded the 15 member states of the european union to make a commitment that by 2006 their contribution for development finance will be <unk> of gdp many member states are currently contributing much less than this <eos>\n",
      "epoch:1  Step:8000  loss:1.8112603426  WER:0.3810975610\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this sector also creates many jobs jobs in the processing industry and in the shipyards industry industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:100  loss:1.4890203476  WER:0.3477837499\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 4.007615230460922e-07\n",
      "pred: unfortunately it cannot be be guaranteed <eos>\n",
      "tar:  unfortunately it cannot currently be guaranteed <eos>\n",
      "pred: i welcome the report commitment to a vaccination of vaccinate to keep cattle <eos>\n",
      "tar:  i welcome this report's dedication to a policy to vaccinate to live <eos>\n",
      "epoch:1  Step:10000  loss:1.2537827492  WER:0.3061224490\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this sector also creates many jobs jobs in the processing industry or in the <unk> industry industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:125  loss:1.4888461781  WER:0.3479075848\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 4.809218436873748e-07\n",
      "pred: in measures measures measures are therefore necessary to alleviate the serious effects of the <unk> to renew the agreement with morocco <eos>\n",
      "tar:  sustained socio-economic flanking measures are therefore needed to alleviate the dire effects of this failure to renew the agreement with morocco <eos>\n",
      "pred: madam president commissioner i would firstly like to congratulate the rapporteur mrs prets on her report and mrs mrs maes on the opinion of has drafted on behalf of the committee on foreign affairs human rights common security and defence policy <eos>\n",
      "tar:  madam president commissioner i would firstly like to congratulate the rapporteur mrs prets on her report and also mrs maes on the opinion she has produced on behalf of the committee on foreign affairs human rights common security and defence policy <eos>\n",
      "epoch:1  Step:12000  loss:1.5649846792  WER:0.3478260870\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: yes there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: moreover sector also generates many jobs jobs in the processing industry or in the shipyards industry industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:150  loss:1.4838058424  WER:0.3442174996\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 5.610821643286574e-07\n",
      "pred: time is running and the the markets and economies are not getting <eos>\n",
      "tar:  time is ticking by and rival markets and economies are not asleep <eos>\n",
      "pred: similarly the current negotiations should not be linked to the question of the eu' s finances after 2006 because are separate issues <eos>\n",
      "tar:  similarly the current negotiations should not be linked to the issue of the union' s finances after 2006 these are separate issues <eos>\n",
      "epoch:1  Step:14000  loss:1.4568909407  WER:0.3621867882\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this sector also creates many jobs jobs in the processing industry or in the <unk> industry industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:175  loss:1.4734762096  WER:0.3466394311\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 6.412424849699399e-07\n",
      "pred: the european council has done the idea it its <unk> it it has given the commission the task paper to prepare a detailed proposal to december with the commission council bank for present forward the heads of state and government in approval in december <eos>\n",
      "tar:  the european council has endorsed the ideas in the initiative indeed it has given the commission the green light to prepare a detailed proposal in conjunction with the european investment bank to put to the heads of state or government for approval in december <eos>\n",
      "pred: we us therefore urge call the countries that have not ratified these conventions on national minorities to do that it now they is now now away <eos>\n",
      "tar:  let us therefore urgently <unk> those countries which have not ratified these conventions on national minorities to see to it that this is done straight away <eos>\n",
      "epoch:1  Step:16000  loss:1.6418762207  WER:0.3739424704\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this sector also creates many jobs jobs in the processing industry or in the <unk> <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:200  loss:1.4743540096  WER:0.3423609165\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 7.214028056112226e-07\n",
      "pred: religious religious schools extremism schools which offer a fertile property ground for the <unk> must be the <unk> and must content of the content must the content must <eos>\n",
      "tar:  the radical islamic religious schools which are an intellectual breeding ground for the taliban must undergo organisational restructuring and the reform of the content of their curricula <eos>\n",
      "pred: this is the we support for and is what we are advocating and is what we are doing and what fact says it the up to each legislation of each country to decide what is legal and not is not <eos>\n",
      "tar:  that is what we stand for that is what we are defending that is what we are implementing and in fact that leaves it up to the legislation of each country to establish what is legal and what is not legal <eos>\n",
      "epoch:1  Step:18000  loss:1.5206960440  WER:0.3634868421\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates many jobs jobs in the processing industry and in the shipyards industry industry the <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:225  loss:1.4687374258  WER:0.3420710136\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 8.01563126252505e-07\n",
      "pred: i would also like to point out that i promotion of all the enterprises in all by european to the farmers who europe the agricultural is very matter of great concern to me because rural structures are on the farms of of availability the availability willingness to engage them <eos>\n",
      "tar:  i would also like to point out that the promotion of all farming concerns above all those belonging to young farmers in all europe's regions is a matter of great concern to me as rural structures depend on the <unk> existence and on the latter's willingness to protect them <eos>\n",
      "pred: in a representative of the turkish general prosecutor office the the <unk> of the <unk> in ankara and <unk> and the authorities and all was the was <eos>\n",
      "tar:  with a representative of the turkish public prosecutor's office present the offices of the <unk> in ankara were searched by the authorities and everything in them seized <eos>\n",
      "epoch:1  Step:20000  loss:1.5422763824  WER:0.3545966229\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this sector also creates many jobs jobs on the processing industry or in the shipyards industry industry the the <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:250  loss:1.4679034615  WER:0.3429076374\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 8.817234468937877e-07\n",
      "pred: what i would like to know is what will happen with the draft charter which the convention is up <eos>\n",
      "tar:  what i would like to know is what will happen to the draft charter that the convention draws up? <eos>\n",
      "pred: the debates in have had in parliament on this subject from november november make the issue a clear <eos>\n",
      "tar:  the debates we have had in parliament on this subject since last november make this only too evident <eos>\n",
      "epoch:1  Step:22000  loss:1.3517097235  WER:0.3516068053\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this sector also creates many jobs jobs on the processing industry and in the <unk> industry industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:275  loss:1.4639973068  WER:0.3409455069\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 9.618837675350702e-07\n",
      "pred: mr president i shall respond briefly to the comments comments that have been made <eos>\n",
      "tar:  mr president i will respond briefly to the few comments that have been made <eos>\n",
      "pred: let me first the first place to address those members meps especially course particularly particular the british who who are asked calling with price increases <eos>\n",
      "tar:  allow me in the first place to address those fellow meps of mine in particular the british ones who have been concerned about price rises <eos>\n",
      "epoch:1  Step:24000  loss:1.5567671061  WER:0.3650254669\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates many jobs jobs in the processing industry or in the shipyards industry industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:300  loss:1.4612934685  WER:0.3421554797\n",
      "Train\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 1.0420440881763529e-06\n",
      "pred: mr president the commission and the council have asked for urgent procedure <eos>\n",
      "tar:  mr president the commission and the council have asked for urgent procedure <eos>\n",
      "pred: in hope to in the context of the debate on migration on migration and development we will be able to with parliament to outline the are core elements of the policy in the european union are <eos>\n",
      "tar:  i hope that in the framework of the debate on communications about migration and development we will be able together with parliament to discuss what the key elements of return policy in the european union are <eos>\n",
      "epoch:2  Step:26000  loss:1.3532965183  WER:0.3212765957\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: in sector also generates many jobs jobs in the processing industry or in the <unk> <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:325  loss:1.4516161060  WER:0.3408399322\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 1.1222044088176353e-06\n",
      "pred: mr president since may 2000 eight portuguese prisoners have been held prisoner in the territory of <unk> <eos>\n",
      "tar:  mr president since may 2000 eight portuguese citizens have been held prisoner in the territory of <unk> <eos>\n",
      "pred: the complexity not lead to be done in the report <eos>\n",
      "tar:  this will also have to be discussed in the commission for the complicated procedure must not result in the funds ceasing to be used at all <eos>\n",
      "epoch:2  Step:28000  loss:1.7277468443  WER:0.4058919804\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this sector also creates many jobs jobs on the processing industry or in the <unk> industry industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:350  loss:1.4530527782  WER:0.3411297334\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 1.202364729458918e-06\n",
      "pred: i think that is time to acknowledged that <eos>\n",
      "tar:  i think it is time we recognise this <eos>\n",
      "pred: i is my duty to speak on their behalf <eos>\n",
      "tar:  it is my duty to speak on their behalf <eos>\n",
      "epoch:2  Step:30000  loss:1.5387917757  WER:0.3727454910\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this sector also creates many jobs jobs in the processing industry or in the <unk> <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:375  loss:1.4464704132  WER:0.3385944531\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 1.2825250501002004e-06\n",
      "pred: this this proposal was been <unk> by the conservative majority <eos>\n",
      "tar:  today this proposal has been <unk> by a conservative majority <eos>\n",
      "pred: apart what worries me most more is that the is illustrates to be the s desire to <unk> it in beyond is goes the support of this country has enjoying from the <eos>\n",
      "tar:  furthermore what concerns me the most is that this affair appears to demonstrate <unk> s desire to <unk> itself <unk> it also demonstrates the support that the country is receiving within europe <eos>\n",
      "epoch:2  Step:32000  loss:1.5824952126  WER:0.3784246575\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this sector also creates many jobs jobs in the processing industry or in the <unk> industry industry the <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:400  loss:1.4500599146  WER:0.3404115962\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 1.362685370741483e-06\n",
      "pred: as other negotiations are not far as the commission is concerned <eos>\n",
      "tar:  any further negotiations are as far as the commission is concerned not in order <eos>\n",
      "pred: in particular we want to look the problems raised have treaty of amsterdam has and that fact that this financial financial implications of the treaty of amsterdam are only this understood this year and we will to take them of these <eos>\n",
      "tar:  in particular we want to consider the problems that the treaty of amsterdam presents and the fact that the full financial implications of the treaty of amsterdam are only being realised this year and we have to take account of that <eos>\n",
      "epoch:2  Step:34000  loss:1.5029816628  WER:0.3391812865\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates many jobs jobs in the processing industry or in the <unk> industry industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:425  loss:1.4443594456  WER:0.3404303857\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 1.4428456913827657e-06\n",
      "pred: great importance is also attached to major projects <eos>\n",
      "tar:  great importance is also attached to major projects <eos>\n",
      "pred: in is in this context that the membership to the hague hague convention is parental responsibility is currently present being considered <eos>\n",
      "tar:  it is in this context that community accession to the 1996 hague convention on parental responsibility is at present being considered <eos>\n",
      "epoch:2  Step:36000  loss:1.3619484901  WER:0.3013698630\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this sector also creates many jobs jobs on the processing industry and in the shipyards industry industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:450  loss:1.4448809147  WER:0.3402517513\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 1.5230060120240484e-06\n",
      "pred: the wto will apply with the wto agreement for three fundamental reasons: the first approach to do with the approach adopted favours monopolies as has with <eos>\n",
      "tar:  the same will happen with the wto agreement for three basic reasons the first has to do with the approach which favours monopolies as did gatt <eos>\n",
      "pred: the the acceding acceding countries states sit sitting on the table table and in parliament <eos>\n",
      "tar:  already the newly acceding member states are sitting around the council table and in parliament <eos>\n",
      "epoch:2  Step:38000  loss:1.3732736111  WER:0.3248259861\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates many jobs jobs in the processing industry or in the <unk> industry industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:475  loss:1.4388049936  WER:0.3377395661\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 1.6031663326653306e-06\n",
      "pred: in spite of these misgivings i mine i find accept understand why agreeing all the tobacco of tobacco to which which that and <unk> - to are more itself more dangerous than those benefit <eos>\n",
      "tar:  in spite of these misgivings of mine i cannot quite understand our allowing all other forms of tobacco product - cigarettes <unk> and pipes - which are in themselves more dangerous for the individual who uses them <eos>\n",
      "pred: in the of the series of maritime disasters that have occurred in european waters in november and december 2002 in the prestige disaster the continues to <unk> the another <unk> slicks than are <unk> the coastal of marine life and the and by the recent <unk> between the <unk> and <unk> and the subsequent and the subsequent <unk> between the <unk> of the <unk> and the <unk> the <unk> and the council recognise the the is a need need for a coordinated response response <eos>\n",
      "tar:  in light of the series of maritime disasters that have occurred in european waters in november and december 2002 namely the prestige disaster which continues to produce yet more oil slicks which are devastating the coastline and marine life of galicia compounded with the recent collision between the <unk> the <unk> and the <unk> and the second collision between the wreck of the <unk> and the <unk> the <unk> will the council recognise that there is a clear need for a coordinated eu response? <eos>\n",
      "epoch:2  Step:40000  loss:1.5651882887  WER:0.3675213675\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this sector also creates many jobs jobs in the processing industry and in the shipyards industry industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:500  loss:1.4422144794  WER:0.3396133370\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 1.6833266533066133e-06\n",
      "pred: the central aspect in is environmental environmental impact was been mentioned in the of biofuels <eos>\n",
      "tar:  one crucial point that of their environmental impact has been addressed in discussion of biofuels <eos>\n",
      "pred: as regards the creation and subsequent implementation of multilateral instruments the commission shares the view opinion that the eu union should play a fundamental role and assist third countries to fulfilling their obligations as we have already for example with the to the <eos>\n",
      "tar:  as for the creation and subsequent implementation of multilateral instruments the commission shares the report’s opinion that the european union should play a leading role and assist third countries in fulfilling their obligations as we have done for example in relation to counter-terrorism <eos>\n",
      "epoch:2  Step:42000  loss:1.1872040033  WER:0.2912280702\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: in sector also creates many jobs jobs in the processing industry or in the <unk> industry industry the <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:525  loss:1.4369699526  WER:0.3356213180\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 1.763486973947896e-06\n",
      "pred: moreover have also come proposing for the lines for some time in have to same to the proposal paper on by the <unk> group in 1997 <eos>\n",
      "tar:  we have actually been arguing along these lines for some time i refer the house to the white paper published by the uen group in 1997 assessing the value of a europe of the nations as against a <unk> europe <eos>\n",
      "pred: while in european 20% of the eu has 73% of direct aid our farm survey shows that 40% 40% of the have to 20% farmers of farmers with the highest levels income income <eos>\n",
      "tar:  while the top 20% in the eu receive 73% of direct aid our farm survey shows that approximately 40% of payments went to the 20% of farmers with the highest family farm income <eos>\n",
      "epoch:2  Step:44000  loss:1.3859292269  WER:0.3132328308\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates many jobs jobs in the processing industry and in the <unk> industry industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:550  loss:1.4344849586  WER:0.3385112455\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 1.8436472945891784e-06\n",
      "pred: madam president i would to refer paragraph 26 of the <unk> report on enlargement and enlargement <eos>\n",
      "tar:  madam president i want to highlight paragraph 26 of mr <unk> report on agriculture and enlargement <eos>\n",
      "pred: granting these the right to vote at administrative level amounts recognising them additional right to recognising right of also the its duties rights <eos>\n",
      "tar:  granting them the right to vote at local level means recognising an additional right but this recognition is not without its associated duties <eos>\n",
      "epoch:2  Step:46000  loss:1.6012953520  WER:0.3652482270\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this sector also creates many jobs jobs in the processing industry and in the yards industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:575  loss:1.4348890448  WER:0.3390989284\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 1.923807615230461e-06\n",
      "pred: this issue has been been mainly mainly because of national national differences between government level rather than directly issues directly attributed in parliament but the reached in parliament have been effective efficient and useful in terms of approach approach and are very welcome <eos>\n",
      "tar:  the issue has long been problematic mainly because of the national differences at governmental level rather than the issues directly originating from parliament but compromises produced in parliament have been both efficient and expedient in terms of their approach and are very welcome <eos>\n",
      "pred: i hope that this amendment will be be supported by a sufficient majority <eos>\n",
      "tar:  i hope that this amendment will also be supported by a sufficient majority <eos>\n",
      "epoch:2  Step:48000  loss:1.4785318375  WER:0.3327895595\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates many jobs jobs in the processing industry or in the shipyards industry industry the the the the <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:600  loss:1.4295394039  WER:0.3388611396\n",
      "Train\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 2.003967935871744e-06\n",
      "pred: to use alibi is not seem up for the production as as scientifically is been demonstrated proven to is less less harmful to the environment than the <unk> of crop cultivation which the have to order to grow cotton <eos>\n",
      "tar:  the environmental alibi does not stand up because cotton production - and this has been scientifically proven - is much less harmful to the environment than similar types of subsidised production which producers abandoned in order to grow cotton <eos>\n",
      "pred: the citizens complain that they are entitled the right to vote on day but yet they they they are not fully full rights to participation in the day-to-day activities of the institutions which which they are at european national at national regional regional at local and local level <eos>\n",
      "tar:  our citizens complain that they are given the right to vote every now and then but that they do not enjoy full rights of participation in the day-to-day activities of the institutions for which they vote: at european level at national level and at regional and local level <eos>\n",
      "epoch:3  Step:50000  loss:1.3009368181  WER:0.3242187500\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates many jobs jobs in the processing industry and in the <unk> industry industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:625  loss:1.4281772757  WER:0.3387287098\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 2.0841282565130264e-06\n",
      "pred: this proposal also also provide the centres which set up to that to enable victims to identify the compensation to in <eos>\n",
      "tar:  this proposal would also involve information centres being set up so as to enable victims to identify appropriate claim representatives <eos>\n",
      "pred: only then will we achieve a viable railway market with <unk> competition <eos>\n",
      "tar:  only then can we achieve a healthy railway market through genuine competition <eos>\n",
      "epoch:3  Step:52000  loss:1.4783684015  WER:0.3488372093\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: in sector also creates many jobs jobs on the processing industry or in the shipyards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:650  loss:1.4264654064  WER:0.3403465540\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 2.164288577154309e-06\n",
      "pred: the european must take take and the act to provide the to rapid and effective action intervention in must have the from a to <eos>\n",
      "tar:  the commission must therefore quickly rack its brains to find instruments for rapid and effective emergency aid we will expect proposals for solutions soon <eos>\n",
      "pred: the who health organisation with its current members members membership members have set themselves mark to the objective <eos>\n",
      "tar:  the world health organisation and its current complement of <unk> members have <unk> their colours to the <unk> and are committed to improving health throughout the world <eos>\n",
      "epoch:3  Step:54000  loss:1.6512761116  WER:0.3726591760\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates many jobs jobs on the processing industry or in the <unk> sector industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:675  loss:1.4263248491  WER:0.3361804811\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 2.2444488977955912e-06\n",
      "pred: this is a problem that the throughout over the world not not least for united states of america and also is also real problem for the eu too well <eos>\n",
      "tar:  that is a problem for democracies all over the world and not least the united states of america but it is a real problem for the eu as well <eos>\n",
      "pred: mr harbour would a supplementary question <eos>\n",
      "tar:  mr harbour has a supplementary question <eos>\n",
      "epoch:3  Step:56000  loss:1.3716020584  WER:0.3378378378\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates many jobs jobs in the processing industry and in the <unk> sector industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:700  loss:1.4226760769  WER:0.3361256212\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 2.324609218436874e-06\n",
      "pred: during will have to this subject during question time and i will have the opportunity to clarify some some details in my replies <eos>\n",
      "tar:  we will return to this topic during question time when i will have the opportunity to give you more detail in my answers <eos>\n",
      "pred: madam president faced would to even broader picture than mr fatuzzo i i me start a about say that after the analyses and the <unk> of auditors' and and the negative of has have about is a encouraging to be a subject on the that which little criticism and that is a positive for the citizens <eos>\n",
      "tar:  madam president i have an even bigger audience than mr fatuzzo so let me be positive and say that after the <unk> of the court of auditors debate and the criticism that we heard it is really refreshing to have a debate on something with so little criticism something which is so positive for eu citizens <eos>\n",
      "epoch:3  Step:58000  loss:1.3780536652  WER:0.3429602888\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates many jobs jobs on the processing industry and in the yards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:725  loss:1.4254817390  WER:0.3383077102\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 2.4047695390781566e-06\n",
      "pred: of course we do europe do not want to make conditions to our aid but we do feel with the many iranians who fate have been affected by the and <unk> to and fair elections to their country <eos>\n",
      "tar:  of course we in europe do not seek to attach conditions to our help but we do join with the many iranians whose lives have been saved in wanting and expecting free and fair elections for their country <eos>\n",
      "pred: although we will going to vote in favour of this resolution - which i consider as a good resolution resolution which for again resolution need again - i would have have worded a other differently <eos>\n",
      "tar:  although we are going to vote in favour of this resolution - which i regard as a good consensus resolution calling for the obvious once again - i would perhaps have worded certain points differently <eos>\n",
      "epoch:3  Step:60000  loss:1.4301630259  WER:0.3570219966\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: in sector also creates many jobs jobs on the processing industry and in the <unk> industry industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:750  loss:1.4243652678  WER:0.3355952049\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 2.484929859719439e-06\n",
      "pred: the the united nations has done a work but so no success in the inspections <eos>\n",
      "tar:  yes the united nations has done sterling work but with no success for weapons inspection to date <eos>\n",
      "pred: i would to remind the ladies and gentlemen that the a given point european will have to be a limit to european integration which that would like to remind out that as the of geographical proximity and historical historical links we have having involved with the such as morocco algeria or the on the mediterranean of the mediterranean <eos>\n",
      "tar:  i wish to remind you ladies and gentlemen that at a certain point there will have to be a limit to european integration and i should like to point out that in terms of geographical proximity and of historical ties we are closely connected to countries such as morocco algeria or others on the shores of the mediterranean <eos>\n",
      "epoch:3  Step:62000  loss:1.2518086433  WER:0.2937365011\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this sector also creates many jobs jobs on the processing industry and in the yards industry industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:775  loss:1.4280517435  WER:0.3390545451\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 2.5650901803607214e-06\n",
      "pred: the conclusions were the of the white paper on transport policy up to 2010 year 2010 <eos>\n",
      "tar:  our conclusions form part of the white paper on transport policy up until the year 2010 <eos>\n",
      "pred: i would like to point to all all in especially to commissioner that russia is currently focusing its attention to its communications communications and natural resources in the north <eos>\n",
      "tar:  i would like to say to you all and especially the commissioner that russia is now turning its attention to its traffic connections and natural resources in the north <eos>\n",
      "epoch:3  Step:64000  loss:1.2146177292  WER:0.3028455285\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: in sector also creates many jobs jobs in the processing industry and in the yards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:800  loss:1.4165428114  WER:0.3372144665\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 2.645250501002004e-06\n",
      "pred: i sitting is adjourned <eos>\n",
      "tar:  the sitting is resumed <eos>\n",
      "pred: with regard to the we have of the amendment adopted by the committee committee in the report prepared up by mr caudron but are of prepared to to examine the amendments tabled in the session <eos>\n",
      "tar:  with regard to ethics we did consider the amendment adopted by the iter committee in the report drawn up by mr caudron we are quite prepared however to consider the amendments tabled at plenary <eos>\n",
      "epoch:3  Step:66000  loss:1.3437441587  WER:0.3063583815\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates many jobs jobs in the processing industry and in the <unk> industry industry the <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:825  loss:1.4145676422  WER:0.3374444582\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 2.7254108216432868e-06\n",
      "pred: <eos>\n",
      "tar:  <eos>\n",
      "pred: the i clarify mr <unk> that general affairs council of for 18 june will essentially seek to adopt approval for the council of for the common strategy for the mediterranean which in in in principle be presented to the european european council and and <eos>\n",
      "tar:  secondly to enlighten mr <unk> the general affairs council planned for 18 june will basically aim to secure approval within the council framework for the common strategy for the mediterranean which will then in principle be submitted to the feira european council <eos>\n",
      "epoch:3  Step:68000  loss:1.2889888287  WER:0.3247011952\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates many jobs jobs in the processing industry and in the yards industry industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:850  loss:1.4166781664  WER:0.3397407171\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 2.805571142284569e-06\n",
      "pred: mr president i hope i will not use my the five five minutes <eos>\n",
      "tar:  mr president i hope i will not use up all these five minutes <eos>\n",
      "pred: i (pt) i of all i should like to congratulate the rapporteur on his excellent work on the commission s report on the implementation and effects of directive <unk> on the interoperability of the trans-european high-speed rail system <eos>\n",
      "tar:  - (fr) first of all i should like to congratulate the rapporteur on his excellent work on the commission' s report on the implementation and effects of directive <unk> concerning the interoperability of the trans-european high-speed rail system <eos>\n",
      "epoch:3  Step:70000  loss:1.3869624138  WER:0.3188118812\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates many jobs jobs in the processing industry or in the <unk> industry industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:875  loss:1.4210179853  WER:0.3349507029\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 2.885731462925852e-06\n",
      "pred: the current regime has brought stability in the sector allowing enabled the community to export a stable exporter fair exporter of the markets <eos>\n",
      "tar:  the present system has brought stability to the sector and enabled the community to be a modest but regular exporter to world markets <eos>\n",
      "pred: women are still the most offenders because salaries salaries remaining lower lower lower than those of men and women still finding considerable difficulty in being promoted to the senior and executive positions <eos>\n",
      "tar:  women are still the worst affected with average salaries still being considerably lower than those of men and women still experience considerable difficulty in being promoted to more senior and executive positions <eos>\n",
      "epoch:3  Step:72000  loss:1.2476561069  WER:0.3164835165\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates many jobs jobs in the processing industry and in the yards sector industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:900  loss:1.4166453171  WER:0.3368851444\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 2.9658917835671345e-06\n",
      "pred: i believe we balance has been struck between the market regulatory of and the market and the commission's right of veto to order to move the internal of the internal market for which we are all striving agreement <eos>\n",
      "tar:  i think a balance has been achieved between the national <unk> <unk> of the market and the commission's right of veto in order to stimulate the progress of the internal market of which we are all in favour even my group <eos>\n",
      "pred: mr president for year the again the agricultural budget is not subject to some certain amount of rigour which in believe is a political considerations rather than the actual situation of agriculture farming sector <eos>\n",
      "tar:  mr president this year once again the agriculture budget has been subject to a certain amount of rigour which i believe reflects certain political views rather than the true situation of the agriculture sector <eos>\n",
      "epoch:3  Step:74000  loss:1.2469515800  WER:0.3066132265\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this sector also creates many jobs jobs on the processing industry or in the yards industry industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:925  loss:1.4115960407  WER:0.3355040015\n",
      "Train\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 3.0460521042084174e-06\n",
      "pred: on regards safety aspects would have course have liked a more precise vote in favour of independent <unk> certification bodies as that in will still a monopoly situation in some countries and consequently there are consequently conflicts of interest when the comes to certification and <unk> <unk> <eos>\n",
      "tar:  as regards safety i would of course have welcomed a more clear-cut vote in favour of independent third-party certification bodies given that there is still a monopoly situation in some countries and that there are therefore conflicts of interest when it comes to certification or rather <unk> <eos>\n",
      "pred: thirdly third point is that it is essential essential to the peace clause and the special safeguard clause should be renewed in order to maintain the in agricultural markets and farmers' incomes <eos>\n",
      "tar:  the third point is that it is considered essential that the peace clause and the special safeguard clause should be renewed in order to maintain stability in agricultural markets and farmers' incomes <eos>\n",
      "epoch:4  Step:76000  loss:1.4004672766  WER:0.3384615385\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates many jobs jobs in the processing industry and in the <unk> sector industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:950  loss:1.4124917698  WER:0.3368701594\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 3.1262124248496994e-06\n",
      "pred: it is up council's s task to adhere the treaty what we what we are <eos>\n",
      "tar:  it is the council' s task to pursue the treaty that is what we do <eos>\n",
      "pred: <eos>\n",
      "tar:  it is not a matter of isolated cases <eos>\n",
      "epoch:4  Step:78000  loss:1.5480509996  WER:0.3614718615\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates many jobs jobs in the processing industry and in the yards industry industry into of the <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:975  loss:1.4083325386  WER:0.3328554242\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 3.2063727454909823e-06\n",
      "pred: i hope that this european parliament initiative will help to identify common elements of a policy that will enable the european union to play a more active role in this issue <eos>\n",
      "tar:  i hope that this european parliamentary initiative will help to identify common elements of a policy which could enable the european union to play a more active role on this issue <eos>\n",
      "pred: one of the most most affected affected by unemployment in the european union is women <eos>\n",
      "tar:  one of the groups most badly affected by unemployment in the european union is women <eos>\n",
      "epoch:4  Step:80000  loss:1.3746389151  WER:0.3299120235\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates many jobs jobs in the processing industry or in the <unk> <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:1000  loss:1.4157912493  WER:0.3334349937\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 3.2865330661322647e-06\n",
      "pred: i me just that that i am that fact that we have not been this report been succeeded in focusing our attention on this aspect related aspect and risk risk of being down our message and in to a ethical and general debate which would once be unnecessary and would be to put up a and repression <eos>\n",
      "tar:  let me say now that i regret the fact that we have not in this report completely succeeded in focussing our attention on this specifically health-related aspect at the risk of watering down our message and reverting to an ethical and generalised debate which would again be futile and would attempt to set up prevention and suppression in opposition to each other <eos>\n",
      "pred: we are waiting for this commission commission policy and and we we the <eos>\n",
      "tar:  we are waiting for the new chemicals policy commission - where is it? <eos>\n",
      "epoch:4  Step:82000  loss:1.5228645802  WER:0.3509272468\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates many jobs jobs on the processing industry or in the yards industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:1025  loss:1.4070765829  WER:0.3323378025\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 3.3666933867735476e-06\n",
      "pred: what does the council think of the green paper on <unk> <eos>\n",
      "tar:  what does the council think about the green paper on <unk> <eos>\n",
      "pred: we have overcome this problem the the in this court of justice this year <eos>\n",
      "tar:  we have overcome this by frontloading buildings for the court of justice this year <eos>\n",
      "epoch:4  Step:84000  loss:1.2224681377  WER:0.3140311804\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: in sector also creates many jobs jobs in the processing industry and in the yards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:1050  loss:1.4107665443  WER:0.3373107523\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 3.44685370741483e-06\n",
      "pred: because lifelong learning seems to be a huge task while living <unk> our <unk> our our daily every life <eos>\n",
      "tar:  because lifelong learning seems to be a mammoth task while <unk> as you <unk> stimulates our curiosity everyday anew <eos>\n",
      "pred: mr president encouraging competition in the postal services for promote benefit of consumers is one of the commission’s priorities accepted the commission <eos>\n",
      "tar:  mr president promoting competition in the postal market to the benefit of consumers is one of the declared priorities of the commission <eos>\n",
      "epoch:4  Step:86000  loss:1.3138110638  WER:0.3051181102\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates many jobs jobs in the processing industry and in the <unk> <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:1075  loss:1.4098898840  WER:0.3328403440\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 3.527014028056113e-06\n",
      "pred: thank you commissioner <eos>\n",
      "tar:  thank you commissioner <eos>\n",
      "pred: i is satisfaction that the fact that the conference was able to take place - all - a is itself is very very deal - and we must congratulate congratulate the portuguese presidency and commissioner nielson whose <unk> were to overcome the difficulties of and <unk> <unk> of many who who were that the summit would be a but it would going to be a empty vacuum in the union's relations of the european union <eos>\n",
      "tar:  there was satisfaction at the fact that the conference was able to take place at all - which in itself is a great thing - and we should therefore congratulate the portuguese presidency and commissioner nielson whose efforts helped to overcome the difficulties <unk> and lazy <unk> of many people who assumed that this summit would be suspended although it was going to fill an unjustifiable vacuum in the external relations of the european union <eos>\n",
      "epoch:4  Step:88000  loss:1.4912952185  WER:0.3589001447\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates many jobs jobs in the processing industry and in the yards industry industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:1100  loss:1.4092249155  WER:0.3356304551\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 3.6071743486973953e-06\n",
      "pred: commissioners you your many proposals you referred to the added value of medicines <eos>\n",
      "tar:  commissioners among your numerous proposals you referred to the added value of medicines <eos>\n",
      "pred: during the debate on the year's report i said that the ultimate responsibility for arms exports lies to national governments <eos>\n",
      "tar:  during the debate on last year's report i stated that the ultimate responsibility for arms exports belongs to national governments <eos>\n",
      "epoch:4  Step:90000  loss:1.1316410303  WER:0.3036253776\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates many jobs jobs on the processing industry and in the yards industry industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:1125  loss:1.4115511847  WER:0.3342104292\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 3.6873346693386774e-06\n",
      "pred: mr president would the commissioner agree with me that there are to be a growing and growing abuse of children that are one the competence of the european union? <eos>\n",
      "tar:  mr president does the commissioner agree with me that there seems to be a new and growing abuse of children which is within the competence of the european union? <eos>\n",
      "pred: this this of this mr president i am that <unk> the the way provisions of the out this programme – eur <unk> million for two years – especially since that there the financial provisions there is a limit – is that i quote <unk> <unk> annual are be authorised by the budgetary authority in the financial of the financial <unk> <eos>\n",
      "tar:  in light of this mr president i feel distinctly <unk> by the financial envelope for carrying out this programme – eur <unk> million over two years – especially given that in the financial provisions there is a restriction that states and i quote that <unk> appropriations shall be authorised by the budgetary authority within the limits of the financial <unk> <eos>\n",
      "epoch:4  Step:92000  loss:1.4039969444  WER:0.3406113537\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates many jobs jobs in the processing industry and in the <unk> <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:1150  loss:1.4046642685  WER:0.3366752126\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 3.7674949899799602e-06\n",
      "pred: first proposals proposals of information proposals on to the of fully full account of best practice in a member the member states <eos>\n",
      "tar:  firstly the development of the proposals relating to <unk> will take full account of best practice in several of the member states <eos>\n",
      "pred: the proposal of the proposal is to establish a common framework for the development transmission and assessment of comparable labour indices as is to same of article 1 <eos>\n",
      "tar:  the aim of the proposal is to establish a common framework for the production transmission and evaluation of comparable <unk> indices that is the objective of article 1 <eos>\n",
      "epoch:4  Step:94000  loss:1.1734958887  WER:0.2926829268\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates many jobs jobs in the processing industry and in the yards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:1175  loss:1.4035435295  WER:0.3316047011\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 3.847655310621243e-06\n",
      "pred: this is an european union that active in its civil and and far more so in the middle east peace process <eos>\n",
      "tar:  this is a european union increasingly active in its civilian role and far more present in the middle east peace process <eos>\n",
      "pred: i also congratulate mrs thank mrs ayuso for her report on behalf of the committee on the environment public health and consumer policy and i congratulate the who has spoken on this report and who debate <eos>\n",
      "tar:  i also congratulate and thank mrs ayuso for her opinion on behalf of the committee on the environment public health and consumer policy and i thank everybody who has spoken on this report and this debate <eos>\n",
      "epoch:4  Step:96000  loss:1.0772814751  WER:0.2718052738\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates many jobs jobs on the processing industry and in the yards industry industry and into the <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:1200  loss:1.4060338116  WER:0.3332352243\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 3.9278156312625255e-06\n",
      "pred: i recognise that the is a matter of concern that the on the transport of live are being being applied implemented in some regions <eos>\n",
      "tar:  i agree that it is a matter for concern that rules on the transport of animals are not being sufficiently enforced in certain areas <eos>\n",
      "pred: as i know - i i know him well i i been in him this movement will not not be about an halt and will a as it is on and will be done <unk> with <unk> and the other presidents presidents of not that it <eos>\n",
      "tar:  if i know him and i know him well and have confidence in him that movement will certainly not come to a halt and as long as it keeps going it will be skilfully conducted mr <unk> and the other prime ministers will see to that <eos>\n",
      "epoch:4  Step:98000  loss:1.7003203630  WER:0.3712121212\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates many jobs jobs in the processing industry and in the <unk> sector industry the the into <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:1225  loss:1.4013691807  WER:0.3352568483\n",
      "Train\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 4.007975951903808e-06\n",
      "pred: in this regard the commission is proposing to further the further the actions actions of local action groups to the the countries in <eos>\n",
      "tar:  in this connection the commission is proposing to limit yet further the cooperation actions of local action groups to only candidate countries <eos>\n",
      "pred: for is for this reason that we and above particular the commission must show them that we take data protection seriously and that we is not just the merely symbolic sense that we are putting it in stage <eos>\n",
      "tar:  it is for that reason that we and in particular the commission must show them that we take data protection seriously and that it is not in a merely symbolic sense that we are putting it centre stage <eos>\n",
      "epoch:5  Step:100000  loss:1.1333479881  WER:0.2766884532\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates many jobs jobs in the processing industry or in the yards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:5  Step:1250  loss:1.4072051954  WER:0.3361806722\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 78\u001b[39m\n\u001b[32m     75\u001b[39m loss = loss_fn(logits.transpose( \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m ), targ )\n\u001b[32m     77\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m optimizer.step()\n\u001b[32m     80\u001b[39m scheduler.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "epoch_num = 100\n",
    "\n",
    "# WarmupとCosine Decayを行うスケジューラを利用\n",
    "num_global_steps = len( train_loader ) * epoch_num\n",
    "print( \"num_global_steps:\", num_global_steps )\n",
    "num_warmup_steps = num_global_steps * 0.1\n",
    "print( \"num_warmup_steps:\", num_warmup_steps )\n",
    "scheduler = get_linear_schedule_with_warmup( optimizer, num_warmup_steps, num_global_steps ) \n",
    "\n",
    "tr_print_coef = 2000\n",
    "val_print_coef = 100\n",
    "\n",
    "#PATH = \"./model_NTT_auto_curr3.pt\"\n",
    "#\n",
    "#if device != torch.device(\"cpu\"):\n",
    "#    checkpoint = torch.load(PATH)\n",
    "#else:\n",
    "#    checkpoint = torch.load(PATH, map_location=torch.device('cpu'))\n",
    "#\n",
    "#model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device( \"cpu\")\n",
    "\n",
    "## optimizerのstateを現在のdeviceに移す。これをしないと、保存前後でdeviceの不整合が起こる可能性がある。\n",
    "#for state in optimizer.state.values():\n",
    "#    for k, v in state.items():\n",
    "#        if isinstance(v, torch.Tensor):\n",
    "#            state[k] = v.to(device)\n",
    "##epoch = checkpoint['epoch']\n",
    "##loss = checkpoint['loss']\n",
    "\n",
    "history = {\"train_loss\": [], \"val_loss\": [], \"train_wer\": [], \"val_wer\": [] }\n",
    "\n",
    "n = 0\n",
    "train_loss = 0\n",
    "val_loss = 0\n",
    "\n",
    "f_train = open( \"train_it3.log\", mode=\"w\", encoding = \"UTF-8\" )\n",
    "f_val = open( \"val_it3.log\", mode=\"w\", encoding = \"UTF-8\" )\n",
    "\n",
    "tra_global_step = 0\n",
    "val_global_step = 0\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    \n",
    "    model.train()\n",
    "    print( \"Train\")\n",
    "    train_loss = 0\n",
    "    train_wer = 0\n",
    "    n = 0\n",
    "    for i, (src, src_len, tar, tar_len) in enumerate(train_loader):\n",
    "        #display = True\n",
    "\n",
    "        tra_global_step += 1\n",
    "        if tra_global_step % 2000 == 0:\n",
    "            display = True\n",
    "            train_loss = 0\n",
    "            train_wer = 0\n",
    "            n = 0\n",
    "        else:\n",
    "            display = False\n",
    "        if display:\n",
    "            print(\"\\n-----------------Train Mode-----------------\\n\")\n",
    "        if display:\n",
    "            print(\"lr:\", optimizer.param_groups[0][\"lr\"] )\n",
    "\n",
    "        src = src[:,:max(src_len)].to(device)\n",
    "        tar = tar[:,:max(tar_len)]\n",
    "        dec_in = tar[:,:-1].to(device)\n",
    "        targ = tar[:,1:].to(device)\n",
    "\n",
    "        logits = model( src, dec_in )\n",
    "        loss = loss_fn(logits.transpose( 1, 2 ), targ )\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        pre_label_id = torch.argmax( logits, dim = 2 )\n",
    "        predict = []\n",
    "        for pre in pre_label_id:\n",
    "            hypo = []\n",
    "            for m in pre:\n",
    "                hypo.append(token_list_en[m.item()])\n",
    "                if token_list_en[m.item()] == '<eos>':\n",
    "                    break            \n",
    "            predict.append( hypo )\n",
    "        target = []\n",
    "        for pre in targ:\n",
    "            reference = []\n",
    "            for m in pre:\n",
    "                reference.append(token_list_en[m.item()])\n",
    "                if token_list_en[m.item()] == '<eos>':\n",
    "                    break            \n",
    "            target.append( reference )\n",
    "        total_error = 0\n",
    "        total_token_length = 0\n",
    "        for i3, (pred, tar) in enumerate( zip(predict, target) ):\n",
    "            (error, substitute, \n",
    "                delete, insert, ref_length) = \\\n",
    "                levenshtein.calculate_error(pred,tar)\n",
    "            # 誤り文字数を累積する\n",
    "            total_error += error\n",
    "            # 文字の総数を累積する\n",
    "            total_token_length += ref_length  \n",
    "\n",
    "            if i3 < 2 and display:\n",
    "                print( \"pred:\", ' '.join(pred) )\n",
    "                print( \"tar: \", ' '.join(tar) )\n",
    "            \n",
    "        train_loss += loss.item()\n",
    "        train_wer += total_error / total_token_length\n",
    "        n += 1\n",
    "        history[\"train_loss\"].append( train_loss / n )\n",
    "        history[\"train_wer\"].append( train_wer / n )\n",
    "        #if i % tr_print_coef == tr_print_coef - 1:\n",
    "        if display:\n",
    "            print(f\"epoch:{epoch+1}  Step:{tra_global_step}  loss:{train_loss/n:.10f}  WER:{ train_wer / n:.10f}\")\n",
    "        if tra_global_step % 2000 == 0:\n",
    "            PATH = './model_NTT_auto_curr_it3.pt'\n",
    "            torch.save({'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss,},\n",
    "               PATH)\n",
    "            #with open(\"history_NTT_auto_curr_it3.pkl\", \"wb\") as f:\n",
    "            #    pickle.dump( history, f )\n",
    "\n",
    "        f_train.write( \"Epoch: \" + str(epoch) + \", Step: \" + str(tra_global_step) + \", Loss: \" + str(train_loss/n) + \", WER: \" + str(train_wer / n ) + \"\\n\" )\n",
    "        f_train.flush()\n",
    "        \n",
    "        display_taihi = display\n",
    "        #if tra_global_step % 100 == 0:\n",
    "        if display:\n",
    "            val_loss = 0\n",
    "            val_wer = 0\n",
    "            val_n = 0\n",
    "            print(\"\\n-----------------Validation Mode-----------------\\n\")\n",
    "\n",
    "            for i_val, (val_src, val_src_len, val_tar, val_tar_len) in enumerate(val_loader):\n",
    "                val_global_step += 1\n",
    "                if i_val == 0:\n",
    "                    display = True\n",
    "                else:\n",
    "                    display = False\n",
    "                val_src = val_src[:,:max(val_src_len)].to(device)\n",
    "                val_tar = val_tar[:,:max(val_tar_len)]\n",
    "                val_dec_in = val_tar[:,:-1].to(device)\n",
    "                val_targ = val_tar[:,1:].to(device)\n",
    "\n",
    "                logits = model( val_src, val_dec_in )\n",
    "                loss = loss_fn(logits.transpose( 1, 2 ), val_targ )\n",
    "\n",
    "                pre_label_id = torch.argmax( logits, dim = 2 )\n",
    "                predict = []\n",
    "                for pre in pre_label_id:\n",
    "                    hypo = []\n",
    "                    for m in pre:\n",
    "                        hypo.append(token_list_en[m.item()])\n",
    "                        if token_list_en[m.item()] == '<eos>':\n",
    "                            break            \n",
    "                    predict.append( hypo )\n",
    "                target = []\n",
    "                for pre in val_targ:\n",
    "                    reference = []\n",
    "                    for m in pre:\n",
    "                        reference.append(token_list_en[m.item()])\n",
    "                        if token_list_en[m.item()] == '<eos>':\n",
    "                            break            \n",
    "                    target.append( reference )\n",
    "                val_total_error = 0\n",
    "                # 文字の総数を累積する\n",
    "                val_total_token_length = 0\n",
    "                for i3, (pred, tar) in enumerate( zip(predict, target) ):\n",
    "                    (error, substitute, \n",
    "                        delete, insert, ref_length) = \\\n",
    "                        levenshtein.calculate_error(pred,tar)\n",
    "                    # 誤り文字数を累積する\n",
    "                    val_total_error += error\n",
    "                    # 文字の総数を累積する\n",
    "                    val_total_token_length += ref_length  \n",
    "\n",
    "                    if i3 < 2 and i_val == 0:\n",
    "                        print( \"pred:\", ' '.join(pred) )\n",
    "                        print( \"tar: \", ' '.join(tar) )\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_wer += val_total_error / val_total_token_length\n",
    "                val_n += 1\n",
    "                #print( \"val_wer:\", val_wer )\n",
    "                #print( \"val_n:\", val_n )\n",
    "                history[\"val_loss\"].append( val_loss / val_n )\n",
    "                history[\"val_wer\"].append( val_wer / val_n )\n",
    "            print(f\"epoch:{epoch+1}  Step:{val_global_step}  loss:{val_loss/val_n:.10f}  WER:{ val_wer / val_n:.10f}\")\n",
    "            f_val.write( \"Epoch: \" + str(epoch) + \", Step: \" + str(val_global_step)  + \", Loss: \" + str(val_loss/val_n) + \", WER: \" + str(val_wer / val_n ) + \"\\n\" ) \n",
    "            f_val.flush()\n",
    "        if tra_global_step % 2000 == 0:\n",
    "            with open(\"history_NTT_auto_curr_it3.pkl\", \"wb\") as f:\n",
    "                pickle.dump( history, f )\n",
    "        display = display_taihi\n",
    "\n",
    "f_train.close()\n",
    "f_val.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "65d4d9fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
