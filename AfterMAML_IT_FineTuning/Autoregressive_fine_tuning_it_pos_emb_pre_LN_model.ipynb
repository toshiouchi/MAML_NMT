{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8bf21a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install janome\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "#from torchtext.vocab import vocab\n",
    "#import torchtext.transforms as T\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#from torchvision import transforms\n",
    "import numpy as np\n",
    "import math\n",
    "import janome\n",
    "from janome.tokenizer import Tokenizer\n",
    "#import spacy\n",
    "from collections import Counter\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import time\n",
    "#from torchtext.vocab import build_vocab_from_iterator\n",
    "import levenshtein\n",
    "import json\n",
    "import pickle\n",
    "from timm.scheduler import CosineLRScheduler\n",
    "import nltk\n",
    "from nltk import bleu_score\n",
    "from torch.nn.init import constant_, xavier_uniform_\n",
    "from torch.nn.parameter import Parameter\n",
    "from transformers import  get_linear_schedule_with_warmup\n",
    "from collections import OrderedDict\n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "346dde95-1503-4243-9963-31dcdce0e2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device( \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d745f5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49528 20556\n"
     ]
    }
   ],
   "source": [
    "with open( \"corpus/id_to_word_s.pkl\", \"rb\" ) as f:\n",
    "    token_list = pickle.load(f)\n",
    "with open( \"corpus/id_to_word_t.pkl\", \"rb\" ) as f:\n",
    "    token_list_en = pickle.load(f)\n",
    "with open( \"corpus/word_to_id_s.pkl\", \"rb\" ) as f:\n",
    "    idx_list = pickle.load(f)\n",
    "with open( \"corpus/word_to_id_t.pkl\", \"rb\" ) as f:\n",
    "    idx_list_en = pickle.load(f)\n",
    "\n",
    "pad_idx_s = idx_list['<pad>']\n",
    "pad_idx_t = idx_list_en['<pad>']\n",
    "\n",
    "enc_vocab_size, dec_vocab_size = len(token_list), len(token_list_en)\n",
    "print(enc_vocab_size, dec_vocab_size)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86f1161a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<sos>', '<eos>', '<unk>', '<blank>', '<mask>', 'der']\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([ 0, 1,2,3,4,5,6 ])\n",
    "\n",
    "#print( token_list[2] )\n",
    "\n",
    "#n = 0\n",
    "\n",
    "#ii = [ i for i in a ]\n",
    "\n",
    "#print( ii )\n",
    "\n",
    "b = [ token_list[i.item()] for i in a ]\n",
    "\n",
    "print( b )\n",
    "\n",
    "\n",
    "d = idx_list['<pad>']\n",
    "\n",
    "print( d )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "649837a5-2937-4e81-880b-a57f2532b967",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    ''' ミニバッチデータを作成するクラス\n",
    "        torch.utils.data.Datasetクラスを継承し，\n",
    "        以下の関数を定義する\n",
    "        __len__: 総サンプル数を出力する関数\n",
    "        __getitem__: 1サンプルのデータを出力する関数\n",
    "    feat_scp:  特徴量リストファイル\n",
    "    label_scp: ラベルファイル\n",
    "    feat_mean: 特徴量の平均値ベクトル\n",
    "    feat_std:  特徴量の次元毎の標準偏差を並べたベクトル \n",
    "    pad_index: バッチ化の際にフレーム数を合わせる\n",
    "               ためにpaddingする整数値\n",
    "    splice:    前後(splice)フレームを特徴量を結合する\n",
    "               splice=1とすると，前後1フレーム分結合\n",
    "               するので次元数は3倍になる．\n",
    "               splice=0の場合は何もしない\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 et,\n",
    "                 pad_index_s,\n",
    "                 pad_index_t\n",
    "                 ):\n",
    "\n",
    "        # 読み込みながら情報を取得する\n",
    "        self.pad_index_s = pad_index_s\n",
    "        self.pad_index_t = pad_index_t\n",
    "        print( \"pad_index_s:\", self.pad_index_s )\n",
    "        print( \"pad_index_t:\", self.pad_index_t )\n",
    "        self.en_list = []\n",
    "        self.en_lens = []\n",
    "        self.et_list = []\n",
    "        self.et_lens = []\n",
    "        self.num_data = 0\n",
    "        for line in et:\n",
    "            # 各行をスペースで区切り，\n",
    "            # リスト型の変数にする\n",
    "            self.en_list.append( line['target'] )\n",
    "            self.en_lens.append( len( line['target']) )\n",
    "            #self.en_att_list.append( line['target']['attention_mask'] )\n",
    "\n",
    "            self.et_list.append( line['source'] )\n",
    "            self.et_lens.append( len( line['source']) )\n",
    "            #self.et_att_list.append( line['source']['attention_mask'] )\n",
    "            self.num_data += 1\n",
    "\n",
    "        #self.en_list = np.int64( np.array( self.en_list ) )\n",
    "        #self.et_list = np.int64( np.array( self.et_list ) )\n",
    "        self.en_lens = np.int64( np.array( self.en_lens ) )\n",
    "        self.et_lens = np.int64( np.array( self.et_lens ) )\n",
    "\n",
    "        # フレーム数の最大値を得る\n",
    "        self.max_en_len = np.max(self.en_lens)\n",
    "        # ラベル長の最大値を得る\n",
    "        self.max_et_len = np.max(self.et_lens)\n",
    "\n",
    "        for n in range(self.num_data):\n",
    "            if n % 10000 == 0:\n",
    "                print( \"n:\", n )\n",
    "            # 埋めるフレームの数\n",
    "            # = 最大フレーム数 - 自分のフレーム数\n",
    "            pad_len = self.max_en_len - self.en_lens[n]\n",
    "            # pad_indexの値で埋める\n",
    "            tmp = self.en_list[n]\n",
    "            self.en_list[n] = np.pad( tmp, (0, pad_len), mode='constant', constant_values=(self.pad_index_t, self.pad_index_t ))\n",
    "            pad_len = self.max_et_len - self.et_lens[n]\n",
    "            # pad_indexの値で埋める\n",
    "            self.et_list[n] = np.pad(self.et_list[n],[0, pad_len],mode='constant', constant_values=self.pad_index_s)\n",
    "\n",
    "        self.en_list = np.int64( np.array( self.en_list ) )\n",
    "        self.et_list = np.int64( np.array( self.et_list ) )\n",
    "\n",
    "    def __len__(self):\n",
    "        ''' 学習データの総サンプル数を返す関数\n",
    "        本実装では発話単位でバッチを作成するため，\n",
    "        総サンプル数=発話数である．\n",
    "        '''\n",
    "        return self.num_data\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ''' サンプルデータを返す関数\n",
    "        本実装では発話単位でバッチを作成するため，\n",
    "        idx=発話番号である．\n",
    "        '''\n",
    "\n",
    "        # ラベル\n",
    "        et = self.et_list[idx]\n",
    "        et_len = self.et_lens[idx]\n",
    "\n",
    "        # 発話ID\n",
    "        en = self.en_list[idx]\n",
    "        en_len = self.en_lens[idx]\n",
    "\n",
    "        # 特徴量，ラベル，フレーム数，\n",
    "        # ラベル長，発話IDを返す\n",
    "        #return (jps, jp_lens, ens,  en_lens)\n",
    "        return (et, et_len, en, en_len )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa12965b-1beb-4be9-967f-7c533486462d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it_train: 499000\n",
      "it_val: 500\n",
      "pad_index_s: 0\n",
      "pad_index_t: 0\n",
      "n: 0\n",
      "n: 10000\n",
      "n: 20000\n",
      "n: 30000\n",
      "n: 40000\n",
      "n: 50000\n",
      "n: 60000\n",
      "n: 70000\n",
      "n: 80000\n",
      "n: 90000\n",
      "n: 100000\n",
      "n: 110000\n",
      "n: 120000\n",
      "n: 130000\n",
      "n: 140000\n",
      "n: 150000\n",
      "n: 160000\n",
      "n: 170000\n",
      "n: 180000\n",
      "n: 190000\n",
      "n: 200000\n",
      "n: 210000\n",
      "n: 220000\n",
      "n: 230000\n",
      "n: 240000\n",
      "n: 250000\n",
      "n: 260000\n",
      "n: 270000\n",
      "n: 280000\n",
      "n: 290000\n",
      "n: 300000\n",
      "n: 310000\n",
      "n: 320000\n",
      "n: 330000\n",
      "n: 340000\n",
      "n: 350000\n",
      "n: 360000\n",
      "n: 370000\n",
      "n: 380000\n",
      "n: 390000\n",
      "n: 400000\n",
      "n: 410000\n",
      "n: 420000\n",
      "n: 430000\n",
      "n: 440000\n",
      "n: 450000\n",
      "n: 460000\n",
      "n: 470000\n",
      "n: 480000\n",
      "n: 490000\n",
      "pad_index_s: 0\n",
      "pad_index_t: 0\n",
      "n: 0\n"
     ]
    }
   ],
   "source": [
    "with open(\"data_train_it.pkl\", mode=\"rb\") as f:\n",
    "    it_train = pickle.load(f)\n",
    "with open(\"data_val_it.pkl\", mode=\"rb\") as f:\n",
    "    it_val = pickle.load(f)\n",
    "\n",
    "#it_train = it_train[:10000]\n",
    "\n",
    "print( \"it_train:\", len( it_train ) )\n",
    "print( \"it_val:\", len( it_val ) )\n",
    "\n",
    "train_dataset = SequenceDataset( it_train, pad_idx_s, pad_idx_t )\n",
    "val_dataset = SequenceDataset( it_val, pad_idx_s, pad_idx_t  )\n",
    "    \n",
    "batch_size = 20\n",
    "#num_workers = 4 if torch.cuda.is_available() else 0\n",
    "num_workers = 0 if device == torch.device( 'cpu' ) else 4\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers)\n",
    "# 開発データのDataLoaderを呼び出す\n",
    "# 開発データはデータはシャッフルしない\n",
    "val_loader = DataLoader(val_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers)    \n",
    "\n",
    "#Iter = iter(train_loader)\n",
    "#xdata, xatt, ydata, yatt = next(Iter) #教師データ、ラベルデータ\n",
    "#print(xdata, xatt, ydata, yatt)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b825cab-27ba-4da2-8050-2631036aa9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        #self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        #return self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66322069-a9f2-458e-af00-45953e75b074",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    '''\n",
    "    位置埋め込み （Positional embedding）\n",
    "    dim_embedding: 埋込み次元\n",
    "    max_len      : 入力の最大系列長\n",
    "    '''\n",
    "    def __init__(self, dim_embedding: int, max_len: int=5000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pos_emb = nn.Embedding(max_len, dim_embedding)\n",
    "\n",
    "    '''\n",
    "    位置エンコーディングの順伝播\n",
    "    x: 位置エンコーディングを埋め込む対象のテンソル,\n",
    "       [バッチサイズ, 系列長, 埋め込み次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        seq = x.shape[1]\n",
    "        positions = torch.arange(start=0, end=seq, step=1, device=x.device).to(torch.long)\n",
    "        positions = self.pos_emb(positions)[:seq,:]\n",
    "        \n",
    "        return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cea6ff9-e8c6-436a-941b-34a4e2e4fced",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fPositionalEmbedding(nn.Module):\n",
    "    '''\n",
    "    位置埋め込み （Positional embedding）\n",
    "    dim_embedding: 埋込み次元\n",
    "    max_len      : 入力の最大系列長\n",
    "    '''\n",
    "    def __init__(self, dim_embedding: int, name, max_len: int=5000):\n",
    "        super().__init__()\n",
    "\n",
    "        #self.pos_emb = nn.Embedding(max_len, dim_embedding)\n",
    "        self.name = name\n",
    "        \n",
    "    '''\n",
    "    位置エンコーディングの順伝播\n",
    "    x: 位置エンコーディングを埋め込む対象のテンソル,\n",
    "       [バッチサイズ, 系列長, 埋め込み次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor, weights):\n",
    "        seq = x.shape[1]\n",
    "        positions = torch.arange(start=0, end=seq, step=1, device=x.device).to(torch.long)\n",
    "        #positions = self.pos_emb(positions)[:seq,:]\n",
    "        positions = F.embedding( positions, weights[self.name] )[:seq,:]\n",
    "        \n",
    "        return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e05abc0d-48d7-4e58-8bc4-0a5310e59330",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "    '''\n",
    "    自己アテンション\n",
    "    dim_hidden: 入力特徴量の次元\n",
    "    num_heads : マルチヘッドアテンションのヘッド数\n",
    "    qkv_bias  : クエリなどを生成する全結合層のバイアスの有無\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 qkv_bias: bool=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # 特徴量を各ヘッドのために分割するので、\n",
    "        # 特徴量次元をヘッド数で割り切れるか検証\n",
    "        assert dim_hidden % num_heads == 0\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # ヘッド毎の特徴量次元\n",
    "        dim_head = dim_hidden // num_heads\n",
    "\n",
    "        # ソフトマックスのスケール値\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        # ヘッド毎にクエリ、キーおよびバリューを生成するための全結合層\n",
    "        self.proj_in_q = nn.Linear(\n",
    "            dim_hidden, dim_hidden, bias=qkv_bias)\n",
    "        self.proj_in_k = nn.Linear(\n",
    "            dim_hidden, dim_hidden, bias=qkv_bias)\n",
    "        self.proj_in_v = nn.Linear(\n",
    "            dim_hidden, dim_hidden, bias=qkv_bias)\n",
    "\n",
    "        #self.in_proj_weight = nn.Parameter( torch.randn( dim_hidden * 3, dim_hidden ) )\n",
    "        #self.in_proj_bias = nn.Parameter( torch.randn( dim_hidden * 3 ) )\n",
    "        #self.in_proj_weight = Parameter( torch.empty( dim_hidden * 3, dim_hidden ) )\n",
    "        #self.in_proj_bias = Parameter( torch.empty( dim_hidden * 3 ) )\n",
    "\n",
    "        # 各ヘッドから得られた特徴量を一つにまとめる全結合層\n",
    "        self.proj_out = nn.Linear(dim_hidden, dim_hidden)\n",
    "        \n",
    "        #self._reset_parameters()\n",
    "        \n",
    "    #def _reset_parameters(self):\n",
    "    #    xavier_uniform_( self.in_proj_weight )\n",
    "    #    constant_( self.in_proj_bias, 0.0 )\n",
    "\n",
    "    def split_head(self, x):\n",
    "        x = torch.tensor_split(x, self.num_heads, dim = 2)\n",
    "        x = torch.stack(x, dim = 1)\n",
    "        return x\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, mask: torch.Tensor):\n",
    "\n",
    "        #bs_q, ns_q, ds_q = q.size()\n",
    "        #bs_k, ns_k, ds_k = k.size()\n",
    "        \n",
    "        ##  k = v assumpotion\n",
    "        #if q is k:\n",
    "        #    qkv = q @ self.in_proj_weight.transpose(-2,-1) + self.in_proj_bias\n",
    "        #    qkv = qkv.view( bs_q, ns_q, 3, ds_q )\n",
    "        #    q, k, v = torch.unbind( qkv, dim = 2 )\n",
    "        #else:\n",
    "        #    W_q, W_kv = self.in_proj_weight.split([ds_q, ds_q * 2])\n",
    "        #    b_q, b_kv = self.in_proj_bias.split([ds_q, ds_q * 2])\n",
    "        #    q = q @ W_q.transpose(-2,-1) + b_q\n",
    "        #    kv =  k @ W_kv.transpose(-2,-1) + b_kv\n",
    "        #    kv = kv.view( bs_k, ns_k, 2, ds_k )\n",
    "        #    k, v = torch.unbind( kv, dim = 2 )\n",
    "        \n",
    "        q = self.proj_in_q(q)\n",
    "        k = self.proj_in_k(k)\n",
    "        v = self.proj_in_v(v)\n",
    "        \n",
    "        q = self.split_head(q)\n",
    "        k = self.split_head(k)\n",
    "        v = self.split_head(v)\n",
    "\n",
    "        # クエリとキーの行列積とアテンションの計算(今回マスクは不使用)\n",
    "        # attnは[バッチサイズ, ヘッド数, 特徴量数, 特徴量数]\n",
    "        attn = q.matmul(k.transpose(-2, -1))\n",
    "        #print( \"attn size:\", attn.size() )\n",
    "        #print( \"mask size:\", mask.size() )\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill_(mask, -torch.finfo(torch.float).max)\n",
    "        attn = (attn * self.scale).softmax(dim=-1)\n",
    "\n",
    "        # アテンションとバリューの行列積によりバリューを収集\n",
    "        # xは[バッチサイズ, ヘッド数, 特徴量数, ヘッドの特徴量次元]\n",
    "        x = attn.matmul(v)\n",
    "\n",
    "        # permute関数により\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数, ヘッドの特徴量次元]\n",
    "        # flatten関数により全てのヘッドから得られる特徴量を連結して、\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数 * ヘッドの特徴量次元]\n",
    "        x = x.permute(0, 2, 1, 3).flatten(2)\n",
    "        x = self.proj_out(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a51e5fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class feMHA(nn.Module):\n",
    "    '''\n",
    "    自己アテンション\n",
    "    dim_hidden: 入力特徴量の次元\n",
    "    num_heads : マルチヘッドアテンションのヘッド数\n",
    "    qkv_bias  : クエリなどを生成する全結合層のバイアスの有無\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 qkv_bias: bool=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # 特徴量を各ヘッドのために分割するので、\n",
    "        # 特徴量次元をヘッド数で割り切れるか検証\n",
    "        assert dim_hidden % num_heads == 0\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # ヘッド毎の特徴量次元\n",
    "        dim_head = dim_hidden // num_heads\n",
    "\n",
    "        # ソフトマックスのスケール値\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        # ヘッド毎にクエリ、キーおよびバリューを生成するための全結合層\n",
    "        #self.proj_in = nn.Linear(\n",
    "        #    dim_hidden, dim_hidden * 3, bias=qkv_bias)\n",
    "\n",
    "        # 各ヘッドから得られた特徴量を一つにまとめる全結合層\n",
    "        #self.proj_out = nn.Linear(dim_hidden, dim_hidden)\n",
    "\n",
    "    def split_head(self, x):\n",
    "        x = torch.tensor_split(x, self.num_heads, dim = 2)\n",
    "        x = torch.stack(x, dim = 1)\n",
    "        return x\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor,  v: torch.Tensor, mask: torch.Tensor, i, weights ):\n",
    "        bs, ns = q.shape[:2]\n",
    "\n",
    "        #qkv = self.proj_in(x)\n",
    "        q = F.linear(q, weights['encoder.encoder_layers.' + str(i) + '.attention.proj_in_q.weight'], weights['encoder.encoder_layers.' + str(i) + '.attention.proj_in_q.bias'])\n",
    "        k = F.linear(k, weights['encoder.encoder_layers.' + str(i) + '.attention.proj_in_k.weight'], weights['encoder.encoder_layers.' + str(i) + '.attention.proj_in_k.bias'])\n",
    "        v = F.linear(v, weights['encoder.encoder_layers.' + str(i) + '.attention.proj_in_v.weight'], weights['encoder.encoder_layers.' + str(i) + '.attention.proj_in_v.bias'])\n",
    "\n",
    "        q = self.split_head( q )\n",
    "        k = self.split_head( k )\n",
    "        v = self.split_head( v )\n",
    "\n",
    "        # クエリとキーの行列積とアテンションの計算(今回マスクは不使用)\n",
    "        # attnは[バッチサイズ, ヘッド数, 特徴量数, 特徴量数]\n",
    "        attn = q.matmul(k.transpose(-2, -1))\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill_(mask, -torch.finfo(torch.float).max)\n",
    "        attn = (attn * self.scale).softmax(dim=-1)\n",
    "\n",
    "        # アテンションとバリューの行列積によりバリューを収集\n",
    "        # xは[バッチサイズ, ヘッド数, 特徴量数, ヘッドの特徴量次元]\n",
    "        x = attn.matmul(v)\n",
    "\n",
    "        # permute関数により\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数, ヘッドの特徴量次元]\n",
    "        # flatten関数により全てのヘッドから得られる特徴量を連結して、\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数 * ヘッドの特徴量次元]\n",
    "        x = x.permute(0, 2, 1, 3).flatten(2)\n",
    "        #x = self.proj_out(x)\n",
    "        x = F.linear(x, weights['encoder.encoder_layers.' + str(i) + '.attention.proj_out.weight'], weights['encoder.encoder_layers.' + str(i) + '.attention.proj_out.bias'])\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5839e53-a27a-4be2-8e24-b5a5b0deec8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fdsMHA(nn.Module):\n",
    "    '''\n",
    "    自己アテンション\n",
    "    dim_hidden: 入力特徴量の次元\n",
    "    num_heads : マルチヘッドアテンションのヘッド数\n",
    "    qkv_bias  : クエリなどを生成する全結合層のバイアスの有無\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 qkv_bias: bool=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # 特徴量を各ヘッドのために分割するので、\n",
    "        # 特徴量次元をヘッド数で割り切れるか検証\n",
    "        assert dim_hidden % num_heads == 0\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # ヘッド毎の特徴量次元\n",
    "        dim_head = dim_hidden // num_heads\n",
    "\n",
    "        # ソフトマックスのスケール値\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        # ヘッド毎にクエリ、キーおよびバリューを生成するための全結合層\n",
    "        #self.proj_in = nn.Linear(\n",
    "        #    dim_hidden, dim_hidden * 3, bias=qkv_bias)\n",
    "\n",
    "        # 各ヘッドから得られた特徴量を一つにまとめる全結合層\n",
    "        #self.proj_out = nn.Linear(dim_hidden, dim_hidden)\n",
    "\n",
    "    def split_head(self, x):\n",
    "        x = torch.tensor_split(x, self.num_heads, dim = 2)\n",
    "        x = torch.stack(x, dim = 1)\n",
    "        return x\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor,  v: torch.Tensor, mask: torch.Tensor, i, weights ):\n",
    "        bs, ns = q.shape[:2]\n",
    "\n",
    "        #qkv = self.proj_in(x)\n",
    "        #print( \"size q:\", q.size() )\n",
    "        #print( \"size weigths q:\", weights['d_layers.' + str(i) + '.crossattn.proj_in_q.weight'].size() )\n",
    "        #print( \"size k:\", k.size() )\n",
    "        #print( \"size weights k:\", weights['d_layers.' + str(i) + '.crossattn.proj_in_k.weight'].size() )\n",
    "        q = F.linear(q, weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_in_q.weight'], weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_in_q.bias'])\n",
    "        k = F.linear(k, weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_in_k.weight'], weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_in_k.bias'])\n",
    "        v = F.linear(v, weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_in_v.weight'], weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_in_v.bias'])\n",
    "\n",
    "        q = self.split_head( q )\n",
    "        k = self.split_head( k )\n",
    "        v = self.split_head( v )\n",
    "\n",
    "        # view関数により\n",
    "        # [バッチサイズ, 特徴量数, QKV, ヘッド数, ヘッドの特徴量次元]\n",
    "        # permute関数により\n",
    "        # [QKV, バッチサイズ, ヘッド数, 特徴量数, ヘッドの特徴量次元]\n",
    "        #qkv = qkv.view(\n",
    "        #    bs, ns, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        # クエリ、キーおよびバリューに分解\n",
    "        #q, k, v = qkv.unbind(0)\n",
    "\n",
    "        # クエリとキーの行列積とアテンションの計算(今回マスクは不使用)\n",
    "        # attnは[バッチサイズ, ヘッド数, 特徴量数, 特徴量数]\n",
    "        attn = q.matmul(k.transpose(-2, -1))\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill_(mask, -torch.finfo(torch.float).max)      \n",
    "        attn = (attn * self.scale).softmax(dim=-1)\n",
    "\n",
    "        # アテンションとバリューの行列積によりバリューを収集\n",
    "        # xは[バッチサイズ, ヘッド数, 特徴量数, ヘッドの特徴量次元]\n",
    "        x = attn.matmul(v)\n",
    "\n",
    "        # permute関数により\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数, ヘッドの特徴量次元]\n",
    "        # flatten関数により全てのヘッドから得られる特徴量を連結して、\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数 * ヘッドの特徴量次元]\n",
    "        x = x.permute(0, 2, 1, 3).flatten(2)\n",
    "        #x = self.proj_out(x)\n",
    "        x = F.linear(x, weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_out.weight'], weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_out.bias'])\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0464026d-c2fd-456a-89dc-e70cd632fbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fdcMHA(nn.Module):\n",
    "    '''\n",
    "    自己アテンション\n",
    "    dim_hidden: 入力特徴量の次元\n",
    "    num_heads : マルチヘッドアテンションのヘッド数\n",
    "    qkv_bias  : クエリなどを生成する全結合層のバイアスの有無\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 qkv_bias: bool=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # 特徴量を各ヘッドのために分割するので、\n",
    "        # 特徴量次元をヘッド数で割り切れるか検証\n",
    "        assert dim_hidden % num_heads == 0\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # ヘッド毎の特徴量次元\n",
    "        dim_head = dim_hidden // num_heads\n",
    "\n",
    "        # ソフトマックスのスケール値\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        # ヘッド毎にクエリ、キーおよびバリューを生成するための全結合層\n",
    "        #self.proj_in = nn.Linear(\n",
    "        #    dim_hidden, dim_hidden * 3, bias=qkv_bias)\n",
    "\n",
    "        # 各ヘッドから得られた特徴量を一つにまとめる全結合層\n",
    "        #self.proj_out = nn.Linear(dim_hidden, dim_hidden)\n",
    "\n",
    "    def split_head(self, x):\n",
    "        x = torch.tensor_split(x, self.num_heads, dim = 2)\n",
    "        x = torch.stack(x, dim = 1)\n",
    "        return x\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor,  v: torch.Tensor, mask: torch.Tensor, i, weights ):\n",
    "        bs, ns = q.shape[:2]\n",
    "\n",
    "        #qkv = self.proj_in(x)\n",
    "        #print( \"size q:\", q.size() )\n",
    "        #print( \"size weigths q:\", weights['d_layers.' + str(i) + '.crossattn.proj_in_q.weight'].size() )\n",
    "        #print( \"size k:\", k.size() )\n",
    "        #print( \"size weights k:\", weights['d_layers.' + str(i) + '.crossattn.proj_in_k.weight'].size() )\n",
    "        q = F.linear(q, weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_in_q.weight'], weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_in_q.bias'])\n",
    "        k = F.linear(k, weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_in_k.weight'], weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_in_k.bias'])\n",
    "        v = F.linear(v, weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_in_v.weight'], weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_in_v.bias'])\n",
    "\n",
    "        q = self.split_head( q )\n",
    "        k = self.split_head( k )\n",
    "        v = self.split_head( v )\n",
    "\n",
    "        # view関数により\n",
    "        # [バッチサイズ, 特徴量数, QKV, ヘッド数, ヘッドの特徴量次元]\n",
    "        # permute関数により\n",
    "        # [QKV, バッチサイズ, ヘッド数, 特徴量数, ヘッドの特徴量次元]\n",
    "        #qkv = qkv.view(\n",
    "        #    bs, ns, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        # クエリ、キーおよびバリューに分解\n",
    "        #q, k, v = qkv.unbind(0)\n",
    "\n",
    "        # クエリとキーの行列積とアテンションの計算(今回マスクは不使用)\n",
    "        # attnは[バッチサイズ, ヘッド数, 特徴量数, 特徴量数]\n",
    "        attn = q.matmul(k.transpose(-2, -1))\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill_(mask, -torch.finfo(torch.float).max)      \n",
    "        attn = (attn * self.scale).softmax(dim=-1)\n",
    "\n",
    "        # アテンションとバリューの行列積によりバリューを収集\n",
    "        # xは[バッチサイズ, ヘッド数, 特徴量数, ヘッドの特徴量次元]\n",
    "        x = attn.matmul(v)\n",
    "\n",
    "        # permute関数により\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数, ヘッドの特徴量次元]\n",
    "        # flatten関数により全てのヘッドから得られる特徴量を連結して、\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数 * ヘッドの特徴量次元]\n",
    "        x = x.permute(0, 2, 1, 3).flatten(2)\n",
    "        #x = self.proj_out(x)\n",
    "        x = F.linear(x, weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_out.weight'], weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_out.bias'])\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88160e0b-1bca-44a6-88d3-22f706a89b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "    '''\n",
    "    Transformerエンコーダ内の順伝播型ニューラルネットワーク\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    dim_feedforward: 中間特徴量の次元\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, dim_feedforward: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(dim_hidden, dim_feedforward)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, dim_hidden)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbd75eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class feFNN(nn.Module):\n",
    "    '''\n",
    "    Transformerエンコーダ内の順伝播型ニューラルネットワーク\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    dim_feedforward: 中間特徴量の次元\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, dim_feedforward: int):\n",
    "        super().__init__()\n",
    "\n",
    "        #self.linear1 = nn.Linear(dim_hidden, dim_feedforward)\n",
    "        #self.linear2 = nn.Linear(dim_feedforward, dim_hidden)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor, i, weights ):\n",
    "        #x = self.linear1(x)\n",
    "        x = F.linear(x, weights['encoder.encoder_layers.' + str(i) + '.fnn.linear1.weight'], weights['encoder.encoder_layers.' + str(i) + '.fnn.linear1.bias'])\n",
    "        x = self.activation(x)\n",
    "        #x = self.linear2(x)\n",
    "        x = F.linear(x, weights['encoder.encoder_layers.' + str(i) + '.fnn.linear2.weight'], weights['encoder.encoder_layers.' + str(i) + '.fnn.linear2.bias'])\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1830c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fdFNN(nn.Module):\n",
    "    '''\n",
    "    Transformerエンコーダ内の順伝播型ニューラルネットワーク\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    dim_feedforward: 中間特徴量の次元\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, dim_feedforward: int):\n",
    "        super().__init__()\n",
    "\n",
    "        #self.linear1 = nn.Linear(dim_hidden, dim_feedforward)\n",
    "        #self.linear2 = nn.Linear(dim_feedforward, dim_hidden)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor, i, weights ):\n",
    "        #x = self.linear1(x)\n",
    "        x = F.linear(x, weights['decoder.decoder_layers.' + str(i) + '.fnn.linear1.weight'], weights['decoder.decoder_layers.' + str(i) + '.fnn.linear1.bias'])\n",
    "        x = self.activation(x)\n",
    "        #x = self.linear2(x)\n",
    "        x = F.linear(x, weights['decoder.decoder_layers.' + str(i) + '.fnn.linear2.weight'], weights['decoder.decoder_layers.' + str(i) + '.fnn.linear2.bias'])\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "caaa86ef-4575-4c9c-9265-a337fb7a9734",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    '''\n",
    "    Transformerエンコーダ層\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    num_heads      : ヘッド数\n",
    "    dim_feedforward: 中間特徴量の次元\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 dim_feedforward: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = MHA(dim_hidden, num_heads, qkv_bias = True)\n",
    "        self.fnn = FNN(dim_hidden, dim_feedforward)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(dim_hidden)\n",
    "        self.norm2 = nn.LayerNorm(dim_hidden)\n",
    "        \n",
    "        self.dropout = nn.Dropout( dropout )\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor):\n",
    "        ''' B2T\n",
    "        x0 = x\n",
    "        x = self.attention( x, x, x, mask )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x = self.norm1(x)\n",
    "        x1 = x\n",
    "        x = self.fnn( x )\n",
    "        x = self.dropout( x )\n",
    "        x =  x + x1 + x0\n",
    "        x = self.norm2( x )\n",
    "        '''\n",
    "        #pre-LN\n",
    "        x0 = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attention( x, x, x, mask )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x1 = x\n",
    "        x = self.norm2( x )\n",
    "        x = self.fnn( x )\n",
    "        x = self.dropout( x )\n",
    "        x =  x + x1\n",
    "        \n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a2533c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fTransformerEncoderLayer(nn.Module):\n",
    "    '''\n",
    "    Transformerエンコーダ層\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    num_heads      : ヘッド数\n",
    "    dim_feedforward: 中間特徴量の次元\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 dim_feedforward: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fattention = feMHA(dim_hidden, num_heads)\n",
    "        self.fffn = feFNN(dim_hidden, dim_feedforward)\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.dropout = nn.Dropout( dropout )\n",
    "        \n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor, i, weights ):\n",
    "        '''B2T\n",
    "        x0 = x\n",
    "        x = self.fattention( x, x, x, mask, i, weights )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['encoder.encoder_layers.' + str(i) + '.norm1.weight'], bias=weights['encoder.encoder_layers.' + str(i) + '.norm1.bias'], eps=1e-05)\n",
    "        x1 = x\n",
    "        x = self.fffn( x, i, weights ) \n",
    "        x = self.dropout( x )\n",
    "        x = x + x1 + x0\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['encoder.encoder_layers.' + str(i) + '.norm2.weight'], bias=weights['encoder.encoder_layers.' + str(i) + '.norm2.bias'], eps=1e-05)\n",
    "        '''\n",
    "        #pre-LN\n",
    "        x0 = x\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['encoder.encoder_layers.' + str(i) + '.norm1.weight'], bias=weights['encoder.encoder_layers.' + str(i) + '.norm1.bias'], eps=1e-05)\n",
    "        x = self.fattention( x, x, x, mask, i, weights )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x1 = x\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['encoder.encoder_layers.' + str(i) + '.norm2.weight'], bias=weights['encoder.encoder_layers.' + str(i) + '.norm2.bias'], eps=1e-05)\n",
    "        x = self.fffn( x, i, weights ) \n",
    "        x = self.dropout( x )\n",
    "        x = x + x1\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "387ffc10-2b89-4039-8a6a-4b4a8fbde394",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    '''\n",
    "    Transformerエンコーダ層\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    num_heads      : ヘッド数\n",
    "    dim_feedforward: 中間特徴量の次元\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 dim_feedforward: int, dropout: float=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.selfattn = MHA(dim_hidden, num_heads, qkv_bias = True)\n",
    "        self.crossattn = MHA(dim_hidden, num_heads, qkv_bias = True)\n",
    "        self.fnn = FNN(dim_hidden, dim_feedforward)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(dim_hidden)\n",
    "        self.norm2 = nn.LayerNorm(dim_hidden)\n",
    "        self.norm3 = nn.LayerNorm(dim_hidden)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor,  self_mask: torch.Tensor,  cross_mask: torch.Tensor):\n",
    "        '''B2T\n",
    "        x0 = x\n",
    "        x = self.selfattn( x, x, x, self_mask )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x = self.norm1( x )\n",
    "        x1 = x\n",
    "        x = self.crossattn( x, y, y, cross_mask )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x1\n",
    "        x = self.norm2( x )\n",
    "        x2 = x\n",
    "        x = self.fnn( x )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x2 + x0\n",
    "        x = self.norm3( x )\n",
    "        '''\n",
    "        #pre-LN\n",
    "        x0 = x\n",
    "        x = self.norm1( x )\n",
    "        x = self.selfattn( x, x, x, self_mask )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x1 = x\n",
    "        x = self.norm2( x )\n",
    "        x = self.crossattn( x, y, y, cross_mask )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x1\n",
    "        x2 = x\n",
    "        x = self.norm3( x )\n",
    "        x = self.fnn( x )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x2\n",
    "        \n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "063637b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fTransformerDecoderLayer(nn.Module):\n",
    "    '''\n",
    "    Transformerエンコーダ層\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    num_heads      : ヘッド数\n",
    "    dim_feedforward: 中間特徴量の次元\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 dim_feedforward: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fselfattn = fdsMHA(dim_hidden, num_heads)\n",
    "        self.fcrossattn = fdcMHA(dim_hidden, num_heads)\n",
    "        self.fffn = fdFNN(dim_hidden, dim_feedforward)\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        #self.norm1 = nn.LayerNorm(dim_hidden)\n",
    "        #self.norm2 = nn.LayerNorm(dim_hidden)\n",
    "        #self.norm3 = nn.LayerNorm(dim_hidden)\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor,  self_mask: torch.Tensor,  cross_mask: torch.Tensor, i, weights):\n",
    "        '''B2T\n",
    "        x0 = x\n",
    "        x = self.fselfattn( x, x, x, self_mask, i, weights )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['decoder.decoder_layers.' + str(i) + '.norm1.weight'], bias=weights['decoder.decoder_layers.' + str(i) + '.norm1.bias'], eps=1e-05)\n",
    "        x1 = x\n",
    "        x = self.fcrossattn( x, y, y, cross_mask, i, weights )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x1\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['decoder.decoder_layers.' + str(i) + '.norm2.weight'], bias=weights['decoder.decoder_layers.' + str(i) + '.norm2.bias'], eps=1e-05)\n",
    "        x2 = x\n",
    "        x = self.fffn( x, i, weights )\n",
    "        x = self.dropout( x ) \n",
    "        x = x + x2 + x0\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['decoder.decoder_layers.' + str(i) + '.norm3.weight'], bias=weights['decoder.decoder_layers.' + str(i) + '.norm3.bias'], eps=1e-05)\n",
    "        '''\n",
    "        #pre-LN\n",
    "        x0 = x\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['decoder.decoder_layers.' + str(i) + '.norm1.weight'], bias=weights['decoder.decoder_layers.' + str(i) + '.norm1.bias'], eps=1e-05)\n",
    "        x = self.fselfattn( x, x, x, self_mask, i, weights )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x1 = x\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['decoder.decoder_layers.' + str(i) + '.norm2.weight'], bias=weights['decoder.decoder_layers.' + str(i) + '.norm2.bias'], eps=1e-05)\n",
    "        x = self.fcrossattn( x, y, y, cross_mask, i, weights )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x1\n",
    "        x2 = x\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['decoder.decoder_layers.' + str(i) + '.norm3.weight'], bias=weights['decoder.decoder_layers.' + str(i) + '.norm3.bias'], eps=1e-05)\n",
    "        x = self.fffn( x, i, weights )\n",
    "        x = self.dropout( x ) \n",
    "        x = x + x2\n",
    "        \n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0509fcd8-bf62-4006-a5ca-94f2fe331e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    '''\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    dim_feedforward: FNNにおける中間特徴量の次元\n",
    "    num_heads      : マルチヘッドアテンションのヘッド数\n",
    "    num_layers     : Transformerエンコーダの層数\n",
    "    '''\n",
    "    def __init__(self, text_vocab_size: int, dim_embedding: int, dim_feedforward: int,\n",
    "                 num_heads: int,  num_layers: int, pad_index:int, dropout: float = 0.1 ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 単語埋め込み\n",
    "        self.embed = nn.Embedding(\n",
    "            text_vocab_size, dim_embedding, padding_idx=pad_index)        \n",
    "        \n",
    "        # 位置エンコーディング\n",
    "        #self.pos_enc = PositionalEncoding(dim_embedding)\n",
    "        self.pos_emb = PositionalEmbedding(dim_embedding)\n",
    "        #self.dropout = nn.Dropout( dropout )\n",
    "        \n",
    "        # Transformerエンコーダ層\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(dim_hidden=dim_embedding, num_heads=num_heads, dim_feedforward = dim_feedforward, dropout = dropout )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(dim_embedding)\n",
    "       \n",
    "        self.pad_index = pad_index\n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x           : 入力, [バッチサイズ, 入力チャネル数, 高さ, 幅]\n",
    "    return_embed: 特徴量を返すかロジットを返すかを選択する真偽値\n",
    "    '''\n",
    "    def forward(self, src: torch.Tensor, src_mask: torch.Tensor=None, src_padding_mask: torch.Tensor=None ):\n",
    "\n",
    "        if src_padding_mask is not None and src_mask is None:\n",
    "            mask = src_padding_mask[:,None,None,:]\n",
    "            mask = mask.expand( (-1, self.num_heads, src.size(1), -1) )\n",
    "        elif src_padding_mask is not None and src_mask is not None:\n",
    "            mask1 = src_padding_mask[:,None,None,:]\n",
    "            mask1 = mask1.expand( (-1, self.num_heads, src.size(1), -1 ) )\n",
    "            mask2 = src_mask[None,None,:,:]\n",
    "            mask2 = mask2.expand( ( src.size(0), self.num_heads, -1, -1 ) )\n",
    "            mask = torch.logical_or(mask1, mask2 )\n",
    "        elif src_padding_mask is None and src_mask is not None:\n",
    "            mask = src_mask[None,None,:,:]\n",
    "            mask = mask.padding( ( src.size(0), src.size(1), -1, -1 ) )\n",
    "        else:\n",
    "            mask = None\n",
    "\n",
    "        x = self.embed( src ) * math.sqrt( self.dim_embedding )\n",
    "\n",
    "        #x = self.pos_enc( x )\n",
    "        position = self.pos_emb( x )\n",
    "        x = x + position\n",
    "        #x = self.dropout( x )\n",
    "        #x = self.norm( x )\n",
    "        \n",
    "        # Transformerエンコーダ層を適用\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer( x, mask )\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd45f25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fTransformerEncoder(nn.Module):\n",
    "    '''\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    dim_feedforward: FNNにおける中間特徴量の次元\n",
    "    num_heads      : マルチヘッドアテンションのヘッド数\n",
    "    num_layers     : Transformerエンコーダの層数\n",
    "    '''\n",
    "    def __init__(self, text_vocab_size: int, dim_embedding: int, dim_feedforward: int,\n",
    "                 num_heads: int,  num_layers: int, pad_idx:int, dropout: float = 0.1 ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 単語埋め込み\n",
    "        #self.embed = nn.Embedding(\n",
    "        #    text_vocab_size, dim_embedding, padding_idx=pad_index)        \n",
    "        \n",
    "        # 位置エンコーディング\n",
    "        #self.pos_enc = PositionalEncoding(dim_embedding)\n",
    "        self.fpos_emb = fPositionalEmbedding(dim_embedding, \"encoder.pos_emb.pos_emb.weight\" )\n",
    "        #self.dropout = nn.Dropout( dropout )\n",
    "        \n",
    "        # Transformerエンコーダ層\n",
    "        self.fenclayer = fTransformerEncoderLayer(dim_hidden=dim_embedding, num_heads=num_heads, dim_feedforward = dim_feedforward, dropout = dropout )\n",
    "        \n",
    "        # ロジットを生成する前のレイヤー正規化と全結合\n",
    "        #self.norm = nn.LayerNorm(dim_embedding)\n",
    "        \n",
    "        self.pad_idx = pad_idx\n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x           : 入力, [バッチサイズ, 入力チャネル数, 高さ, 幅]\n",
    "    return_embed: 特徴量を返すかロジットを返すかを選択する真偽値\n",
    "    '''\n",
    "    def forward(self, src: torch.Tensor, src_mask: torch.Tensor=None, src_padding_mask: torch.Tensor=None, weights = None ):\n",
    "\n",
    "        if src_padding_mask is not None and src_mask is None:\n",
    "            mask = src_padding_mask[:,None,None,:]\n",
    "            mask = mask.expand( (-1, self.num_heads, src.size(1), -1) )\n",
    "        elif src_padding_mask is not None and src_mask is not None:\n",
    "            mask1 = src_padding_mask[:,None,None,:]\n",
    "            mask1 = mask1.expand( (-1, self.num_heads, src.size(1), -1 ) )\n",
    "            mask2 = src_mask[None,None,:,:]\n",
    "            mask2 = mask2.expand( ( src.size(0), self.num_heads, -1, -1 ) )\n",
    "            mask = torch.logical_or(mask1, mask2 )\n",
    "        elif src_padding_mask is None and src_mask is not None:\n",
    "            mask = src_mask[None,None,:,:]\n",
    "            mask = mask.padding( ( src.size(0), src.size(1), -1, -1 ) )\n",
    "        else:\n",
    "            mask = None\n",
    "\n",
    "        #x = self.embed( src ) * math.sqrt( self.dim_embedding )\n",
    "        x = F.embedding( src, weights['encoder.embed.weight'], padding_idx = self.pad_idx )  * math.sqrt( self.dim_embedding )\n",
    "        \n",
    "        #x = self.pos_enc( x )\n",
    "        position = self.fpos_emb( x, weights )\n",
    "        x = x + position\n",
    "        #x = self.dropout( x )\n",
    "        ##x = self.norm(x)\n",
    "        #x = F.layer_norm(x, (self.dim_embedding,), weight=weights['encoder.norm.weight'], bias=weights['encoder.norm.bias'], eps=1e-05)\n",
    "    \n",
    "        # Transformerエンコーダ層を適用\n",
    "        #for layer in self.encoder_layers:\n",
    "        #    x = layer( x, mask )\n",
    "        for block in range( self.num_layers ):\n",
    "            x = self.fenclayer(x, mask, block, weights )\n",
    "\n",
    "        #x = self.norm(x)\n",
    "        x = F.layer_norm(x, (self.dim_embedding,), weight=weights['encoder.norm.weight'], bias=weights['encoder.norm.bias'], eps=1e-05)\n",
    "        \n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15582995-ff08-4fbe-9f54-9e61e13f89e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    '''\n",
    "    CaptioningTransformerのコンストラクタ\n",
    "    dim_embedding  : 埋め込み次元\n",
    "    dim_feedforward: FNNの中間特徴次元\n",
    "    num_heads      : マルチヘッドアテンションのヘッド数\n",
    "    num_layers     : Transformerデコーダ層の数\n",
    "    vocab_size     : 辞書の次元\n",
    "    pad_index     : NULLのID\n",
    "    dropout        : ドロップアウト確率\n",
    "    '''\n",
    "    def __init__(self, vocab_size: int, dim_embedding: int, dim_feedforward: int,\n",
    "                 num_heads: int, num_layers: int, \n",
    "                 pad_index: int, dropout: float=0.1, ds_rate: float=0.1 ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 単語埋め込み\n",
    "        self.embed = nn.Embedding(\n",
    "            vocab_size, dim_embedding, padding_idx=pad_index)\n",
    "        \n",
    "        # 位置エンコーディング\n",
    "        #self.pos_enc = PositionalEncoding(dim_embedding)\n",
    "        self.pos_emb = PositionalEmbedding(dim_embedding)\n",
    "        #self.dropout = nn.Dropout( dropout )\n",
    "\n",
    "        # Transformerデコーダ\n",
    "        #self.decoder_layers = nn.ModuleList([\n",
    "        #    TransformerDecoderLayer(\n",
    "        #        dim_embedding, num_heads, dim_feedforward, dropout)\n",
    "        #    for _ in range(num_layers)\n",
    "        #])\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer(dim_hidden=dim_embedding, num_heads=num_heads, dim_feedforward = dim_feedforward, dropout = dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # ロジットを生成する前のレイヤー正規化と全結合\n",
    "        #self.norm = nn.LayerNorm(dim_embedding)\n",
    "        \n",
    "        # 単語出力分布計算\n",
    "        self.linear = nn.Linear(dim_embedding, vocab_size)\n",
    "        \n",
    "        self.pad_index = pad_index\n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "\n",
    "\n",
    "    ''' CaptioningTransformerの順伝播処理\n",
    "    features: 画像特徴量 [バッチサイズ, 埋め込み次元]\n",
    "    captions: 正解キャプション [バッチサイズ, 系列長]\n",
    "\n",
    "    '''\n",
    "    #def forward(self, features: torch.Tensor, caption_lengths: torch.Tensor):\n",
    "    #def forward(self, features: torch.Tensor, captions: torch.Tensor, padding_mask_src: torch.Tensor=None, \\\n",
    "    #            padding_mask_tgt: torch.Tensor=None, mask_tgt: torch.Tensor=None ):\n",
    "    def forward(self, features: torch.Tensor, captions: torch.Tensor, memory_padding_mask: torch.Tensor=None, \\\n",
    "                tgt_padding_mask: torch.Tensor=None, tgt_mask: torch.Tensor=None ):\n",
    "\n",
    "        #feature_lengths = torch.ones( (features.size(0) ), device=features.device ) * features.size(1)\n",
    "\n",
    "        tgt = captions\n",
    "        if tgt_padding_mask is not None and tgt_mask is not None:\n",
    "            self_mask1 = tgt_padding_mask[:,None,None,:]\n",
    "            self_mask1 = self_mask1.expand( (-1, self.num_heads, tgt.size(1), -1) )\n",
    "            self_mask2 = tgt_mask[None,None,:,:]\n",
    "            self_mask2 = self_mask2.expand( (tgt.size(0), self.num_heads, -1, -1 ))\n",
    "            self_mask = torch.logical_or(self_mask1, self_mask2 )\n",
    "        elif tgt_padding_mask is not None and tgt_mask is None:\n",
    "            self_mask = tgt_padding_mask[:,None,None,:]\n",
    "            self_mask = self_mask.expand( (-1, self.num_heads, tgt.size(1), -1 ) )\n",
    "        elif tgt_padding_mask is None and tgt_mask is not None:\n",
    "            self_mask = tgt_mask[None,None,:,:]\n",
    "            self_mask = self_mask.expand( (tgt.size(0), self.num_heads, -1, -1 ))\n",
    "        elif tgt_padding_mask is None and tgt_mask is None:\n",
    "            self_mask = None\n",
    "            \n",
    "        if memory_padding_mask is not None:\n",
    "            cross_mask = memory_padding_mask[:,None,None,:]\n",
    "            cross_mask = cross_mask.expand((-1,self.num_heads, tgt.size(1), -1))\n",
    "        else:\n",
    "            cross_mask = None\n",
    "        \n",
    "        # 単語埋め込み [バッチサイズ, 系列長]\n",
    "        # -> [バッチサイズ, 系列長, 埋め込み次元]\n",
    "        embeddings = self.embed(captions) * math.sqrt( self.dim_embedding )\n",
    "        seq = embeddings.shape[1]\n",
    "        \n",
    "        # 位置エンコーディング\n",
    "        #embeddings = self.pos_enc( embeddings )\n",
    "        positions = self.pos_emb(embeddings)\n",
    "        embeddings = embeddings + positions\n",
    "        #embeddings = self.dropout( embeddings )\n",
    "        #embeddings = self.norm(embeddings)\n",
    "        \n",
    "        # Transformerデコーダでキャプション生成\n",
    "        # 画像の特徴も入力する\n",
    "        for layer in self.decoder_layers:\n",
    "            embeddings = layer( embeddings, features, self_mask, cross_mask )\n",
    "\n",
    "        \n",
    "        # [バッチサイズ, 系列長, 埋め込み次元]\n",
    "        # -> [バッチサイズ, 系列長, 辞書の次元]\n",
    "        preds = self.linear(embeddings)\n",
    "        #print( \"argmax of preds:\", torch.argmax( preds, dim = 2 ))\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa9c54fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fTransformerDecoder(nn.Module):\n",
    "    '''\n",
    "    CaptioningTransformerのコンストラクタ\n",
    "    dim_embedding  : 埋め込み次元\n",
    "    dim_feedforward: FNNの中間特徴次元\n",
    "    num_heads      : マルチヘッドアテンションのヘッド数\n",
    "    num_layers     : Transformerデコーダ層の数\n",
    "    vocab_size     : 辞書の次元\n",
    "    pad_index     : NULLのID\n",
    "    dropout        : ドロップアウト確率\n",
    "    '''\n",
    "    def __init__(self, vocab_size: int, dim_embedding: int, dim_feedforward: int,\n",
    "                 num_heads: int, num_layers: int, \n",
    "                 pad_idx: int, dropout: float=0.1, ds_rate: float=0.1 ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 単語埋め込み\n",
    "        #self.embed = nn.Embedding(\n",
    "        #    vocab_size, dim_embedding, padding_idx=pad_index)\n",
    "        \n",
    "        # 位置エンコーディング\n",
    "        #self.pos_enc = PositionalEncoding(dim_embedding)\n",
    "        self.fpos_emb = fPositionalEmbedding(dim_embedding, \"decoder.pos_emb.pos_emb.weight\" )\n",
    "        #self.dropout = nn.Dropout( dropout )\n",
    "        \n",
    "        # Transformerデコーダ\n",
    "        self.fdeclayer = fTransformerDecoderLayer(dim_hidden=dim_embedding, num_heads=num_heads, dim_feedforward = dim_feedforward, dropout = dropout )\n",
    "        \n",
    "        # 単語出力分布計算\n",
    "        #self.linear = nn.Linear(dim_embedding, vocab_size)\n",
    "        \n",
    "        self.pad_idx = pad_idx\n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    ''' CaptioningTransformerの順伝播処理\n",
    "    features: 画像特徴量 [バッチサイズ, 埋め込み次元]\n",
    "    captions: 正解キャプション [バッチサイズ, 系列長]\n",
    "\n",
    "    '''\n",
    "    def forward(self, features: torch.Tensor, captions: torch.Tensor, memory_padding_mask: torch.Tensor=None, \\\n",
    "                tgt_padding_mask: torch.Tensor=None, tgt_mask: torch.Tensor=None, weights = None ):\n",
    "\n",
    "        tgt = captions\n",
    "        if tgt_padding_mask is not None and tgt_mask is not None:\n",
    "            self_mask1 = tgt_padding_mask[:,None,None,:]\n",
    "            self_mask1 = self_mask1.expand( (-1, self.num_heads, tgt.size(1), -1) )\n",
    "            self_mask2 = tgt_mask[None,None,:,:]\n",
    "            self_mask2 = self_mask2.expand( (tgt.size(0), self.num_heads, -1, -1 ))\n",
    "            self_mask = torch.logical_or(self_mask1, self_mask2 )\n",
    "        elif tgt_padding_mask is not None and tgt_mask is None:\n",
    "            self_mask = tgt_padding_mask[:,None,None,:]\n",
    "            self_mask = self_mask.expand( (-1, self.num_heads, tgt.size(1), -1 ) )\n",
    "        elif tgt_padding_mask is None and tgt_mask is not None:\n",
    "            self_mask = tgt_mask[None,None,:,:]\n",
    "            self_mask = self_mask.expand( (tgt.size(0), self.num_heads, -1, -1 ))\n",
    "        elif tgt_padding_mask is None and tgt_mask is None:\n",
    "            self_mask = None\n",
    "            \n",
    "        if memory_padding_mask is not None:\n",
    "            cross_mask = memory_padding_mask[:,None,None,:]\n",
    "            cross_mask = cross_mask.expand((-1,self.num_heads, tgt.size(1), -1))\n",
    "        else:\n",
    "            cross_mask = None\n",
    "        \n",
    "        # 単語埋め込み [バッチサイズ, 系列長]\n",
    "        # -> [バッチサイズ, 系列長, 埋め込み次元]\n",
    "        embeddings = F.embedding( captions, weights['decoder.embed.weight'], padding_idx = self.pad_idx )  * math.sqrt( self.dim_embedding )\n",
    "        seq = embeddings.shape[1]\n",
    "        \n",
    "        # 位置エンコーディング\n",
    "        #embeddings = self.pos_enc(embeddings)\n",
    "        positions = self.fpos_emb( embeddings, weights )\n",
    "        embeddings = embeddings + positions\n",
    "        #embeddings = self.dropout( embeddings )\n",
    "        ##embeddings = self.norm(embeddings)\n",
    "        #embeddings = F.layer_norm(embeddings, (self.dim_embedding,), weight=weights['decoder.norm.weight'], bias=weights['decoder.norm.bias'], eps=1e-05)\n",
    "        \n",
    "        # Transformerデコーダでキャプション生成\n",
    "        # 画像の特徴も入力する\n",
    "        #for layer in self.decoder_layers:\n",
    "        #    #embeddings = layer( embeddings, features, tgt_key_padding_mask = padding_mask_tgt, \\\n",
    "        #    #                                memory_key_padding_mask = padding_mask_src, tgt_is_causal = True, tgt_mask = mask_tgt )\n",
    "        for block in range( self.num_layers ):\n",
    "            embeddings = self.fdeclayer(embeddings, features, self_mask, cross_mask, block, weights )\n",
    "  \n",
    "        # [バッチサイズ, 系列長, 埋め込み次元]\n",
    "        # -> [バッチサイズ, 系列長, 辞書の次元]\n",
    "        #preds = self.linear(embeddings)\n",
    "        preds = F.linear( embeddings, weights['decoder.linear.weight'], weights['decoder.linear.bias'] )\n",
    "        #print( \"argmax of preds:\", torch.argmax( preds, dim = 2 ))\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14d1d088-4485-4752-b85b-25197f08824b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim_embedding: int, dim_feedforward: int,\n",
    "                 num_heads: int, num_layers: int, enc_vocab_size: int, dec_vocab_size: int,\n",
    "                 j_pad_index: int,e_pad_index: int, dropout: float=0.1, ds_rate: float=0.1 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = TransformerEncoder(enc_vocab_size, dim_embedding, dim_feedforward, num_heads, num_layers, j_pad_index, dropout )\n",
    "        self.decoder = TransformerDecoder(dec_vocab_size, dim_embedding, dim_feedforward, num_heads, num_layers, e_pad_index, dropout )\n",
    "        self.fencoder = fTransformerEncoder(enc_vocab_size, dim_embedding, dim_feedforward, num_heads, num_layers, j_pad_index, dropout )\n",
    "        self.fdecoder = fTransformerDecoder(dec_vocab_size, dim_embedding, dim_feedforward, num_heads, num_layers, e_pad_index, dropout )\n",
    "        self.j_pad_index = j_pad_index\n",
    "        self.e_pad_index = e_pad_index\n",
    "\n",
    "        self._reset_parameters()\n",
    "        #self._reset_parameters2()\n",
    "    \n",
    "    def _reset_parameters(self):\n",
    "        r\"\"\"Initiate parameters in the transformer model.\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                xavier_uniform_(p)\n",
    "                #xavier_normal_(p)\n",
    "                #kaiming_uniform_(p)\n",
    "                #kaiming_normal_(p)\n",
    "\n",
    "    #def _reset_parameters2(self):\n",
    "    #    for module in self.modules():\n",
    "    #        if isinstance(module, nn.Linear):\n",
    "    #            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    #            if module.bias is not None:\n",
    "    #                nn.init.zeros_(module.bias)\n",
    "    #        elif isinstance(module, nn.Embedding):\n",
    "    #            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    #        elif isinstance(module, nn.LayerNorm):\n",
    "    #            nn.init.zeros_(module.bias)\n",
    "    #            nn.init.ones_(module.weight)\n",
    "        \n",
    "    ''' CaptioningTransformerの順伝播処理\n",
    "    features: 画像特徴量 [バッチサイズ, 埋め込み次元]\n",
    "    captions: 正解キャプション [バッチサイズ, 系列長]\n",
    "\n",
    "    '''\n",
    "    def forward(self, text, dec_input):\n",
    "\n",
    "        seq_len_src = text.shape[1]\n",
    "        seq_len_tgt = dec_input.shape[1]\n",
    "\n",
    "        mask_tgt = nn.Transformer.generate_square_subsequent_mask( seq_len_tgt, dtype=bool ).to(device)\n",
    "        mask_src = torch.zeros((seq_len_src, seq_len_src), device=device).type(torch.bool)\n",
    "\n",
    "        padding_mask_src = (text == idx_list['<pad>'])\n",
    "        padding_mask_tgt = (dec_input == idx_list_en['<pad>'])\n",
    "    \n",
    "        x = self.encoder( text, mask_src, padding_mask_src )\n",
    "        preds = self.decoder(x,dec_input, padding_mask_src, padding_mask_tgt, mask_tgt )\n",
    "\n",
    "        return preds\n",
    "    \n",
    "    def adaptation(self, text, dec_input, weights):\n",
    "\n",
    "        seq_len_src = text.shape[1]\n",
    "        seq_len_tgt = dec_input.shape[1]\n",
    "\n",
    "        mask_tgt = nn.Transformer.generate_square_subsequent_mask( seq_len_tgt, dtype=bool ).to(device)\n",
    "        mask_src = torch.zeros((seq_len_src, seq_len_src), device=device).type(torch.bool)\n",
    "\n",
    "        padding_mask_src = (text == idx_list['<pad>']).to(text.device )\n",
    "        padding_mask_tgt = (dec_input == idx_list_en['<pad>']).to(text.device )\n",
    "        \n",
    "        x = self.fencoder( text, mask_src, padding_mask_src, weights)\n",
    "        preds = self.fdecoder( x, dec_input, padding_mask_src, padding_mask_tgt, mask_tgt, weights )\n",
    "        \n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88539768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#model = Transformer(512, 2048, 8, 6, enc_vocab_size, dec_vocab_size, idx_list['<pad>'], idx_list_en['<pad>'] ).to(device)\n",
    "dim_hidden = 768\n",
    "dim_feedforward = dim_hidden * 4\n",
    "heads = 12\n",
    "layers =8\n",
    "dropout = 0.1\n",
    "#dim_hidden = 256\n",
    "#dim_feedforward = dim_hidden * 4\n",
    "#heads = 4\n",
    "#layers =4\n",
    "#dropout = 0.0\n",
    "model = Transformer(dim_hidden, dim_feedforward, heads, layers, enc_vocab_size, dec_vocab_size, idx_list['<pad>'], idx_list_en['<pad>'], dropout = dropout ).to(device)\n",
    "#model = torch.load( \"best_model.pth\", weights_only = False )\n",
    "model = model.to(device)\n",
    "#model = MyTransformer(768, 768 * 4, 12, 12, enc_vocab_size, dec_vocab_size, idx_list['<pad>'], idx_list_en['<pad>'], start_idx = start_idx_t, max_seq = 200  ).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=idx_list_en['<pad>'])\n",
    "loss_fn = criterion\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "PATH = \"./model_NTT_auto_curr3.pt\"\n",
    "if os.path.isfile(PATH):\n",
    "    print( device )\n",
    "    if device != torch.device(\"cpu\"):\n",
    "        checkpoint = torch.load(PATH)\n",
    "    else:\n",
    "        checkpoint = torch.load(PATH, map_location=torch.device('cpu'))\n",
    "\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    #optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    #scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    ## optimizerのstateを現在のdeviceに移す。これをしないと、保存前後でdeviceの不整合が起こる可能性がある。\n",
    "    #for state in optimizer.state.values():\n",
    "    #    for k, v in state.items():\n",
    "    #        if isinstance(v, torch.Tensor):\n",
    "    #            state[k] = v.to(device)\n",
    "    #new_epoch = checkpoint['epoch']\n",
    "    #loss = checkpoint['loss']\n",
    "    #tra_global_step = checkpoint['tra_global_step']\n",
    "    #val_global_step = checkpoint['val_global_step']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b845bbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "outputs1 size: torch.Size([8, 120, 20556])\n",
      "tensor(-7.4923, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "outputs2 size: torch.Size([8, 120, 20556])\n",
      "tensor(-7.4923, device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): TransformerEncoder(\n",
       "    (embed): Embedding(49528, 768, padding_idx=0)\n",
       "    (pos_emb): PositionalEmbedding(\n",
       "      (pos_emb): Embedding(5000, 768)\n",
       "    )\n",
       "    (encoder_layers): ModuleList(\n",
       "      (0-7): 8 x TransformerEncoderLayer(\n",
       "        (attention): MHA(\n",
       "          (proj_in_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_in_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_in_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_out): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (fnn): FNN(\n",
       "          (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELU(approximate='none')\n",
       "        )\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (embed): Embedding(20556, 768, padding_idx=0)\n",
       "    (pos_emb): PositionalEmbedding(\n",
       "      (pos_emb): Embedding(5000, 768)\n",
       "    )\n",
       "    (decoder_layers): ModuleList(\n",
       "      (0-7): 8 x TransformerDecoderLayer(\n",
       "        (selfattn): MHA(\n",
       "          (proj_in_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_in_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_in_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_out): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (crossattn): MHA(\n",
       "          (proj_in_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_in_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_in_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_out): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (fnn): FNN(\n",
       "          (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELU(approximate='none')\n",
       "        )\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (linear): Linear(in_features=768, out_features=20556, bias=True)\n",
       "  )\n",
       "  (fencoder): fTransformerEncoder(\n",
       "    (fpos_emb): fPositionalEmbedding()\n",
       "    (fenclayer): fTransformerEncoderLayer(\n",
       "      (fattention): feMHA()\n",
       "      (fffn): feFNN(\n",
       "        (activation): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (fdecoder): fTransformerDecoder(\n",
       "    (fpos_emb): fPositionalEmbedding()\n",
       "    (fdeclayer): fTransformerDecoderLayer(\n",
       "      (fselfattn): fdsMHA()\n",
       "      (fcrossattn): fdcMHA()\n",
       "      (fffn): fdFNN(\n",
       "        (activation): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#seed = 0\n",
    "\n",
    "#random.seed(seed)\n",
    "#np.random.seed(seed)\n",
    "#torch.manual_seed(seed)\n",
    "#torch.backends.cudnn.benchmark = False\n",
    "#torch.backends.cudnn.deterministic = True\n",
    "\n",
    "text = torch.randint( 0, enc_vocab_size, size=(8, 100 ))\n",
    "dec_input = torch.randint( 0, dec_vocab_size, size=(1,120))\n",
    "\n",
    "seq_len_src = text.shape[1]\n",
    "seq_len_tgt = dec_input.shape[1]\n",
    "\n",
    "mask_tgt = nn.Transformer.generate_square_subsequent_mask( seq_len_tgt, dtype=bool ).to(device)\n",
    "mask_src = torch.zeros((seq_len_src, seq_len_src), device=device).type(torch.bool)\n",
    "\n",
    "padding_mask_src = (text == idx_list['<pad>'])\n",
    "padding_mask_tgt = (dec_input == idx_list_en['<pad>'])\n",
    "\n",
    "weights = OrderedDict(model.named_parameters())\n",
    "model.eval()\n",
    "print( device )\n",
    "outputs1 = model( text.to(device), dec_input.to(device) )\n",
    "print( \"outputs1 size:\", outputs1.size())\n",
    "print( outputs1[0][0][0])\n",
    "\n",
    "#for name in weights:\n",
    "#    print( name )\n",
    "model.eval()\n",
    "outputs2 = model.adaptation( text.to(device), dec_input.to(device), weights )\n",
    "print( \"outputs2 size:\", outputs2.size() )\n",
    "print( outputs2[0][0][0])\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b01dfbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_global_steps: 2495000\n",
      "num_warmup_steps: 249500.0\n",
      "Train\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 8.012024048096193e-08\n",
      "pred: for some reason when we express our views we often stress the particular the various problems rather of focusing on the positive results and its potential of is in future cooperation <eos>\n",
      "tar:  for some reason when we give our opinions we often stress in particular russia's various problems instead of concentrating on its positive achievements and the potential that exists in future cooperation <eos>\n",
      "pred: i right method to not seem to be the right way of create a more stable and united europe than is the objective of enlargement <eos>\n",
      "tar:  the chosen solution does not seem to be the best way to create a more stable and united europe which is the aim of enlargement <eos>\n",
      "epoch:1  Step:2000  loss:1.3230314255  WER:0.3429084381\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also generates a jobs jobs in the processing industry or in the <unk> sector industry industry industry sector <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:25  loss:1.4460163689  WER:0.3609931408\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 1.6028056112224448e-07\n",
      "pred: in we actually actually hoping to is that we will complete this document today <eos>\n",
      "tar:  what we are actually hoping for is that we can complete this document today <eos>\n",
      "pred: how can they be <eos>\n",
      "tar:  how can this be? <eos>\n",
      "epoch:1  Step:4000  loss:1.4557483196  WER:0.3653295129\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also generates many jobs jobs in the processing industry or in the korean <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:50  loss:1.4433844471  WER:0.3559604241\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 2.4044088176352706e-07\n",
      "pred: i fully support the commission in this point <eos>\n",
      "tar:  i fully support the commission on this point <eos>\n",
      "pred: the legislation of legislation that is wallström is responsible the of is a horizontal directive of directive but in the <unk> but which <unk> however was however however a would be a legislation in forward in due course <eos>\n",
      "tar:  the piece of legislation that commissioner wallström is in charge of is a horizontal piece of legislation as is directive <unk> in directive <unk> it was envisaged that there would be sectoral legislation brought forward in due course <eos>\n",
      "epoch:1  Step:6000  loss:1.1919544935  WER:0.3221153846\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also generates a jobs jobs in the processing industry or in the korean <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:75  loss:1.4362136030  WER:0.3566135113\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 3.206012024048096e-07\n",
      "pred: they seek for immediate success at at enjoy is the matter record in the united states which high the the interest interest rates and is is set to a because the federal reserve has just increased interest interest interest rates <eos>\n",
      "tar:  they look for immediate returns and currently this is a safer bet in the united states particularly because of its higher interest rates this trend looks set to continue as the federal reserve has just <unk> up its interest rates <eos>\n",
      "pred: as i have said i would particularly like to thank mr wieland but also all speakers who have cooperated very him for the their great and they have done to order to reach this agreement <eos>\n",
      "tar:  as i already stated i would particularly like to thank mr wieland but also other members who have worked with him for all the sound work they have done in order to reach this agreement <eos>\n",
      "epoch:1  Step:8000  loss:1.2105718851  WER:0.3302752294\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also generates many jobs jobs in the processing industry or in the yards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:100  loss:1.4350073242  WER:0.3545302789\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 4.007615230460922e-07\n",
      "pred: the members of the <unk> delegation to the european parliament supported supported mr <unk> s report in order to give their support to the implementation of the galileo programme but they wish keen to express their disagreement with the joint approach and to approach <eos>\n",
      "tar:  the members of the <unk> delegation to the european parliament have supported mr <unk> s report in order to give their backing to the implementation of the galileo programme but they are anxious to show their disagreement with the mixed community and intergovernmental approach which has been retained <eos>\n",
      "pred: mr president commissioner ladies and gentlemen at the request of the council the european commission has drawn a communication on the need to develop a development development of a coherent space strategy in collaboration with the european space agency <eos>\n",
      "tar:  mr president commissioner ladies and gentlemen at the request of the council the european commission has produced a communication on the need to undertake the rapid development of a consistent space strategy in conjunction with the european space agency <eos>\n",
      "epoch:1  Step:10000  loss:1.1790916920  WER:0.2699619772\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates many jobs jobs in the processing industry or in the yards sector <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:125  loss:1.4309992266  WER:0.3547198210\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 4.809218436873748e-07\n",
      "pred: if if you could a preparatory in preparatory preparatory or group or an annual meeting of one of these <unk> if if whether <unk> of by the delegations and the resources which to the community delegation would would immediately create the institution <eos>\n",
      "tar:  commissioner if you attended a meeting either a preparatory working meeting or an annual meeting of one of these <unk> and saw the resources deployed by other delegations and the resources available to the community delegation you would immediately give the order to create such a unit <eos>\n",
      "pred: the european pharmaceutical industry is too lacking in terms and focusing on neglected diseases could be fresh with new new impetus <eos>\n",
      "tar:  the european pharmaceutical industry is too lacking in innovation and focusing on neglected diseases could provide it with a new impetus <eos>\n",
      "epoch:1  Step:12000  loss:1.5160669088  WER:0.3711340206\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore also also generates many jobs jobs in the processing industry or in the <unk> <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:150  loss:1.4301537180  WER:0.3544266916\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 5.610821643286574e-07\n",
      "pred: thirdly europe must become more efficient and comprehensible and of responding responding the attacks from by influential forces which in my country too use europe as a scapegoat for their failures <eos>\n",
      "tar:  thirdly europe must become more effective and coherent capable of vigorously <unk> the attacks launched by eurosceptic groupings which in my country too use europe as a scapegoat for domestic failures <eos>\n",
      "pred: it should and the to happen on and and right right and they members congress s have a bill to bill <unk> is right right we have to separate the sanctions from those current ones currently currently currently in force <eos>\n",
      "tar:  we cannot allow this to go on it is not right and the us <unk> who sent the letter to bill clinton are quite right: we have to <unk> economic sanctions from the military sanctions that are currently in place <eos>\n",
      "epoch:1  Step:14000  loss:1.3552424908  WER:0.3232142857\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore also also generates many jobs jobs in the processing industry or in the shipyards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:175  loss:1.4252601528  WER:0.3488702591\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 6.412424849699399e-07\n",
      "pred: mr you mr president i too would like to congratulate my thanks of thanks to karl-heinz florenz florenz the rapporteur and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and\n",
      "tar:  thank you mr president i too should like to add my words of praise to mr karl-heinz florenz the rapporteur <eos>\n",
      "pred: in could defend defend their interests more forcefully <eos>\n",
      "tar:  they could thus defend their interests more forcefully <eos>\n",
      "epoch:1  Step:16000  loss:1.5316053629  WER:0.4288389513\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore also also generates many jobs jobs on the processing industry or in the <unk> <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:200  loss:1.4178927374  WER:0.3571597659\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 7.214028056112226e-07\n",
      "pred: you gahler has the floor <eos>\n",
      "tar:  mr gahler has the floor <eos>\n",
      "pred: mr president the about the effects of gmos are the minds in reflected the minds of many of us overshadowed by doubts about their implications on other species of plants or animals and and the the control of the world seed bank which a <unk> of <unk> <eos>\n",
      "tar:  mr president concerns about the effects of gmos on human health are in the minds of many of us overshadowed by doubts about their effects upon other species of plants or animal life and about the control of the world's seed bank by a handful of companies <eos>\n",
      "epoch:1  Step:18000  loss:1.4496059418  WER:0.3293650794\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also generates many jobs jobs in the processing industry or in the korean <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:225  loss:1.4209692335  WER:0.3528266174\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 8.01563126252505e-07\n",
      "pred: it is is no longer be levelled at us at <eos>\n",
      "tar:  this criticism can no longer be levelled at us <eos>\n",
      "pred: the money-laundering directive will a number of obligations on financial institutions financial institutions which the result that they simply are simply to to new <unk> and the need to extend the scope of the directive to a professional <eos>\n",
      "tar:  the money-laundering directive imposes a string of obligations on credit and financial institutions with the result that money <unk> have simply switched to other sectors hence the need to extend the scope of the directive to other professions <eos>\n",
      "epoch:1  Step:20000  loss:1.5231338739  WER:0.3502538071\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also generates many jobs jobs in the processing industry and in the yards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:250  loss:1.4144612169  WER:0.3456039675\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 8.817234468937877e-07\n",
      "pred: <unk> <unk> <unk> a a member of the ecb central bank governing board has a german newspaper that we was to be statistics in to the euro area <eos>\n",
      "tar:  mr <unk> <unk> <unk> a member of the european central bank's executive board told a german newspaper that there had to be statistics relevant to the euro area <eos>\n",
      "pred: while the statistics are available to our and national institutions the is no no enough satisfactory statistics available the social partners <eos>\n",
      "tar:  while good statistics are available in parliaments and national institutions there are still not any satisfactory surveys of the social partners <eos>\n",
      "epoch:1  Step:22000  loss:1.5660638809  WER:0.3659574468\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore also also generates many jobs jobs in the processing industry or in the korean <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:275  loss:1.4115078354  WER:0.3529857318\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 9.618837675350702e-07\n",
      "pred: the vote will take place today <unk> <unk> <eos>\n",
      "tar:  the vote will take place at <unk> <unk> <eos>\n",
      "pred: i a of the council's s commitment i members member of parliament i remember aware that the fact that a representative of the high representative mr solana has participated in and with a deputy from the nato of nato - in the negotiations which led 13 march 2001 led in the signing and albanian representatives signing an cease-fire and and the initiate cease-fire between initiated between the albanian authorities and the representatives representatives community in 23 march <eos>\n",
      "tar:  as confirmation of the council' s commitment the honourable members of parliament should be aware of the fact that a representative of the high representative javier solana actively participated - together with a representative of the secretary-general of nato - in the negotiations which on 13 march 2001 resulted in the serbian and albanian representatives signing the cease-fire agreement and to a dialogue being started between the <unk> authorities and the <unk> albanian representatives on 23 march <eos>\n",
      "epoch:1  Step:24000  loss:1.3560187817  WER:0.4788069074\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also generates many jobs jobs on the processing industry or in the <unk> <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:300  loss:1.4070467138  WER:0.3472936341\n",
      "Train\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 1.0420440881763529e-06\n",
      "pred: mr president i am speaking today on behalf of mrs corbey and would like to make five comments <eos>\n",
      "tar:  mr president i am speaking today on behalf of mrs corbey and should like to express five comments <eos>\n",
      "pred: apart these criticisms i i would like to thank the italian presidency of the council for having <unk> to to the convention’s text in the the months and weeks and for the keeping the commitment it had in this of this <eos>\n",
      "tar:  despite these criticisms though i would like to thank the italian presidency of the council for having kept close to the convention’s text throughout all these weeks and months and for actually keeping the commitment it made in respect of it <eos>\n",
      "epoch:2  Step:26000  loss:1.4043987989  WER:0.3309143687\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also generates many jobs jobs in the processing industry or in the yards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:325  loss:1.4025949574  WER:0.3523296699\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 1.1222044088176353e-06\n",
      "pred: the palestinian authorities must provide clear and firm support for the efforts government efforts to reorganise the security forces and must restore public order <eos>\n",
      "tar:  the palestinian authorities must secure clear and firm support for the new government’s efforts to reorganise the security forces they must restore public order <eos>\n",
      "pred: at least 75% of all deaths deaths and injuries could and should be saved through reproductive health care <eos>\n",
      "tar:  at least 75% of all maternal deaths and injuries could and should be prevented through reproductive health care <eos>\n",
      "epoch:2  Step:28000  loss:1.1599198580  WER:0.3291139241\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: yes there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also generates many jobs jobs in the processing industry or in the yards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:350  loss:1.4053022099  WER:0.3483508416\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 1.202364729458918e-06\n",
      "pred: the fact complaints about citizens who problems with the institutions are essentially due blame with the much as to left and even <eos>\n",
      "tar:  the many complaints from citizens having problems with eu institutions are essentially to do with so much still being withheld or suppressed <eos>\n",
      "pred: these procedures and tested procedures will result for further <unk> in the consultation and consultation of workers including the other things the ability of the <unk> and technological development investment well as long-term investment in human and the <unk> of employability through training training of training and training the alternative options to redundancies and redundancies and whenever possible can possible the <unk> within the workers of workers affected by restructuring measures <eos>\n",
      "tar:  these tried and tested procedures will make for further advances in the information and consultation of workers including among other things the anticipation of market trends or technological developments as well as long-term investment in people and the encouragement of employability by the use of education and training careers advice alternatives to closure and redundancies and wherever this is possible the redeployment within the enterprise of workers affected by restructuring measures <eos>\n",
      "epoch:2  Step:30000  loss:1.2706307173  WER:0.2826797386\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: yes there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also generates many jobs jobs in the processing industry or in the yards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:375  loss:1.3989010954  WER:0.3457974218\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 1.2825250501002004e-06\n",
      "pred: amendments nos 10 16 and 17 cannot be accepted either <eos>\n",
      "tar:  amendments nos 10 16 and 17 cannot be retained either <eos>\n",
      "pred: european democracy and the democracy have certainly have any lessons from any from certainly not from a belgian government which has corrupt politicians financed by arms dealers the the of the european of one a corrupt party condemned is condemned by the courts of justice death of european commissioner <eos>\n",
      "tar:  european democracy and austrian democracy should not receive any lessons from anybody and certainly not from a belgian government that embraces corrupt parties financed by arms dealers and that appointed the leader of such a corrupt party which was condemned by the court to the post of european commissioner <eos>\n",
      "epoch:2  Step:32000  loss:1.3717364073  WER:0.3493282150\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also generates many jobs jobs in the processing industry or in the yards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:400  loss:1.3979971504  WER:0.3473790914\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 1.362685370741483e-06\n",
      "pred: i regret sorry to note that after the members of the group people's s party have for the motion in the it have changed changed their minds and are that they s opinion is not be be excluded as consideration but same of certainly not binding but it is certainly useful useful and it is be absurd to to do it of it <eos>\n",
      "tar:  i am sad to see that although the members of the european people' s party voted for the motion in committee they have now changed their minds and feel that <unk> s opinion should not now be taken into consideration the opinion is certainly not binding but it is certainly very useful and it would be absurd not to make use of it <eos>\n",
      "pred: replacing replacement reduction and refinement of animal tests are a question of civilisation <eos>\n",
      "tar:  the replacement reduction and refinement of animal tests are a question of civilisation <eos>\n",
      "epoch:2  Step:34000  loss:1.3337981701  WER:0.3102766798\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also generates many jobs jobs in the processing industry or in the yards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:425  loss:1.3963998270  WER:0.3424692932\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 1.4428456913827657e-06\n",
      "pred: hunting hunting must these species must bring significant and tangible benefits benefits for the local and local the local population who live in same and them have from a result of the species in <eos>\n",
      "tar:  <unk> hunting of these species must generate significant and tangible conservation benefits for the species and for the local people who share the area with or suffer as a result of the species concerned <eos>\n",
      "pred: perhaps parliament - and you mr president - could take the initiative and this and suggesting out to the council that it is a example example of the it make people involved involved in the activities and and and and and and and and <eos>\n",
      "tar:  perhaps parliament - and you mr president - could take the initiative on this by pointing out to the council that this is an important example of how to get citizens closely involved in the union <eos>\n",
      "epoch:2  Step:36000  loss:1.4484183788  WER:0.3560311284\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also generates many jobs jobs in the processing industry or in the yards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:450  loss:1.3936255836  WER:0.3504845104\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 1.5230060120240484e-06\n",
      "pred: we know that poverty immigration is the cause of poverty and poor <eos>\n",
      "tar:  we know that illegal immigration is the result of poverty and underdevelopment <eos>\n",
      "pred: why why the ecb central bank has increased interest lowered interest interest rates in line with american interest rates in financing bid to fund the <eos>\n",
      "tar:  so far the european central bank has raised or lowered its interest rates in line with us interest rates in a bid to fund speculation <eos>\n",
      "epoch:2  Step:38000  loss:1.5210635662  WER:0.3493282150\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore also also generates many jobs jobs in the processing industry or in the shipyards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:475  loss:1.3889453506  WER:0.3480123160\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 1.6031663326653306e-06\n",
      "pred: mrs schreyer you have just said explicitly that the commission is not be in the way <eos>\n",
      "tar:  ms schreyer you have just stated clearly that the commission would not proceed in this way today <eos>\n",
      "pred: in this case however is also also about question of establishing effective contacts between europe european and the united states so that the and and <unk> can be and develop <eos>\n",
      "tar:  in this case it is however also a question of creating efficient contacts between the eu and the united states so that business trade and aviation can thrive and develop <eos>\n",
      "epoch:2  Step:40000  loss:1.5281831026  WER:0.3571428571\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also generates many jobs jobs in the processing industry or in the shipyards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:500  loss:1.3884929276  WER:0.3374272692\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 1.6833266533066133e-06\n",
      "pred: it is be said enough that this proposal is important especially the other - something this is often overlooked - for developing countries must that sufficient resources must be made available for <eos>\n",
      "tar:  it cannot be emphasised enough that the proposal is important for among others - and this is sometimes overlooked - the developing countries and that sufficient resources must be made available <eos>\n",
      "pred: at the same time and of the flight crew and well of and other measures are of which are part in the safety are to the to be regulated by law as which which we have have our request and the in committee and at the for second reading <eos>\n",
      "tar:  at the same time supervision of the flight crew as also maintenance and other measures all of which are factors in aviation safety are in future to be regulated by law something for which we clearly expressed a desire during discussions in committee and during preparations for second reading <eos>\n",
      "epoch:2  Step:42000  loss:1.3778390884  WER:0.4160583942\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also generates many jobs jobs in the processing industry or in the <unk> <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:525  loss:1.3886967754  WER:0.3423093040\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 1.763486973947896e-06\n",
      "pred: the states have been a large extent been to protect with the whole the whole of society - including society including well as the course the animal welfare - has not been protected <eos>\n",
      "tar:  the states have to a large degree failed to comply with the directive; the whole of society - human society as well as of course the animal kingdom - has not felt protected <eos>\n",
      "pred: the the report on the reform of the meda programme is not go the wrong course in many number of counts <eos>\n",
      "tar:  however the report on the reform of the meda programme does not set the right tone on a number of counts <eos>\n",
      "epoch:2  Step:44000  loss:1.4685573578  WER:0.3525773196\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also generates many jobs jobs in the processing industry and in the yards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:550  loss:1.3877741766  WER:0.3391215335\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 1.8436472945891784e-06\n",
      "pred: the the many community directives on the environment do to provide adequate protection because of the to implement and and because failure to comply with effective <eos>\n",
      "tar:  even the many community directives on the environment fail to provide sufficient protection because of failure to apply them and frequent failure to comply with them encouraged by the lack of effective penalties <eos>\n",
      "pred: according to the external evaluation that has been presented to in <unk> it had a significant multiplier effect with jobs and <unk> ventures <eos>\n",
      "tar:  according to the external evaluation it has been subjected to the <unk> has demonstrated a significant multiplier effect creating jobs and joint ventures <eos>\n",
      "epoch:2  Step:46000  loss:1.3482475281  WER:0.4229508197\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also generates many jobs jobs in the processing industry or in the <unk> <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:575  loss:1.3830664110  WER:0.3430183063\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 1.923807615230461e-06\n",
      "pred: i am pleased that mr solana mr patten and mr straw have been to the <unk> not i am not saying that debate parliament debate would be a but we are to be ignoring this issue <eos>\n",
      "tar:  i am glad that mr solana mr patten and mr straw have gone to the region and i am not saying a european parliament debate would be decisive but we seem to be ignoring the issue <eos>\n",
      "pred: that said that it must not underestimate eures importance of eures <eos>\n",
      "tar:  having said this we must not overestimate the value of eures <eos>\n",
      "epoch:2  Step:48000  loss:1.7835747004  WER:0.3830570902\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also generates many jobs jobs in the processing industry or in the yards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:600  loss:1.3796756935  WER:0.3408979916\n",
      "Train\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 2.003967935871744e-06\n",
      "pred: the fourth stage the guidelines have been approved is the draw up national action plans and the joint report on employment by <unk> year <eos>\n",
      "tar:  the fourth once these guidelines have been approved is to draw up national action plans and the joint report on employment by the autumn <eos>\n",
      "pred: mr president in 1989 i witnessed in first <unk> that central and eastern europe <eos>\n",
      "tar:  mr president in 1989 i witnessed the political upheaval in central and eastern europe in person <eos>\n",
      "epoch:3  Step:50000  loss:1.2720522881  WER:0.4166666667\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also generates many jobs jobs in the processing industry and in the yards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:625  loss:1.3796488142  WER:0.3467557290\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 2.0841282565130264e-06\n",
      "pred: all citizens will have entitled to medical from the when they medical care in their own in a member state and they be subjecting red bureaucratic process that now exists many various of many bodies bodies <eos>\n",
      "tar:  all citizens will be entitled to benefits in kind when seeking medical care during their stay in a member state and will avoid the complicated bureaucratic process that currently involves the intervention of various different bodies <eos>\n",
      "pred: in document is a impetus to the nuclear industry when some the same time some commission s estimates suggest a progressive reduction in the share of the consumption supplied by the power from from the current 15% to <unk> in 2030 <eos>\n",
      "tar:  this document gives fresh encouragement to the nuclear industry while at the same time the commission' s estimates suggest a progressive reduction of the proportion of energy consumption supplied by nuclear power down from the current 15% to 6% by 2030 <eos>\n",
      "epoch:3  Step:52000  loss:1.4215787649  WER:0.3355932203\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also generates many jobs jobs in the processing industry or in the yards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:650  loss:1.3769669867  WER:0.3463876043\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 2.164288577154309e-06\n",
      "pred: before the a for his future jean-claude trichet was for governor of the french bank france governor for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for\n",
      "tar:  before being nominated for this position jean-claude trichet was the governor of the <unk> de france <eos>\n",
      "pred: i are two considerations <eos>\n",
      "tar:  there are two conclusions to be drawn from this <eos>\n",
      "epoch:3  Step:54000  loss:1.1347446442  WER:0.4131205674\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also generates many land-based jobs in the processing industry and in the yards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:675  loss:1.3791758490  WER:0.3433548871\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 2.2444488977955912e-06\n",
      "pred: i hope you will a rest of rest that is deserve deserve <eos>\n",
      "tar:  i hope you have the relaxation and rest that we all deserve <eos>\n",
      "pred: what is contrast to the proposals of proposals states' proposals on <eos>\n",
      "tar:  what a contrast to the series of members states' proposals <eos>\n",
      "epoch:3  Step:56000  loss:1.2007241249  WER:0.2918215613\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also generates many jobs jobs in the processing industry or in the yards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:700  loss:1.3771496916  WER:0.3423019449\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 2.324609218436874e-06\n",
      "pred: today days the increasing number of refugees is the is from the third world and we are less <unk> <eos>\n",
      "tar:  these days an increasing proportion of refugees in europe hail from the developing world and we are less <unk> <eos>\n",
      "pred: question no 35 by <unk> <eos>\n",
      "tar:  question no 35 by <unk> <eos>\n",
      "epoch:3  Step:58000  loss:1.3046755791  WER:0.3087649402\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also generates many jobs jobs in the processing industry or in the yards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:725  loss:1.3783553267  WER:0.3433152933\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 2.4047695390781566e-06\n",
      "pred: proposal for a decision <unk> of by the bureau in to annex <unk> <unk> annex vii of the rules of procedure on the implementation of the interinstitutional agreement on the parliament access to sensitive information information in the field of security and defence policy <eos>\n",
      "tar:  proposal for a decision <unk> tabled by the bureau pursuant to point 1 of annex vii to the rules of procedure on the implementation of the interinstitutional agreement governing european parliament access to sensitive council information in the sphere of security and defence policy <eos>\n",
      "pred: we must to be given and lesson and we <eos>\n",
      "tar:  iraq has to be taught a lesson and <unk> <eos>\n",
      "epoch:3  Step:60000  loss:1.2421849966  WER:0.2812500000\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also generates many jobs jobs in the processing industry or in the yards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:750  loss:1.3814137030  WER:0.3396436313\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 2.484929859719439e-06\n",
      "pred: i i would like to start my first to mr comment to at mr <eos>\n",
      "tar:  however i would like to <unk> my remarks with a comment directed at you mr <unk> <eos>\n",
      "pred: as an austrian i cannot accept a report which is a to my country austria an way <unk> way unwarranted compelling way and which is supports and supports a action action by <unk> action of by fourteen 14 member <eos>\n",
      "tar:  as an austrian i cannot accept a report which takes exception to my country in a most unqualified and most unjustified manner and which expressly welcomes and supports <unk> motivated action and unjustified boycotting measures by the fourteen <eos>\n",
      "epoch:3  Step:62000  loss:1.4774484634  WER:0.3385214008\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also generates many jobs jobs in the processing industry or in the yards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:775  loss:1.3756731081  WER:0.3428139778\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 2.5650901803607214e-06\n",
      "pred: the our view the reports on the first guidelines for the 2002 budget of the european institutions seem to be based or at on at essential areas <eos>\n",
      "tar:  in our view the reports on the main guidelines for the 2002 budget of the european institutions seem to be incomplete or vague in three main areas <eos>\n",
      "pred: my specific question commissioner you commissioner is this: the commission be a <unk> or this time or not? <eos>\n",
      "tar:  my specific question to you commissioner is would the commission advocate sporting bans at this time or not? <eos>\n",
      "epoch:3  Step:64000  loss:1.2909858227  WER:0.3056910569\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore also also generates many jobs jobs in the processing industry or in the yards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:800  loss:1.3729752827  WER:0.3397939181\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 2.645250501002004e-06\n",
      "pred: in bush will also to gothenburg <eos>\n",
      "tar:  president bush is coming to gothenburg <eos>\n",
      "pred: the equipment often is often used for these operations must be used with the in in the event of disagreement disagreement between the ship <unk> responsible ship workers and the port the the the competent authority must be able to intervene in prevent operations operations in order to prevent the damage to the integrity of the ships carriers <eos>\n",
      "tar:  heavy equipment which is often used in these operations must be handled with caution and in the event of a disagreement between the two parties the dock personnel and the crew on board the competent authority must be able to intervene to suspend the operations in order to prevent possible damage to the structure of the bulk carriers <eos>\n",
      "epoch:3  Step:66000  loss:1.2829957008  WER:0.3318077803\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also generates many jobs jobs in the processing industry and in the yards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:825  loss:1.3737983704  WER:0.3389128933\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 2.7254108216432868e-06\n",
      "pred: mr president i will begin with zimbabwe <eos>\n",
      "tar:  mr president i shall start with zimbabwe <eos>\n",
      "pred: i will answer answer your question mr martin <eos>\n",
      "tar:  i shall simply answer your question mr martin <eos>\n",
      "epoch:3  Step:68000  loss:1.4415142536  WER:0.3585313175\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also generates many jobs jobs in the processing industry and in the yards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:850  loss:1.3721693420  WER:0.3389215718\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 2.805571142284569e-06\n",
      "pred: the problem is that outstanding commitments this <unk> is is still too wide between commitments and payments in regards the the structural funds and other areas as <eos>\n",
      "tar:  the problem is the outstanding commitments this gap that is still too wide between commitments and payments as regards both the structural funds and other fields too <eos>\n",
      "pred: i am pleased to hear commissioner fischler recommend us fischler that more research is be research way into research because we disaster like as we one we have seen in great britain is be be allowed to happen again <eos>\n",
      "tar:  i am glad to hear commissioner fischler tell mr byrne that more money should find its way into research as a calamity such as the one we have witnessed in great britain must not be allowed to happen again <eos>\n",
      "epoch:3  Step:70000  loss:1.4832935333  WER:0.3592400691\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also generates many jobs jobs in the processing industry or in the yards industries <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:875  loss:1.3729238272  WER:0.3414130248\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 2.885731462925852e-06\n",
      "pred: this combined combined conjunction with the presence which i hope will be from of the european for foreign affairs in the european will provide further more opportunities <eos>\n",
      "tar:  this possibility in conjunction with the presence which i hope will come about of a minister of foreign affairs at the un will provide many more possibilities <eos>\n",
      "pred: mr president i my opinion mrs hautala has also again drawn up a very good report which has received a support in the committee on the environment public health and consumer policy <eos>\n",
      "tar:  mr president in my view mrs hautala has once again drawn up a very important report which has received much support in the committee on the environment public health and consumer policy <eos>\n",
      "epoch:3  Step:72000  loss:1.4186592102  WER:0.3465783664\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also generates many land-based jobs in the processing industry or in the korean <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:900  loss:1.3712830162  WER:0.3405505332\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 2.9658917835671345e-06\n",
      "pred: mr president human rights are an issue on spanish socialists cannot not sensitive to and the the time when most the most of men and women in the member of the european union we rights are one basis basis of their system of coexistence we and have decades live on decades of struggle work and suffering before we a and dignified conditions rights in in our society <eos>\n",
      "tar:  mr president human rights are an issue we spanish socialists are particularly sensitive to since at a time when for the majority of men and women in the countries of the european union human rights formed the main foundation for their system of coexistence spain still had to go through decades of hard work and suffering before achieving acceptable and dignified human rights conditions in our society <eos>\n",
      "pred: even no one can object to international scientific cooperation on equal terms we cannot under any circumstances allow research to be and under controlled by the financial groups <eos>\n",
      "tar:  although no one could object to international scientific cooperation on equal terms we cannot under any circumstances allow research to serve and be controlled by major financial interests <eos>\n",
      "epoch:3  Step:74000  loss:1.3808308840  WER:0.3202846975\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also generates many jobs jobs in the processing industry and in the yards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:925  loss:1.3719732761  WER:0.3408597775\n",
      "Train\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 3.0460521042084174e-06\n",
      "pred: we can accept 29 full 29 amendments <eos>\n",
      "tar:  we can accept in full 29 amendments <eos>\n",
      "pred: mr president i voted voted for the <unk> report because it is certainly right to be restraint certain amount of caution in products products which the market which could be harmful to us all <eos>\n",
      "tar:  mr president i also voted for the <unk> report because it is certainly right to exercise a certain amount of caution when placing products on the market which could be harmful to us all <eos>\n",
      "epoch:4  Step:76000  loss:1.3226982355  WER:0.3232682060\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also generates many jobs jobs in the processing industry or in the yards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:950  loss:1.3677634525  WER:0.3360124851\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 3.1262124248496994e-06\n",
      "pred: it it for the first time in history history the cambodian of cambodia had the opportunity to vote for local representatives and responded responded out en masse to the to do so <eos>\n",
      "tar:  but clearly for the first time in its history the people of cambodia had the opportunity to vote for local councillors and they turned out en masse in order to do so <eos>\n",
      "pred: the next item is the debate <unk> by mr brok on behalf of the committee on foreign affairs human rights common security and defence policy on the proposal for a council decision providing financial financial financial assistance to kosovo (com(1999) - <unk> - <unk> <eos>\n",
      "tar:  the next item is the report <unk> by mr brok on behalf of the committee on foreign affairs human rights common security and defence policy on the proposal for a council decision providing exceptional community financial assistance to kosovo <unk> - <unk> - <unk> <eos>\n",
      "epoch:4  Step:78000  loss:1.5473250151  WER:0.3893129771\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also generates many jobs jobs in the processing industry and in the yards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:975  loss:1.3689758635  WER:0.3394030900\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 3.2063727454909823e-06\n",
      "pred: that lies the danger facing europe <eos>\n",
      "tar:  therein lies the danger facing europe <eos>\n",
      "pred: as i already been mentioned in in month i organised a lunch with with brussels with the <unk> and understanding of and employment colleagues members of the committee on employment and social affairs to share the the the practice practice and the benefits of flexibility because that is what case <eos>\n",
      "tar:  as has already been mentioned earlier this month i hosted a lunch debate in brussels with the involvement and participation association for my fellow members of the committee on employment and social affairs to share with them uk best practice and the benefits of flexibility because that is the issue <eos>\n",
      "epoch:4  Step:80000  loss:1.3756483793  WER:0.3314814815\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also generates many land-based jobs in the processing industry and in the yards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:1000  loss:1.3685699940  WER:0.3377957153\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 3.2865330661322647e-06\n",
      "pred: as mr garriga polledo said this are political aspects to this budget of this budget and it is not just a accounting exercise <eos>\n",
      "tar:  as mr garriga polledo said there are political aspects to a lot of this budget and it is not just an accounting exercise <eos>\n",
      "pred: that is the spirit in which the delegation delegation must start talks talks <eos>\n",
      "tar:  this is the spirit in which the planned delegation should conduct its talks <eos>\n",
      "epoch:4  Step:82000  loss:1.3028259277  WER:0.6233552632\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore also also generates many jobs jobs in the processing industry or in the yards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:1025  loss:1.3661755800  WER:0.3408614900\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 3.3666933867735476e-06\n",
      "pred: parliament should be be kept abreast informed of the made building construction construction in decommissioning of the site and the completion of the two reactors <eos>\n",
      "tar:  parliament should also be kept fully informed of progress in the shelter construction the decommissioning of the site and the completion of the two reactors <eos>\n",
      "pred: my second question is addressed to commissioner verheugen who is not a is somewhat bizarre as usually usually <unk> by our our debates <eos>\n",
      "tar:  my second question is addressed to mr verheugen who is absent which is somewhat unusual he is generally <unk> in attending our sessions <eos>\n",
      "epoch:4  Step:84000  loss:1.3959182501  WER:0.3366733467\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore also also generates many land-based jobs in the processing industry and in the korean <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:1050  loss:1.3714725351  WER:0.3404659465\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 3.44685370741483e-06\n",
      "pred: the no second of an requirement for public access access to the member states' documentation on the location of locations of which they name samples <eos>\n",
      "tar:  amendment 3 (2) contains a demand for free public access to the member states' documentation concerning the selection of places from which to take samples <eos>\n",
      "pred: in my own country for example agreements agreements were to be read behind secret closed where of secrecy was closed secret one <eos>\n",
      "tar:  in my own country for example the agreements had to be read in a room because the agreement was a secret one <eos>\n",
      "epoch:4  Step:86000  loss:1.5174632072  WER:0.3665943601\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also generates many land-based jobs in the processing industry or in the yards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:1075  loss:1.3635299110  WER:0.3406785790\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 3.527014028056113e-06\n",
      "pred: unfortunately the brok report is <unk> by the <unk> of military intervention as a only way to achieving peace peace <eos>\n",
      "tar:  unfortunately the brok report is permeated by traditional <unk> in military interventions as the only way of bringing about peace <eos>\n",
      "pred: mr president commissioner promised yourself you were in commissioner kinnock that the commission would be the best s best administration <eos>\n",
      "tar:  mr president you promised when you were appointed commissioner kinnock that the commission would become the world' s best administration <eos>\n",
      "epoch:4  Step:88000  loss:1.1359293461  WER:0.2815734990\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also generates many jobs jobs in the processing industry or in the yards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:1100  loss:1.3749744606  WER:0.3423389636\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 3.6071743486973953e-06\n",
      "pred: it will allows the securities and insurance regulators to exchange information information with third country authorities on as those and settlement systems with auditors in contribute making of their financial contribute to make the financial of the financial system <eos>\n",
      "tar:  it also allows european securities and insurance regulators to exchange supervisory information with third country bodies such as clearing and settlement systems auditors etc which by virtue of their functions help to strengthen the stability of the financial system <eos>\n",
      "pred: its implementation would make all those who earn a living from the services; it would increase the risk of accidents and pollution <eos>\n",
      "tar:  its enforcement would threaten all those that earn a living from port services; it would increase the risk of accidents and pollution <eos>\n",
      "epoch:4  Step:90000  loss:1.1798107624  WER:0.2709551657\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also generates many land-based jobs in the processing industry or in the yards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:1125  loss:1.3660288429  WER:0.3399201278\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 3.6873346693386774e-06\n",
      "pred: in this same way conflict conflict prevention is not being about creating creation of a police force it requires the three pillars <eos>\n",
      "tar:  in the same way preventive conflict management is not just about the deployment of a police force; it involves all three pillars <eos>\n",
      "pred: if we impose a <unk> of a exclusively <unk> system for much bananas will going to taste <unk> bitter <eos>\n",
      "tar:  if we impose the brutality of an exclusively <unk> regime too rapidly bananas are going to taste incredibly bitter to thousands of small african producers <eos>\n",
      "epoch:4  Step:92000  loss:1.4962586164  WER:0.3481624758\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also generates many jobs jobs in the processing industry or in the yards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:1150  loss:1.3698312187  WER:0.3382094523\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 3.7674949899799602e-06\n",
      "pred: here have here in strasbourg with the schengen schengen information system the more comprehensive european of europe which is be financed from our budget <eos>\n",
      "tar:  we have here in strasbourg with the central schengen computer system the most comprehensive <unk> in europe it must be financed from our budget <eos>\n",
      "pred: in (fr) in west africa where the the unrestricted movement of arms dealers capital and all types of trafficking the destabilising of destabilisation are <unk> and are become and only responsible decision we can support is to support legitimate legitimate and and the state authorities which can can constitute a blockade against the <unk> and the and disorder <eos>\n",
      "tar:  - (fr) in west africa where with its free movement of arms mercenaries capital and all types of trafficking the factors for destabilisation are myriad and easily manipulated the only responsible decision we can take is to back the legal authorities and the state machinery which alone can constitute a bulwark against the agents of anarchy and disorder <eos>\n",
      "epoch:4  Step:94000  loss:1.6023578644  WER:0.3800322061\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also generates many jobs jobs in the processing industry and in the yards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:1175  loss:1.3692102957  WER:0.3355637030\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 3.847655310621243e-06\n",
      "pred: mr president on behalf of the british meps of the house i i thank you president of parliament for you for your condolences of sympathy and for the message of my colleagues and of all in the parties in my country who this house who the has happened in istanbul <eos>\n",
      "tar:  mr president on behalf of the british members of this house may i thank the president of parliament through you for his words of sympathy and extend the sympathy of my colleagues and of colleagues across all parties from my country in this house at what has happened in istanbul <eos>\n",
      "pred: <eos>\n",
      "tar:  <eos>\n",
      "epoch:4  Step:96000  loss:1.4278545380  WER:0.3866877971\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore also also generates many jobs jobs in the processing industry or in the yards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:1200  loss:1.3671208906  WER:0.3418513160\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 3.9278156312625255e-06\n",
      "pred: mr president mr president-in-office of the council mr president of the commission commissioner ladies and gentlemen seldom has we be been any issue and a solution on the which which the council the the council the council the ministers as a whole the commission and parliament have parliament have act acted with such unanimity and common will <eos>\n",
      "tar:  mr president mr president-in-office of the council mr president of the commission commissioner ladies and gentlemen seldom can there have been an issue and a resolution of problems on which the presidency of the council the council of ministers as a whole the commission and we in parliament can have acted with such unanimity and common purpose <eos>\n",
      "pred: if we leave the symbolic deadline of 1 may or 9 may or the most more important one which belongs to for own 13 june the be the then draft constitution will not be far far removed what i initially the the beginning of my speech <eos>\n",
      "tar:  if we allow the symbolic deadline of 1 may or 9 may or the politically more significant one that is primarily your own 13 june to pass by the draft constitution will not be very far from what i called at the beginning of my speech <unk> <unk> <eos>\n",
      "epoch:4  Step:98000  loss:1.2878562212  WER:0.3115823817\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also generates many jobs jobs in the processing industry or in the yards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:1225  loss:1.3645969772  WER:0.3356076172\n",
      "Train\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 4.007975951903808e-06\n",
      "pred: i would like to ask the commissioner how this <unk> forum will to be organised <eos>\n",
      "tar:  i should like to ask the commissioner how this <unk> forum is to be organised <eos>\n",
      "pred: within the framework of the convention which as we well known is to for reform the treaties we will have only have able to discuss the competences might may have to give to the member states but also will also specify in areas in which we in need additional competences in european european level <eos>\n",
      "tar:  within the framework of the convention which as is well known is looking to <unk> the treaties we will not only be able to discuss what competences we might have to relocate in the member states but we must also specify the areas in which we additionally need primary competences at a european level <eos>\n",
      "epoch:5  Step:100000  loss:1.2469282150  WER:0.3022284123\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also generates many jobs jobs in the processing industry or in the yards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:5  Step:1250  loss:1.3656129217  WER:0.3393029029\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 74\u001b[39m\n\u001b[32m     71\u001b[39m dec_in = tar[:,:-\u001b[32m1\u001b[39m].to(device)\n\u001b[32m     72\u001b[39m targ = tar[:,\u001b[32m1\u001b[39m:].to(device)\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_in\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m loss = loss_fn(logits.transpose( \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m ), targ )\n\u001b[32m     77\u001b[39m optimizer.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, text, dec_input)\u001b[39m\n\u001b[32m     52\u001b[39m padding_mask_tgt = (dec_input == idx_list_en[\u001b[33m'\u001b[39m\u001b[33m<pad>\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     54\u001b[39m x = \u001b[38;5;28mself\u001b[39m.encoder( text, mask_src, padding_mask_src )\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m preds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdec_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask_src\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask_tgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_tgt\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m preds\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 99\u001b[39m, in \u001b[36mTransformerDecoder.forward\u001b[39m\u001b[34m(self, features, captions, memory_padding_mask, tgt_padding_mask, tgt_mask)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;66;03m#embeddings = self.dropout( embeddings )\u001b[39;00m\n\u001b[32m     94\u001b[39m \u001b[38;5;66;03m#embeddings = self.norm(embeddings)\u001b[39;00m\n\u001b[32m     95\u001b[39m \n\u001b[32m     96\u001b[39m \u001b[38;5;66;03m# Transformerデコーダでキャプション生成\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# 画像の特徴も入力する\u001b[39;00m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.decoder_layers:\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     embeddings = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcross_mask\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[38;5;66;03m# [バッチサイズ, 系列長, 埋め込み次元]\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# -> [バッチサイズ, 系列長, 辞書の次元]\u001b[39;00m\n\u001b[32m    104\u001b[39m preds = \u001b[38;5;28mself\u001b[39m.linear(embeddings)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mTransformerDecoderLayer.forward\u001b[39m\u001b[34m(self, x, y, self_mask, cross_mask)\u001b[39m\n\u001b[32m     56\u001b[39m x = \u001b[38;5;28mself\u001b[39m.norm3( x )\n\u001b[32m     57\u001b[39m x = \u001b[38;5;28mself\u001b[39m.fnn( x )\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m x = x + x2\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/dropout.py:70\u001b[39m, in \u001b[36mDropout.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/functional.py:1425\u001b[39m, in \u001b[36mdropout\u001b[39m\u001b[34m(input, p, training, inplace)\u001b[39m\n\u001b[32m   1422\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p < \u001b[32m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p > \u001b[32m1.0\u001b[39m:\n\u001b[32m   1423\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1424\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m-> \u001b[39m\u001b[32m1425\u001b[39m     _VF.dropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m(\u001b[38;5;28minput\u001b[39m, p, training)\n\u001b[32m   1426\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/_VF.py:27\u001b[39m, in \u001b[36mVFModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(name)\n\u001b[32m     25\u001b[39m     \u001b[38;5;28mself\u001b[39m.vf = torch._C._VariableFunctions\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mobject\u001b[39m:\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.vf, name)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "epoch_num = 100\n",
    "\n",
    "# WarmupとCosine Decayを行うスケジューラを利用\n",
    "num_global_steps = len( train_loader ) * epoch_num\n",
    "print( \"num_global_steps:\", num_global_steps )\n",
    "num_warmup_steps = num_global_steps * 0.1\n",
    "print( \"num_warmup_steps:\", num_warmup_steps )\n",
    "scheduler = get_linear_schedule_with_warmup( optimizer, num_warmup_steps, num_global_steps ) \n",
    "\n",
    "tr_print_coef = 2000\n",
    "val_print_coef = 100\n",
    "\n",
    "#PATH = \"./model_NTT_auto_curr3.pt\"\n",
    "#\n",
    "#if device != torch.device(\"cpu\"):\n",
    "#    checkpoint = torch.load(PATH)\n",
    "#else:\n",
    "#    checkpoint = torch.load(PATH, map_location=torch.device('cpu'))\n",
    "#\n",
    "#model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device( \"cpu\")\n",
    "\n",
    "## optimizerのstateを現在のdeviceに移す。これをしないと、保存前後でdeviceの不整合が起こる可能性がある。\n",
    "#for state in optimizer.state.values():\n",
    "#    for k, v in state.items():\n",
    "#        if isinstance(v, torch.Tensor):\n",
    "#            state[k] = v.to(device)\n",
    "##epoch = checkpoint['epoch']\n",
    "##loss = checkpoint['loss']\n",
    "\n",
    "history = {\"train_loss\": [], \"val_loss\": [], \"train_wer\": [], \"val_wer\": [] }\n",
    "\n",
    "n = 0\n",
    "train_loss = 0\n",
    "val_loss = 0\n",
    "\n",
    "f_train = open( \"train_it3.log\", mode=\"w\", encoding = \"UTF-8\" )\n",
    "f_val = open( \"val_it3.log\", mode=\"w\", encoding = \"UTF-8\" )\n",
    "\n",
    "tra_global_step = 0\n",
    "val_global_step = 0\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    \n",
    "    model.train()\n",
    "    print( \"Train\")\n",
    "    train_loss = 0\n",
    "    train_wer = 0\n",
    "    n = 0\n",
    "    for i, (src, src_len, tar, tar_len) in enumerate(train_loader):\n",
    "        #display = True\n",
    "\n",
    "        tra_global_step += 1\n",
    "        if tra_global_step % 2000 == 0:\n",
    "            display = True\n",
    "            train_loss = 0\n",
    "            train_wer = 0\n",
    "            n = 0\n",
    "        else:\n",
    "            display = False\n",
    "        if display:\n",
    "            print(\"\\n-----------------Train Mode-----------------\\n\")\n",
    "        if display:\n",
    "            print(\"lr:\", optimizer.param_groups[0][\"lr\"] )\n",
    "\n",
    "        src = src[:,:max(src_len)].to(device)\n",
    "        tar = tar[:,:max(tar_len)]\n",
    "        dec_in = tar[:,:-1].to(device)\n",
    "        targ = tar[:,1:].to(device)\n",
    "\n",
    "        logits = model( src, dec_in )\n",
    "        loss = loss_fn(logits.transpose( 1, 2 ), targ )\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        pre_label_id = torch.argmax( logits, dim = 2 )\n",
    "        predict = []\n",
    "        for pre in pre_label_id:\n",
    "            hypo = []\n",
    "            for m in pre:\n",
    "                hypo.append(token_list_en[m.item()])\n",
    "                if token_list_en[m.item()] == '<eos>':\n",
    "                    break            \n",
    "            predict.append( hypo )\n",
    "        target = []\n",
    "        for pre in targ:\n",
    "            reference = []\n",
    "            for m in pre:\n",
    "                reference.append(token_list_en[m.item()])\n",
    "                if token_list_en[m.item()] == '<eos>':\n",
    "                    break            \n",
    "            target.append( reference )\n",
    "        total_error = 0\n",
    "        total_token_length = 0\n",
    "        for i3, (pred, tar) in enumerate( zip(predict, target) ):\n",
    "            (error, substitute, \n",
    "                delete, insert, ref_length) = \\\n",
    "                levenshtein.calculate_error(pred,tar)\n",
    "            # 誤り文字数を累積する\n",
    "            total_error += error\n",
    "            # 文字の総数を累積する\n",
    "            total_token_length += ref_length  \n",
    "\n",
    "            if i3 < 2 and display:\n",
    "                print( \"pred:\", ' '.join(pred) )\n",
    "                print( \"tar: \", ' '.join(tar) )\n",
    "            \n",
    "        train_loss += loss.item()\n",
    "        train_wer += total_error / total_token_length\n",
    "        n += 1\n",
    "        history[\"train_loss\"].append( train_loss / n )\n",
    "        history[\"train_wer\"].append( train_wer / n )\n",
    "        #if i % tr_print_coef == tr_print_coef - 1:\n",
    "        if display:\n",
    "            print(f\"epoch:{epoch+1}  Step:{tra_global_step}  loss:{train_loss/n:.10f}  WER:{ train_wer / n:.10f}\")\n",
    "        if tra_global_step % 2000 == 0:\n",
    "            PATH = './model_NTT_auto_curr_it3.pt'\n",
    "            torch.save({'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss,},\n",
    "               PATH)\n",
    "            #with open(\"history_NTT_auto_curr_it3.pkl\", \"wb\") as f:\n",
    "            #    pickle.dump( history, f )\n",
    "\n",
    "        f_train.write( \"Epoch: \" + str(epoch) + \", Step: \" + str(tra_global_step) + \", Loss: \" + str(train_loss/n) + \", WER: \" + str(train_wer / n ) + \"\\n\" )\n",
    "        f_train.flush()\n",
    "        \n",
    "        display_taihi = display\n",
    "        #if tra_global_step % 100 == 0:\n",
    "        if display:\n",
    "            val_loss = 0\n",
    "            val_wer = 0\n",
    "            val_n = 0\n",
    "            print(\"\\n-----------------Validation Mode-----------------\\n\")\n",
    "\n",
    "            for i_val, (val_src, val_src_len, val_tar, val_tar_len) in enumerate(val_loader):\n",
    "                val_global_step += 1\n",
    "                if i_val == 0:\n",
    "                    display = True\n",
    "                else:\n",
    "                    display = False\n",
    "                val_src = val_src[:,:max(val_src_len)].to(device)\n",
    "                val_tar = val_tar[:,:max(val_tar_len)]\n",
    "                val_dec_in = val_tar[:,:-1].to(device)\n",
    "                val_targ = val_tar[:,1:].to(device)\n",
    "\n",
    "                logits = model( val_src, val_dec_in )\n",
    "                loss = loss_fn(logits.transpose( 1, 2 ), val_targ )\n",
    "\n",
    "                pre_label_id = torch.argmax( logits, dim = 2 )\n",
    "                predict = []\n",
    "                for pre in pre_label_id:\n",
    "                    hypo = []\n",
    "                    for m in pre:\n",
    "                        hypo.append(token_list_en[m.item()])\n",
    "                        if token_list_en[m.item()] == '<eos>':\n",
    "                            break            \n",
    "                    predict.append( hypo )\n",
    "                target = []\n",
    "                for pre in val_targ:\n",
    "                    reference = []\n",
    "                    for m in pre:\n",
    "                        reference.append(token_list_en[m.item()])\n",
    "                        if token_list_en[m.item()] == '<eos>':\n",
    "                            break            \n",
    "                    target.append( reference )\n",
    "                val_total_error = 0\n",
    "                # 文字の総数を累積する\n",
    "                val_total_token_length = 0\n",
    "                for i3, (pred, tar) in enumerate( zip(predict, target) ):\n",
    "                    (error, substitute, \n",
    "                        delete, insert, ref_length) = \\\n",
    "                        levenshtein.calculate_error(pred,tar)\n",
    "                    # 誤り文字数を累積する\n",
    "                    val_total_error += error\n",
    "                    # 文字の総数を累積する\n",
    "                    val_total_token_length += ref_length  \n",
    "\n",
    "                    if i3 < 2 and i_val == 0:\n",
    "                        print( \"pred:\", ' '.join(pred) )\n",
    "                        print( \"tar: \", ' '.join(tar) )\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_wer += val_total_error / val_total_token_length\n",
    "                val_n += 1\n",
    "                #print( \"val_wer:\", val_wer )\n",
    "                #print( \"val_n:\", val_n )\n",
    "                history[\"val_loss\"].append( val_loss / val_n )\n",
    "                history[\"val_wer\"].append( val_wer / val_n )\n",
    "            print(f\"epoch:{epoch+1}  Step:{val_global_step}  loss:{val_loss/val_n:.10f}  WER:{ val_wer / val_n:.10f}\")\n",
    "            f_val.write( \"Epoch: \" + str(epoch) + \", Step: \" + str(val_global_step)  + \", Loss: \" + str(val_loss/val_n) + \", WER: \" + str(val_wer / val_n ) + \"\\n\" ) \n",
    "            f_val.flush()\n",
    "        if tra_global_step % 2000 == 0:\n",
    "            with open(\"history_NTT_auto_curr_it3.pkl\", \"wb\") as f:\n",
    "                pickle.dump( history, f )\n",
    "        display = display_taihi\n",
    "\n",
    "f_train.close()\n",
    "f_val.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "65d4d9fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
