{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8bf21a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install janome\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "#from torchtext.vocab import vocab\n",
    "#import torchtext.transforms as T\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#from torchvision import transforms\n",
    "import numpy as np\n",
    "import math\n",
    "import janome\n",
    "from janome.tokenizer import Tokenizer\n",
    "#import spacy\n",
    "from collections import Counter\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import time\n",
    "#from torchtext.vocab import build_vocab_from_iterator\n",
    "import levenshtein\n",
    "import json\n",
    "import pickle\n",
    "from timm.scheduler import CosineLRScheduler\n",
    "import nltk\n",
    "from nltk import bleu_score\n",
    "from torch.nn.init import constant_, xavier_uniform_\n",
    "from torch.nn.parameter import Parameter\n",
    "from transformers import  get_linear_schedule_with_warmup\n",
    "from collections import OrderedDict\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "346dde95-1503-4243-9963-31dcdce0e2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device( \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d745f5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49528 20556\n"
     ]
    }
   ],
   "source": [
    "with open( \"corpus/id_to_word_s.pkl\", \"rb\" ) as f:\n",
    "    token_list = pickle.load(f)\n",
    "with open( \"corpus/id_to_word_t.pkl\", \"rb\" ) as f:\n",
    "    token_list_en = pickle.load(f)\n",
    "with open( \"corpus/word_to_id_s.pkl\", \"rb\" ) as f:\n",
    "    idx_list = pickle.load(f)\n",
    "with open( \"corpus/word_to_id_t.pkl\", \"rb\" ) as f:\n",
    "    idx_list_en = pickle.load(f)\n",
    "\n",
    "pad_idx_s = idx_list['<pad>']\n",
    "pad_idx_t = idx_list_en['<pad>']\n",
    "\n",
    "enc_vocab_size, dec_vocab_size = len(token_list), len(token_list_en)\n",
    "print(enc_vocab_size, dec_vocab_size)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86f1161a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<sos>', '<eos>', '<unk>', '<blank>', '<mask>', 'der']\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([ 0, 1,2,3,4,5,6 ])\n",
    "\n",
    "#print( token_list[2] )\n",
    "\n",
    "#n = 0\n",
    "\n",
    "#ii = [ i for i in a ]\n",
    "\n",
    "#print( ii )\n",
    "\n",
    "b = [ token_list[i.item()] for i in a ]\n",
    "\n",
    "print( b )\n",
    "\n",
    "\n",
    "d = idx_list['<pad>']\n",
    "\n",
    "print( d )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "649837a5-2937-4e81-880b-a57f2532b967",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    ''' ミニバッチデータを作成するクラス\n",
    "        torch.utils.data.Datasetクラスを継承し，\n",
    "        以下の関数を定義する\n",
    "        __len__: 総サンプル数を出力する関数\n",
    "        __getitem__: 1サンプルのデータを出力する関数\n",
    "    feat_scp:  特徴量リストファイル\n",
    "    label_scp: ラベルファイル\n",
    "    feat_mean: 特徴量の平均値ベクトル\n",
    "    feat_std:  特徴量の次元毎の標準偏差を並べたベクトル \n",
    "    pad_index: バッチ化の際にフレーム数を合わせる\n",
    "               ためにpaddingする整数値\n",
    "    splice:    前後(splice)フレームを特徴量を結合する\n",
    "               splice=1とすると，前後1フレーム分結合\n",
    "               するので次元数は3倍になる．\n",
    "               splice=0の場合は何もしない\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 et,\n",
    "                 pad_index_s,\n",
    "                 pad_index_t\n",
    "                 ):\n",
    "\n",
    "        # 読み込みながら情報を取得する\n",
    "        self.pad_index_s = pad_index_s\n",
    "        self.pad_index_t = pad_index_t\n",
    "        print( \"pad_index_s:\", self.pad_index_s )\n",
    "        print( \"pad_index_t:\", self.pad_index_t )\n",
    "        self.en_list = []\n",
    "        self.en_lens = []\n",
    "        self.et_list = []\n",
    "        self.et_lens = []\n",
    "        self.num_data = 0\n",
    "        for line in et:\n",
    "            # 各行をスペースで区切り，\n",
    "            # リスト型の変数にする\n",
    "            self.en_list.append( line['target'] )\n",
    "            self.en_lens.append( len( line['target']) )\n",
    "            #self.en_att_list.append( line['target']['attention_mask'] )\n",
    "\n",
    "            self.et_list.append( line['source'] )\n",
    "            self.et_lens.append( len( line['source']) )\n",
    "            #self.et_att_list.append( line['source']['attention_mask'] )\n",
    "            self.num_data += 1\n",
    "\n",
    "        #self.en_list = np.int64( np.array( self.en_list ) )\n",
    "        #self.et_list = np.int64( np.array( self.et_list ) )\n",
    "        self.en_lens = np.int64( np.array( self.en_lens ) )\n",
    "        self.et_lens = np.int64( np.array( self.et_lens ) )\n",
    "\n",
    "        # フレーム数の最大値を得る\n",
    "        self.max_en_len = np.max(self.en_lens)\n",
    "        # ラベル長の最大値を得る\n",
    "        self.max_et_len = np.max(self.et_lens)\n",
    "\n",
    "        for n in range(self.num_data):\n",
    "            if n % 10000 == 0:\n",
    "                print( \"n:\", n )\n",
    "            # 埋めるフレームの数\n",
    "            # = 最大フレーム数 - 自分のフレーム数\n",
    "            pad_len = self.max_en_len - self.en_lens[n]\n",
    "            # pad_indexの値で埋める\n",
    "            tmp = self.en_list[n]\n",
    "            self.en_list[n] = np.pad( tmp, (0, pad_len), mode='constant', constant_values=(self.pad_index_t, self.pad_index_t ))\n",
    "            pad_len = self.max_et_len - self.et_lens[n]\n",
    "            # pad_indexの値で埋める\n",
    "            self.et_list[n] = np.pad(self.et_list[n],[0, pad_len],mode='constant', constant_values=self.pad_index_s)\n",
    "\n",
    "        self.en_list = np.int64( np.array( self.en_list ) )\n",
    "        self.et_list = np.int64( np.array( self.et_list ) )\n",
    "\n",
    "    def __len__(self):\n",
    "        ''' 学習データの総サンプル数を返す関数\n",
    "        本実装では発話単位でバッチを作成するため，\n",
    "        総サンプル数=発話数である．\n",
    "        '''\n",
    "        return self.num_data\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ''' サンプルデータを返す関数\n",
    "        本実装では発話単位でバッチを作成するため，\n",
    "        idx=発話番号である．\n",
    "        '''\n",
    "\n",
    "        # ラベル\n",
    "        et = self.et_list[idx]\n",
    "        et_len = self.et_lens[idx]\n",
    "\n",
    "        # 発話ID\n",
    "        en = self.en_list[idx]\n",
    "        en_len = self.en_lens[idx]\n",
    "\n",
    "        # 特徴量，ラベル，フレーム数，\n",
    "        # ラベル長，発話IDを返す\n",
    "        #return (jps, jp_lens, ens,  en_lens)\n",
    "        return (et, et_len, en, en_len )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa12965b-1beb-4be9-967f-7c533486462d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it_train: 499000\n",
      "it_val: 500\n",
      "pad_index_s: 0\n",
      "pad_index_t: 0\n",
      "n: 0\n",
      "n: 10000\n",
      "n: 20000\n",
      "n: 30000\n",
      "n: 40000\n",
      "n: 50000\n",
      "n: 60000\n",
      "n: 70000\n",
      "n: 80000\n",
      "n: 90000\n",
      "n: 100000\n",
      "n: 110000\n",
      "n: 120000\n",
      "n: 130000\n",
      "n: 140000\n",
      "n: 150000\n",
      "n: 160000\n",
      "n: 170000\n",
      "n: 180000\n",
      "n: 190000\n",
      "n: 200000\n",
      "n: 210000\n",
      "n: 220000\n",
      "n: 230000\n",
      "n: 240000\n",
      "n: 250000\n",
      "n: 260000\n",
      "n: 270000\n",
      "n: 280000\n",
      "n: 290000\n",
      "n: 300000\n",
      "n: 310000\n",
      "n: 320000\n",
      "n: 330000\n",
      "n: 340000\n",
      "n: 350000\n",
      "n: 360000\n",
      "n: 370000\n",
      "n: 380000\n",
      "n: 390000\n",
      "n: 400000\n",
      "n: 410000\n",
      "n: 420000\n",
      "n: 430000\n",
      "n: 440000\n",
      "n: 450000\n",
      "n: 460000\n",
      "n: 470000\n",
      "n: 480000\n",
      "n: 490000\n",
      "pad_index_s: 0\n",
      "pad_index_t: 0\n",
      "n: 0\n"
     ]
    }
   ],
   "source": [
    "with open(\"data_train_it.pkl\", mode=\"rb\") as f:\n",
    "    it_train = pickle.load(f)\n",
    "with open(\"data_val_it.pkl\", mode=\"rb\") as f:\n",
    "    it_val = pickle.load(f)\n",
    "\n",
    "#it_train = it_train[:10000]\n",
    "\n",
    "print( \"it_train:\", len( it_train ) )\n",
    "print( \"it_val:\", len( it_val ) )\n",
    "\n",
    "train_dataset = SequenceDataset( it_train, pad_idx_s, pad_idx_t )\n",
    "val_dataset = SequenceDataset( it_val, pad_idx_s, pad_idx_t  )\n",
    "    \n",
    "batch_size = 20\n",
    "#num_workers = 4 if torch.cuda.is_available() else 0\n",
    "num_workers = 0 if device == torch.device( 'cpu' ) else 4\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers)\n",
    "# 開発データのDataLoaderを呼び出す\n",
    "# 開発データはデータはシャッフルしない\n",
    "val_loader = DataLoader(val_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers)    \n",
    "\n",
    "#Iter = iter(train_loader)\n",
    "#xdata, xatt, ydata, yatt = next(Iter) #教師データ、ラベルデータ\n",
    "#print(xdata, xatt, ydata, yatt)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b825cab-27ba-4da2-8050-2631036aa9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        #self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        #return self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66322069-a9f2-458e-af00-45953e75b074",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    '''\n",
    "    位置埋め込み （Positional embedding）\n",
    "    dim_embedding: 埋込み次元\n",
    "    max_len      : 入力の最大系列長\n",
    "    '''\n",
    "    def __init__(self, dim_embedding: int, max_len: int=5000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pos_emb = nn.Embedding(max_len, dim_embedding)\n",
    "\n",
    "    '''\n",
    "    位置エンコーディングの順伝播\n",
    "    x: 位置エンコーディングを埋め込む対象のテンソル,\n",
    "       [バッチサイズ, 系列長, 埋め込み次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        seq = x.shape[1]\n",
    "        positions = torch.arange(start=0, end=seq, step=1, device=x.device).to(torch.long)\n",
    "        positions = self.pos_emb(positions)[:seq,:]\n",
    "        \n",
    "        return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cea6ff9-e8c6-436a-941b-34a4e2e4fced",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fPositionalEmbedding(nn.Module):\n",
    "    '''\n",
    "    位置埋め込み （Positional embedding）\n",
    "    dim_embedding: 埋込み次元\n",
    "    max_len      : 入力の最大系列長\n",
    "    '''\n",
    "    def __init__(self, dim_embedding: int, name, max_len: int=5000):\n",
    "        super().__init__()\n",
    "\n",
    "        #self.pos_emb = nn.Embedding(max_len, dim_embedding)\n",
    "        self.name = name\n",
    "        \n",
    "    '''\n",
    "    位置エンコーディングの順伝播\n",
    "    x: 位置エンコーディングを埋め込む対象のテンソル,\n",
    "       [バッチサイズ, 系列長, 埋め込み次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor, weights):\n",
    "        seq = x.shape[1]\n",
    "        positions = torch.arange(start=0, end=seq, step=1, device=x.device).to(torch.long)\n",
    "        #positions = self.pos_emb(positions)[:seq,:]\n",
    "        positions = F.embedding( positions, weights[self.name] )[:seq,:]\n",
    "        \n",
    "        return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e05abc0d-48d7-4e58-8bc4-0a5310e59330",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "    '''\n",
    "    自己アテンション\n",
    "    dim_hidden: 入力特徴量の次元\n",
    "    num_heads : マルチヘッドアテンションのヘッド数\n",
    "    qkv_bias  : クエリなどを生成する全結合層のバイアスの有無\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 qkv_bias: bool=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # 特徴量を各ヘッドのために分割するので、\n",
    "        # 特徴量次元をヘッド数で割り切れるか検証\n",
    "        assert dim_hidden % num_heads == 0\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # ヘッド毎の特徴量次元\n",
    "        dim_head = dim_hidden // num_heads\n",
    "\n",
    "        # ソフトマックスのスケール値\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        # ヘッド毎にクエリ、キーおよびバリューを生成するための全結合層\n",
    "        self.proj_in_q = nn.Linear(\n",
    "            dim_hidden, dim_hidden, bias=qkv_bias)\n",
    "        self.proj_in_k = nn.Linear(\n",
    "            dim_hidden, dim_hidden, bias=qkv_bias)\n",
    "        self.proj_in_v = nn.Linear(\n",
    "            dim_hidden, dim_hidden, bias=qkv_bias)\n",
    "\n",
    "        #self.in_proj_weight = nn.Parameter( torch.randn( dim_hidden * 3, dim_hidden ) )\n",
    "        #self.in_proj_bias = nn.Parameter( torch.randn( dim_hidden * 3 ) )\n",
    "        #self.in_proj_weight = Parameter( torch.empty( dim_hidden * 3, dim_hidden ) )\n",
    "        #self.in_proj_bias = Parameter( torch.empty( dim_hidden * 3 ) )\n",
    "\n",
    "        # 各ヘッドから得られた特徴量を一つにまとめる全結合層\n",
    "        self.proj_out = nn.Linear(dim_hidden, dim_hidden)\n",
    "        \n",
    "        #self._reset_parameters()\n",
    "        \n",
    "    #def _reset_parameters(self):\n",
    "    #    xavier_uniform_( self.in_proj_weight )\n",
    "    #    constant_( self.in_proj_bias, 0.0 )\n",
    "\n",
    "    def split_head(self, x):\n",
    "        x = torch.tensor_split(x, self.num_heads, dim = 2)\n",
    "        x = torch.stack(x, dim = 1)\n",
    "        return x\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, mask: torch.Tensor):\n",
    "\n",
    "        #bs_q, ns_q, ds_q = q.size()\n",
    "        #bs_k, ns_k, ds_k = k.size()\n",
    "        \n",
    "        ##  k = v assumpotion\n",
    "        #if q is k:\n",
    "        #    qkv = q @ self.in_proj_weight.transpose(-2,-1) + self.in_proj_bias\n",
    "        #    qkv = qkv.view( bs_q, ns_q, 3, ds_q )\n",
    "        #    q, k, v = torch.unbind( qkv, dim = 2 )\n",
    "        #else:\n",
    "        #    W_q, W_kv = self.in_proj_weight.split([ds_q, ds_q * 2])\n",
    "        #    b_q, b_kv = self.in_proj_bias.split([ds_q, ds_q * 2])\n",
    "        #    q = q @ W_q.transpose(-2,-1) + b_q\n",
    "        #    kv =  k @ W_kv.transpose(-2,-1) + b_kv\n",
    "        #    kv = kv.view( bs_k, ns_k, 2, ds_k )\n",
    "        #    k, v = torch.unbind( kv, dim = 2 )\n",
    "        \n",
    "        q = self.proj_in_q(q)\n",
    "        k = self.proj_in_k(k)\n",
    "        v = self.proj_in_v(v)\n",
    "        \n",
    "        q = self.split_head(q)\n",
    "        k = self.split_head(k)\n",
    "        v = self.split_head(v)\n",
    "\n",
    "        # クエリとキーの行列積とアテンションの計算(今回マスクは不使用)\n",
    "        # attnは[バッチサイズ, ヘッド数, 特徴量数, 特徴量数]\n",
    "        attn = q.matmul(k.transpose(-2, -1))\n",
    "        #print( \"attn size:\", attn.size() )\n",
    "        #print( \"mask size:\", mask.size() )\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill_(mask, -torch.finfo(torch.float).max)\n",
    "        attn = (attn * self.scale).softmax(dim=-1)\n",
    "\n",
    "        # アテンションとバリューの行列積によりバリューを収集\n",
    "        # xは[バッチサイズ, ヘッド数, 特徴量数, ヘッドの特徴量次元]\n",
    "        x = attn.matmul(v)\n",
    "\n",
    "        # permute関数により\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数, ヘッドの特徴量次元]\n",
    "        # flatten関数により全てのヘッドから得られる特徴量を連結して、\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数 * ヘッドの特徴量次元]\n",
    "        x = x.permute(0, 2, 1, 3).flatten(2)\n",
    "        x = self.proj_out(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a51e5fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class feMHA(nn.Module):\n",
    "    '''\n",
    "    自己アテンション\n",
    "    dim_hidden: 入力特徴量の次元\n",
    "    num_heads : マルチヘッドアテンションのヘッド数\n",
    "    qkv_bias  : クエリなどを生成する全結合層のバイアスの有無\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 qkv_bias: bool=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # 特徴量を各ヘッドのために分割するので、\n",
    "        # 特徴量次元をヘッド数で割り切れるか検証\n",
    "        assert dim_hidden % num_heads == 0\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # ヘッド毎の特徴量次元\n",
    "        dim_head = dim_hidden // num_heads\n",
    "\n",
    "        # ソフトマックスのスケール値\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        # ヘッド毎にクエリ、キーおよびバリューを生成するための全結合層\n",
    "        #self.proj_in = nn.Linear(\n",
    "        #    dim_hidden, dim_hidden * 3, bias=qkv_bias)\n",
    "\n",
    "        # 各ヘッドから得られた特徴量を一つにまとめる全結合層\n",
    "        #self.proj_out = nn.Linear(dim_hidden, dim_hidden)\n",
    "\n",
    "    def split_head(self, x):\n",
    "        x = torch.tensor_split(x, self.num_heads, dim = 2)\n",
    "        x = torch.stack(x, dim = 1)\n",
    "        return x\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor,  v: torch.Tensor, mask: torch.Tensor, i, weights ):\n",
    "        bs, ns = q.shape[:2]\n",
    "\n",
    "        #qkv = self.proj_in(x)\n",
    "        q = F.linear(q, weights['encoder.encoder_layers.' + str(i) + '.attention.proj_in_q.weight'], weights['encoder.encoder_layers.' + str(i) + '.attention.proj_in_q.bias'])\n",
    "        k = F.linear(k, weights['encoder.encoder_layers.' + str(i) + '.attention.proj_in_k.weight'], weights['encoder.encoder_layers.' + str(i) + '.attention.proj_in_k.bias'])\n",
    "        v = F.linear(v, weights['encoder.encoder_layers.' + str(i) + '.attention.proj_in_v.weight'], weights['encoder.encoder_layers.' + str(i) + '.attention.proj_in_v.bias'])\n",
    "\n",
    "        q = self.split_head( q )\n",
    "        k = self.split_head( k )\n",
    "        v = self.split_head( v )\n",
    "\n",
    "        # クエリとキーの行列積とアテンションの計算(今回マスクは不使用)\n",
    "        # attnは[バッチサイズ, ヘッド数, 特徴量数, 特徴量数]\n",
    "        attn = q.matmul(k.transpose(-2, -1))\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill_(mask, -torch.finfo(torch.float).max)\n",
    "        attn = (attn * self.scale).softmax(dim=-1)\n",
    "\n",
    "        # アテンションとバリューの行列積によりバリューを収集\n",
    "        # xは[バッチサイズ, ヘッド数, 特徴量数, ヘッドの特徴量次元]\n",
    "        x = attn.matmul(v)\n",
    "\n",
    "        # permute関数により\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数, ヘッドの特徴量次元]\n",
    "        # flatten関数により全てのヘッドから得られる特徴量を連結して、\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数 * ヘッドの特徴量次元]\n",
    "        x = x.permute(0, 2, 1, 3).flatten(2)\n",
    "        #x = self.proj_out(x)\n",
    "        x = F.linear(x, weights['encoder.encoder_layers.' + str(i) + '.attention.proj_out.weight'], weights['encoder.encoder_layers.' + str(i) + '.attention.proj_out.bias'])\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5839e53-a27a-4be2-8e24-b5a5b0deec8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fdsMHA(nn.Module):\n",
    "    '''\n",
    "    自己アテンション\n",
    "    dim_hidden: 入力特徴量の次元\n",
    "    num_heads : マルチヘッドアテンションのヘッド数\n",
    "    qkv_bias  : クエリなどを生成する全結合層のバイアスの有無\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 qkv_bias: bool=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # 特徴量を各ヘッドのために分割するので、\n",
    "        # 特徴量次元をヘッド数で割り切れるか検証\n",
    "        assert dim_hidden % num_heads == 0\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # ヘッド毎の特徴量次元\n",
    "        dim_head = dim_hidden // num_heads\n",
    "\n",
    "        # ソフトマックスのスケール値\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        # ヘッド毎にクエリ、キーおよびバリューを生成するための全結合層\n",
    "        #self.proj_in = nn.Linear(\n",
    "        #    dim_hidden, dim_hidden * 3, bias=qkv_bias)\n",
    "\n",
    "        # 各ヘッドから得られた特徴量を一つにまとめる全結合層\n",
    "        #self.proj_out = nn.Linear(dim_hidden, dim_hidden)\n",
    "\n",
    "    def split_head(self, x):\n",
    "        x = torch.tensor_split(x, self.num_heads, dim = 2)\n",
    "        x = torch.stack(x, dim = 1)\n",
    "        return x\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor,  v: torch.Tensor, mask: torch.Tensor, i, weights ):\n",
    "        bs, ns = q.shape[:2]\n",
    "\n",
    "        #qkv = self.proj_in(x)\n",
    "        #print( \"size q:\", q.size() )\n",
    "        #print( \"size weigths q:\", weights['d_layers.' + str(i) + '.crossattn.proj_in_q.weight'].size() )\n",
    "        #print( \"size k:\", k.size() )\n",
    "        #print( \"size weights k:\", weights['d_layers.' + str(i) + '.crossattn.proj_in_k.weight'].size() )\n",
    "        q = F.linear(q, weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_in_q.weight'], weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_in_q.bias'])\n",
    "        k = F.linear(k, weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_in_k.weight'], weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_in_k.bias'])\n",
    "        v = F.linear(v, weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_in_v.weight'], weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_in_v.bias'])\n",
    "\n",
    "        q = self.split_head( q )\n",
    "        k = self.split_head( k )\n",
    "        v = self.split_head( v )\n",
    "\n",
    "        # view関数により\n",
    "        # [バッチサイズ, 特徴量数, QKV, ヘッド数, ヘッドの特徴量次元]\n",
    "        # permute関数により\n",
    "        # [QKV, バッチサイズ, ヘッド数, 特徴量数, ヘッドの特徴量次元]\n",
    "        #qkv = qkv.view(\n",
    "        #    bs, ns, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        # クエリ、キーおよびバリューに分解\n",
    "        #q, k, v = qkv.unbind(0)\n",
    "\n",
    "        # クエリとキーの行列積とアテンションの計算(今回マスクは不使用)\n",
    "        # attnは[バッチサイズ, ヘッド数, 特徴量数, 特徴量数]\n",
    "        attn = q.matmul(k.transpose(-2, -1))\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill_(mask, -torch.finfo(torch.float).max)      \n",
    "        attn = (attn * self.scale).softmax(dim=-1)\n",
    "\n",
    "        # アテンションとバリューの行列積によりバリューを収集\n",
    "        # xは[バッチサイズ, ヘッド数, 特徴量数, ヘッドの特徴量次元]\n",
    "        x = attn.matmul(v)\n",
    "\n",
    "        # permute関数により\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数, ヘッドの特徴量次元]\n",
    "        # flatten関数により全てのヘッドから得られる特徴量を連結して、\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数 * ヘッドの特徴量次元]\n",
    "        x = x.permute(0, 2, 1, 3).flatten(2)\n",
    "        #x = self.proj_out(x)\n",
    "        x = F.linear(x, weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_out.weight'], weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_out.bias'])\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0464026d-c2fd-456a-89dc-e70cd632fbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fdcMHA(nn.Module):\n",
    "    '''\n",
    "    自己アテンション\n",
    "    dim_hidden: 入力特徴量の次元\n",
    "    num_heads : マルチヘッドアテンションのヘッド数\n",
    "    qkv_bias  : クエリなどを生成する全結合層のバイアスの有無\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 qkv_bias: bool=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # 特徴量を各ヘッドのために分割するので、\n",
    "        # 特徴量次元をヘッド数で割り切れるか検証\n",
    "        assert dim_hidden % num_heads == 0\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # ヘッド毎の特徴量次元\n",
    "        dim_head = dim_hidden // num_heads\n",
    "\n",
    "        # ソフトマックスのスケール値\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        # ヘッド毎にクエリ、キーおよびバリューを生成するための全結合層\n",
    "        #self.proj_in = nn.Linear(\n",
    "        #    dim_hidden, dim_hidden * 3, bias=qkv_bias)\n",
    "\n",
    "        # 各ヘッドから得られた特徴量を一つにまとめる全結合層\n",
    "        #self.proj_out = nn.Linear(dim_hidden, dim_hidden)\n",
    "\n",
    "    def split_head(self, x):\n",
    "        x = torch.tensor_split(x, self.num_heads, dim = 2)\n",
    "        x = torch.stack(x, dim = 1)\n",
    "        return x\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor,  v: torch.Tensor, mask: torch.Tensor, i, weights ):\n",
    "        bs, ns = q.shape[:2]\n",
    "\n",
    "        #qkv = self.proj_in(x)\n",
    "        #print( \"size q:\", q.size() )\n",
    "        #print( \"size weigths q:\", weights['d_layers.' + str(i) + '.crossattn.proj_in_q.weight'].size() )\n",
    "        #print( \"size k:\", k.size() )\n",
    "        #print( \"size weights k:\", weights['d_layers.' + str(i) + '.crossattn.proj_in_k.weight'].size() )\n",
    "        q = F.linear(q, weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_in_q.weight'], weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_in_q.bias'])\n",
    "        k = F.linear(k, weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_in_k.weight'], weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_in_k.bias'])\n",
    "        v = F.linear(v, weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_in_v.weight'], weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_in_v.bias'])\n",
    "\n",
    "        q = self.split_head( q )\n",
    "        k = self.split_head( k )\n",
    "        v = self.split_head( v )\n",
    "\n",
    "        # view関数により\n",
    "        # [バッチサイズ, 特徴量数, QKV, ヘッド数, ヘッドの特徴量次元]\n",
    "        # permute関数により\n",
    "        # [QKV, バッチサイズ, ヘッド数, 特徴量数, ヘッドの特徴量次元]\n",
    "        #qkv = qkv.view(\n",
    "        #    bs, ns, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        # クエリ、キーおよびバリューに分解\n",
    "        #q, k, v = qkv.unbind(0)\n",
    "\n",
    "        # クエリとキーの行列積とアテンションの計算(今回マスクは不使用)\n",
    "        # attnは[バッチサイズ, ヘッド数, 特徴量数, 特徴量数]\n",
    "        attn = q.matmul(k.transpose(-2, -1))\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill_(mask, -torch.finfo(torch.float).max)      \n",
    "        attn = (attn * self.scale).softmax(dim=-1)\n",
    "\n",
    "        # アテンションとバリューの行列積によりバリューを収集\n",
    "        # xは[バッチサイズ, ヘッド数, 特徴量数, ヘッドの特徴量次元]\n",
    "        x = attn.matmul(v)\n",
    "\n",
    "        # permute関数により\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数, ヘッドの特徴量次元]\n",
    "        # flatten関数により全てのヘッドから得られる特徴量を連結して、\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数 * ヘッドの特徴量次元]\n",
    "        x = x.permute(0, 2, 1, 3).flatten(2)\n",
    "        #x = self.proj_out(x)\n",
    "        x = F.linear(x, weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_out.weight'], weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_out.bias'])\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88160e0b-1bca-44a6-88d3-22f706a89b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "    '''\n",
    "    Transformerエンコーダ内の順伝播型ニューラルネットワーク\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    dim_feedforward: 中間特徴量の次元\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, dim_feedforward: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(dim_hidden, dim_feedforward)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, dim_hidden)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbd75eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class feFNN(nn.Module):\n",
    "    '''\n",
    "    Transformerエンコーダ内の順伝播型ニューラルネットワーク\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    dim_feedforward: 中間特徴量の次元\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, dim_feedforward: int):\n",
    "        super().__init__()\n",
    "\n",
    "        #self.linear1 = nn.Linear(dim_hidden, dim_feedforward)\n",
    "        #self.linear2 = nn.Linear(dim_feedforward, dim_hidden)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor, i, weights ):\n",
    "        #x = self.linear1(x)\n",
    "        x = F.linear(x, weights['encoder.encoder_layers.' + str(i) + '.fnn.linear1.weight'], weights['encoder.encoder_layers.' + str(i) + '.fnn.linear1.bias'])\n",
    "        x = self.activation(x)\n",
    "        #x = self.linear2(x)\n",
    "        x = F.linear(x, weights['encoder.encoder_layers.' + str(i) + '.fnn.linear2.weight'], weights['encoder.encoder_layers.' + str(i) + '.fnn.linear2.bias'])\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1830c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fdFNN(nn.Module):\n",
    "    '''\n",
    "    Transformerエンコーダ内の順伝播型ニューラルネットワーク\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    dim_feedforward: 中間特徴量の次元\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, dim_feedforward: int):\n",
    "        super().__init__()\n",
    "\n",
    "        #self.linear1 = nn.Linear(dim_hidden, dim_feedforward)\n",
    "        #self.linear2 = nn.Linear(dim_feedforward, dim_hidden)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor, i, weights ):\n",
    "        #x = self.linear1(x)\n",
    "        x = F.linear(x, weights['decoder.decoder_layers.' + str(i) + '.fnn.linear1.weight'], weights['decoder.decoder_layers.' + str(i) + '.fnn.linear1.bias'])\n",
    "        x = self.activation(x)\n",
    "        #x = self.linear2(x)\n",
    "        x = F.linear(x, weights['decoder.decoder_layers.' + str(i) + '.fnn.linear2.weight'], weights['decoder.decoder_layers.' + str(i) + '.fnn.linear2.bias'])\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "caaa86ef-4575-4c9c-9265-a337fb7a9734",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    '''\n",
    "    Transformerエンコーダ層\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    num_heads      : ヘッド数\n",
    "    dim_feedforward: 中間特徴量の次元\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 dim_feedforward: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = MHA(dim_hidden, num_heads, qkv_bias = True)\n",
    "        self.fnn = FNN(dim_hidden, dim_feedforward)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(dim_hidden)\n",
    "        self.norm2 = nn.LayerNorm(dim_hidden)\n",
    "        \n",
    "        self.dropout = nn.Dropout( dropout )\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor):\n",
    "        ''' B2T\n",
    "        x0 = x\n",
    "        x = self.attention( x, x, x, mask )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x = self.norm1(x)\n",
    "        x1 = x\n",
    "        x = self.fnn( x )\n",
    "        x = self.dropout( x )\n",
    "        x =  x + x1 + x0\n",
    "        x = self.norm2( x )\n",
    "        '''\n",
    "        #pre-LN\n",
    "        x0 = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attention( x, x, x, mask )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x1 = x\n",
    "        x = self.norm2( x )\n",
    "        x = self.fnn( x )\n",
    "        x = self.dropout( x )\n",
    "        x =  x + x1\n",
    "        \n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a2533c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fTransformerEncoderLayer(nn.Module):\n",
    "    '''\n",
    "    Transformerエンコーダ層\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    num_heads      : ヘッド数\n",
    "    dim_feedforward: 中間特徴量の次元\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 dim_feedforward: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fattention = feMHA(dim_hidden, num_heads)\n",
    "        self.fffn = feFNN(dim_hidden, dim_feedforward)\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.dropout = nn.Dropout( dropout )\n",
    "        \n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor, i, weights ):\n",
    "        '''B2T\n",
    "        x0 = x\n",
    "        x = self.fattention( x, x, x, mask, i, weights )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['encoder.encoder_layers.' + str(i) + '.norm1.weight'], bias=weights['encoder.encoder_layers.' + str(i) + '.norm1.bias'], eps=1e-05)\n",
    "        x1 = x\n",
    "        x = self.fffn( x, i, weights ) \n",
    "        x = self.dropout( x )\n",
    "        x = x + x1 + x0\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['encoder.encoder_layers.' + str(i) + '.norm2.weight'], bias=weights['encoder.encoder_layers.' + str(i) + '.norm2.bias'], eps=1e-05)\n",
    "        '''\n",
    "        #pre-LN\n",
    "        x0 = x\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['encoder.encoder_layers.' + str(i) + '.norm1.weight'], bias=weights['encoder.encoder_layers.' + str(i) + '.norm1.bias'], eps=1e-05)\n",
    "        x = self.fattention( x, x, x, mask, i, weights )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x1 = x\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['encoder.encoder_layers.' + str(i) + '.norm2.weight'], bias=weights['encoder.encoder_layers.' + str(i) + '.norm2.bias'], eps=1e-05)\n",
    "        x = self.fffn( x, i, weights ) \n",
    "        x = self.dropout( x )\n",
    "        x = x + x1\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "387ffc10-2b89-4039-8a6a-4b4a8fbde394",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    '''\n",
    "    Transformerエンコーダ層\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    num_heads      : ヘッド数\n",
    "    dim_feedforward: 中間特徴量の次元\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 dim_feedforward: int, dropout: float=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.selfattn = MHA(dim_hidden, num_heads, qkv_bias = True)\n",
    "        self.crossattn = MHA(dim_hidden, num_heads, qkv_bias = True)\n",
    "        self.fnn = FNN(dim_hidden, dim_feedforward)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(dim_hidden)\n",
    "        self.norm2 = nn.LayerNorm(dim_hidden)\n",
    "        self.norm3 = nn.LayerNorm(dim_hidden)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor,  self_mask: torch.Tensor,  cross_mask: torch.Tensor):\n",
    "        '''B2T\n",
    "        x0 = x\n",
    "        x = self.selfattn( x, x, x, self_mask )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x = self.norm1( x )\n",
    "        x1 = x\n",
    "        x = self.crossattn( x, y, y, cross_mask )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x1\n",
    "        x = self.norm2( x )\n",
    "        x2 = x\n",
    "        x = self.fnn( x )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x2 + x0\n",
    "        x = self.norm3( x )\n",
    "        '''\n",
    "        #pre-LN\n",
    "        x0 = x\n",
    "        x = self.norm1( x )\n",
    "        x = self.selfattn( x, x, x, self_mask )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x1 = x\n",
    "        x = self.norm2( x )\n",
    "        x = self.crossattn( x, y, y, cross_mask )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x1\n",
    "        x2 = x\n",
    "        x = self.norm3( x )\n",
    "        x = self.fnn( x )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x2\n",
    "        \n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "063637b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fTransformerDecoderLayer(nn.Module):\n",
    "    '''\n",
    "    Transformerエンコーダ層\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    num_heads      : ヘッド数\n",
    "    dim_feedforward: 中間特徴量の次元\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 dim_feedforward: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fselfattn = fdsMHA(dim_hidden, num_heads)\n",
    "        self.fcrossattn = fdcMHA(dim_hidden, num_heads)\n",
    "        self.fffn = fdFNN(dim_hidden, dim_feedforward)\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        #self.norm1 = nn.LayerNorm(dim_hidden)\n",
    "        #self.norm2 = nn.LayerNorm(dim_hidden)\n",
    "        #self.norm3 = nn.LayerNorm(dim_hidden)\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor,  self_mask: torch.Tensor,  cross_mask: torch.Tensor, i, weights):\n",
    "        '''B2T\n",
    "        x0 = x\n",
    "        x = self.fselfattn( x, x, x, self_mask, i, weights )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['decoder.decoder_layers.' + str(i) + '.norm1.weight'], bias=weights['decoder.decoder_layers.' + str(i) + '.norm1.bias'], eps=1e-05)\n",
    "        x1 = x\n",
    "        x = self.fcrossattn( x, y, y, cross_mask, i, weights )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x1\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['decoder.decoder_layers.' + str(i) + '.norm2.weight'], bias=weights['decoder.decoder_layers.' + str(i) + '.norm2.bias'], eps=1e-05)\n",
    "        x2 = x\n",
    "        x = self.fffn( x, i, weights )\n",
    "        x = self.dropout( x ) \n",
    "        x = x + x2 + x0\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['decoder.decoder_layers.' + str(i) + '.norm3.weight'], bias=weights['decoder.decoder_layers.' + str(i) + '.norm3.bias'], eps=1e-05)\n",
    "        '''\n",
    "        #pre-LN\n",
    "        x0 = x\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['decoder.decoder_layers.' + str(i) + '.norm1.weight'], bias=weights['decoder.decoder_layers.' + str(i) + '.norm1.bias'], eps=1e-05)\n",
    "        x = self.fselfattn( x, x, x, self_mask, i, weights )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x1 = x\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['decoder.decoder_layers.' + str(i) + '.norm2.weight'], bias=weights['decoder.decoder_layers.' + str(i) + '.norm2.bias'], eps=1e-05)\n",
    "        x = self.fcrossattn( x, y, y, cross_mask, i, weights )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x1\n",
    "        x2 = x\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['decoder.decoder_layers.' + str(i) + '.norm3.weight'], bias=weights['decoder.decoder_layers.' + str(i) + '.norm3.bias'], eps=1e-05)\n",
    "        x = self.fffn( x, i, weights )\n",
    "        x = self.dropout( x ) \n",
    "        x = x + x2\n",
    "        \n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0509fcd8-bf62-4006-a5ca-94f2fe331e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    '''\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    dim_feedforward: FNNにおける中間特徴量の次元\n",
    "    num_heads      : マルチヘッドアテンションのヘッド数\n",
    "    num_layers     : Transformerエンコーダの層数\n",
    "    '''\n",
    "    def __init__(self, text_vocab_size: int, dim_embedding: int, dim_feedforward: int,\n",
    "                 num_heads: int,  num_layers: int, pad_index:int, dropout: float = 0.1 ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 単語埋め込み\n",
    "        self.embed = nn.Embedding(\n",
    "            text_vocab_size, dim_embedding, padding_idx=pad_index)        \n",
    "        \n",
    "        # 位置エンコーディング\n",
    "        #self.pos_enc = PositionalEncoding(dim_embedding)\n",
    "        self.pos_emb = PositionalEmbedding(dim_embedding)\n",
    "        #self.dropout = nn.Dropout( dropout )\n",
    "        \n",
    "        # Transformerエンコーダ層\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(dim_hidden=dim_embedding, num_heads=num_heads, dim_feedforward = dim_feedforward, dropout = dropout )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(dim_embedding)\n",
    "       \n",
    "        self.pad_index = pad_index\n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x           : 入力, [バッチサイズ, 入力チャネル数, 高さ, 幅]\n",
    "    return_embed: 特徴量を返すかロジットを返すかを選択する真偽値\n",
    "    '''\n",
    "    def forward(self, src: torch.Tensor, src_mask: torch.Tensor=None, src_padding_mask: torch.Tensor=None ):\n",
    "\n",
    "        if src_padding_mask is not None and src_mask is None:\n",
    "            mask = src_padding_mask[:,None,None,:]\n",
    "            mask = mask.expand( (-1, self.num_heads, src.size(1), -1) )\n",
    "        elif src_padding_mask is not None and src_mask is not None:\n",
    "            mask1 = src_padding_mask[:,None,None,:]\n",
    "            mask1 = mask1.expand( (-1, self.num_heads, src.size(1), -1 ) )\n",
    "            mask2 = src_mask[None,None,:,:]\n",
    "            mask2 = mask2.expand( ( src.size(0), self.num_heads, -1, -1 ) )\n",
    "            mask = torch.logical_or(mask1, mask2 )\n",
    "        elif src_padding_mask is None and src_mask is not None:\n",
    "            mask = src_mask[None,None,:,:]\n",
    "            mask = mask.padding( ( src.size(0), src.size(1), -1, -1 ) )\n",
    "        else:\n",
    "            mask = None\n",
    "\n",
    "        x = self.embed( src ) * math.sqrt( self.dim_embedding )\n",
    "\n",
    "        #x = self.pos_enc( x )\n",
    "        position = self.pos_emb( x )\n",
    "        x = x + position\n",
    "        #x = self.dropout( x )\n",
    "        #x = self.norm( x )\n",
    "        \n",
    "        # Transformerエンコーダ層を適用\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer( x, mask )\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd45f25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fTransformerEncoder(nn.Module):\n",
    "    '''\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    dim_feedforward: FNNにおける中間特徴量の次元\n",
    "    num_heads      : マルチヘッドアテンションのヘッド数\n",
    "    num_layers     : Transformerエンコーダの層数\n",
    "    '''\n",
    "    def __init__(self, text_vocab_size: int, dim_embedding: int, dim_feedforward: int,\n",
    "                 num_heads: int,  num_layers: int, pad_idx:int, dropout: float = 0.1 ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 単語埋め込み\n",
    "        #self.embed = nn.Embedding(\n",
    "        #    text_vocab_size, dim_embedding, padding_idx=pad_index)        \n",
    "        \n",
    "        # 位置エンコーディング\n",
    "        #self.pos_enc = PositionalEncoding(dim_embedding)\n",
    "        self.fpos_emb = fPositionalEmbedding(dim_embedding, \"encoder.pos_emb.pos_emb.weight\" )\n",
    "        #self.dropout = nn.Dropout( dropout )\n",
    "        \n",
    "        # Transformerエンコーダ層\n",
    "        self.fenclayer = fTransformerEncoderLayer(dim_hidden=dim_embedding, num_heads=num_heads, dim_feedforward = dim_feedforward, dropout = dropout )\n",
    "        \n",
    "        # ロジットを生成する前のレイヤー正規化と全結合\n",
    "        #self.norm = nn.LayerNorm(dim_embedding)\n",
    "        \n",
    "        self.pad_idx = pad_idx\n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x           : 入力, [バッチサイズ, 入力チャネル数, 高さ, 幅]\n",
    "    return_embed: 特徴量を返すかロジットを返すかを選択する真偽値\n",
    "    '''\n",
    "    def forward(self, src: torch.Tensor, src_mask: torch.Tensor=None, src_padding_mask: torch.Tensor=None, weights = None ):\n",
    "\n",
    "        if src_padding_mask is not None and src_mask is None:\n",
    "            mask = src_padding_mask[:,None,None,:]\n",
    "            mask = mask.expand( (-1, self.num_heads, src.size(1), -1) )\n",
    "        elif src_padding_mask is not None and src_mask is not None:\n",
    "            mask1 = src_padding_mask[:,None,None,:]\n",
    "            mask1 = mask1.expand( (-1, self.num_heads, src.size(1), -1 ) )\n",
    "            mask2 = src_mask[None,None,:,:]\n",
    "            mask2 = mask2.expand( ( src.size(0), self.num_heads, -1, -1 ) )\n",
    "            mask = torch.logical_or(mask1, mask2 )\n",
    "        elif src_padding_mask is None and src_mask is not None:\n",
    "            mask = src_mask[None,None,:,:]\n",
    "            mask = mask.padding( ( src.size(0), src.size(1), -1, -1 ) )\n",
    "        else:\n",
    "            mask = None\n",
    "\n",
    "        #x = self.embed( src ) * math.sqrt( self.dim_embedding )\n",
    "        x = F.embedding( src, weights['encoder.embed.weight'], padding_idx = self.pad_idx )  * math.sqrt( self.dim_embedding )\n",
    "        \n",
    "        #x = self.pos_enc( x )\n",
    "        position = self.fpos_emb( x, weights )\n",
    "        x = x + position\n",
    "        #x = self.dropout( x )\n",
    "        ##x = self.norm(x)\n",
    "        #x = F.layer_norm(x, (self.dim_embedding,), weight=weights['encoder.norm.weight'], bias=weights['encoder.norm.bias'], eps=1e-05)\n",
    "    \n",
    "        # Transformerエンコーダ層を適用\n",
    "        #for layer in self.encoder_layers:\n",
    "        #    x = layer( x, mask )\n",
    "        for block in range( self.num_layers ):\n",
    "            x = self.fenclayer(x, mask, block, weights )\n",
    "\n",
    "        #x = self.norm(x)\n",
    "        x = F.layer_norm(x, (self.dim_embedding,), weight=weights['encoder.norm.weight'], bias=weights['encoder.norm.bias'], eps=1e-05)\n",
    "        \n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15582995-ff08-4fbe-9f54-9e61e13f89e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    '''\n",
    "    CaptioningTransformerのコンストラクタ\n",
    "    dim_embedding  : 埋め込み次元\n",
    "    dim_feedforward: FNNの中間特徴次元\n",
    "    num_heads      : マルチヘッドアテンションのヘッド数\n",
    "    num_layers     : Transformerデコーダ層の数\n",
    "    vocab_size     : 辞書の次元\n",
    "    pad_index     : NULLのID\n",
    "    dropout        : ドロップアウト確率\n",
    "    '''\n",
    "    def __init__(self, vocab_size: int, dim_embedding: int, dim_feedforward: int,\n",
    "                 num_heads: int, num_layers: int, \n",
    "                 pad_index: int, dropout: float=0.1, ds_rate: float=0.1 ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 単語埋め込み\n",
    "        self.embed = nn.Embedding(\n",
    "            vocab_size, dim_embedding, padding_idx=pad_index)\n",
    "        \n",
    "        # 位置エンコーディング\n",
    "        #self.pos_enc = PositionalEncoding(dim_embedding)\n",
    "        self.pos_emb = PositionalEmbedding(dim_embedding)\n",
    "        #self.dropout = nn.Dropout( dropout )\n",
    "\n",
    "        # Transformerデコーダ\n",
    "        #self.decoder_layers = nn.ModuleList([\n",
    "        #    TransformerDecoderLayer(\n",
    "        #        dim_embedding, num_heads, dim_feedforward, dropout)\n",
    "        #    for _ in range(num_layers)\n",
    "        #])\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer(dim_hidden=dim_embedding, num_heads=num_heads, dim_feedforward = dim_feedforward, dropout = dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # ロジットを生成する前のレイヤー正規化と全結合\n",
    "        #self.norm = nn.LayerNorm(dim_embedding)\n",
    "        \n",
    "        # 単語出力分布計算\n",
    "        self.linear = nn.Linear(dim_embedding, vocab_size)\n",
    "        \n",
    "        self.pad_index = pad_index\n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "\n",
    "\n",
    "    ''' CaptioningTransformerの順伝播処理\n",
    "    features: 画像特徴量 [バッチサイズ, 埋め込み次元]\n",
    "    captions: 正解キャプション [バッチサイズ, 系列長]\n",
    "\n",
    "    '''\n",
    "    #def forward(self, features: torch.Tensor, caption_lengths: torch.Tensor):\n",
    "    #def forward(self, features: torch.Tensor, captions: torch.Tensor, padding_mask_src: torch.Tensor=None, \\\n",
    "    #            padding_mask_tgt: torch.Tensor=None, mask_tgt: torch.Tensor=None ):\n",
    "    def forward(self, features: torch.Tensor, captions: torch.Tensor, memory_padding_mask: torch.Tensor=None, \\\n",
    "                tgt_padding_mask: torch.Tensor=None, tgt_mask: torch.Tensor=None ):\n",
    "\n",
    "        #feature_lengths = torch.ones( (features.size(0) ), device=features.device ) * features.size(1)\n",
    "\n",
    "        tgt = captions\n",
    "        if tgt_padding_mask is not None and tgt_mask is not None:\n",
    "            self_mask1 = tgt_padding_mask[:,None,None,:]\n",
    "            self_mask1 = self_mask1.expand( (-1, self.num_heads, tgt.size(1), -1) )\n",
    "            self_mask2 = tgt_mask[None,None,:,:]\n",
    "            self_mask2 = self_mask2.expand( (tgt.size(0), self.num_heads, -1, -1 ))\n",
    "            self_mask = torch.logical_or(self_mask1, self_mask2 )\n",
    "        elif tgt_padding_mask is not None and tgt_mask is None:\n",
    "            self_mask = tgt_padding_mask[:,None,None,:]\n",
    "            self_mask = self_mask.expand( (-1, self.num_heads, tgt.size(1), -1 ) )\n",
    "        elif tgt_padding_mask is None and tgt_mask is not None:\n",
    "            self_mask = tgt_mask[None,None,:,:]\n",
    "            self_mask = self_mask.expand( (tgt.size(0), self.num_heads, -1, -1 ))\n",
    "        elif tgt_padding_mask is None and tgt_mask is None:\n",
    "            self_mask = None\n",
    "            \n",
    "        if memory_padding_mask is not None:\n",
    "            cross_mask = memory_padding_mask[:,None,None,:]\n",
    "            cross_mask = cross_mask.expand((-1,self.num_heads, tgt.size(1), -1))\n",
    "        else:\n",
    "            cross_mask = None\n",
    "        \n",
    "        # 単語埋め込み [バッチサイズ, 系列長]\n",
    "        # -> [バッチサイズ, 系列長, 埋め込み次元]\n",
    "        embeddings = self.embed(captions) * math.sqrt( self.dim_embedding )\n",
    "        seq = embeddings.shape[1]\n",
    "        \n",
    "        # 位置エンコーディング\n",
    "        #embeddings = self.pos_enc( embeddings )\n",
    "        positions = self.pos_emb(embeddings)\n",
    "        embeddings = embeddings + positions\n",
    "        #embeddings = self.dropout( embeddings )\n",
    "        #embeddings = self.norm(embeddings)\n",
    "        \n",
    "        # Transformerデコーダでキャプション生成\n",
    "        # 画像の特徴も入力する\n",
    "        for layer in self.decoder_layers:\n",
    "            embeddings = layer( embeddings, features, self_mask, cross_mask )\n",
    "\n",
    "        \n",
    "        # [バッチサイズ, 系列長, 埋め込み次元]\n",
    "        # -> [バッチサイズ, 系列長, 辞書の次元]\n",
    "        preds = self.linear(embeddings)\n",
    "        #print( \"argmax of preds:\", torch.argmax( preds, dim = 2 ))\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa9c54fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fTransformerDecoder(nn.Module):\n",
    "    '''\n",
    "    CaptioningTransformerのコンストラクタ\n",
    "    dim_embedding  : 埋め込み次元\n",
    "    dim_feedforward: FNNの中間特徴次元\n",
    "    num_heads      : マルチヘッドアテンションのヘッド数\n",
    "    num_layers     : Transformerデコーダ層の数\n",
    "    vocab_size     : 辞書の次元\n",
    "    pad_index     : NULLのID\n",
    "    dropout        : ドロップアウト確率\n",
    "    '''\n",
    "    def __init__(self, vocab_size: int, dim_embedding: int, dim_feedforward: int,\n",
    "                 num_heads: int, num_layers: int, \n",
    "                 pad_idx: int, dropout: float=0.1, ds_rate: float=0.1 ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 単語埋め込み\n",
    "        #self.embed = nn.Embedding(\n",
    "        #    vocab_size, dim_embedding, padding_idx=pad_index)\n",
    "        \n",
    "        # 位置エンコーディング\n",
    "        #self.pos_enc = PositionalEncoding(dim_embedding)\n",
    "        self.fpos_emb = fPositionalEmbedding(dim_embedding, \"decoder.pos_emb.pos_emb.weight\" )\n",
    "        #self.dropout = nn.Dropout( dropout )\n",
    "        \n",
    "        # Transformerデコーダ\n",
    "        self.fdeclayer = fTransformerDecoderLayer(dim_hidden=dim_embedding, num_heads=num_heads, dim_feedforward = dim_feedforward, dropout = dropout )\n",
    "        \n",
    "        # 単語出力分布計算\n",
    "        #self.linear = nn.Linear(dim_embedding, vocab_size)\n",
    "        \n",
    "        self.pad_idx = pad_idx\n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    ''' CaptioningTransformerの順伝播処理\n",
    "    features: 画像特徴量 [バッチサイズ, 埋め込み次元]\n",
    "    captions: 正解キャプション [バッチサイズ, 系列長]\n",
    "\n",
    "    '''\n",
    "    def forward(self, features: torch.Tensor, captions: torch.Tensor, memory_padding_mask: torch.Tensor=None, \\\n",
    "                tgt_padding_mask: torch.Tensor=None, tgt_mask: torch.Tensor=None, weights = None ):\n",
    "\n",
    "        tgt = captions\n",
    "        if tgt_padding_mask is not None and tgt_mask is not None:\n",
    "            self_mask1 = tgt_padding_mask[:,None,None,:]\n",
    "            self_mask1 = self_mask1.expand( (-1, self.num_heads, tgt.size(1), -1) )\n",
    "            self_mask2 = tgt_mask[None,None,:,:]\n",
    "            self_mask2 = self_mask2.expand( (tgt.size(0), self.num_heads, -1, -1 ))\n",
    "            self_mask = torch.logical_or(self_mask1, self_mask2 )\n",
    "        elif tgt_padding_mask is not None and tgt_mask is None:\n",
    "            self_mask = tgt_padding_mask[:,None,None,:]\n",
    "            self_mask = self_mask.expand( (-1, self.num_heads, tgt.size(1), -1 ) )\n",
    "        elif tgt_padding_mask is None and tgt_mask is not None:\n",
    "            self_mask = tgt_mask[None,None,:,:]\n",
    "            self_mask = self_mask.expand( (tgt.size(0), self.num_heads, -1, -1 ))\n",
    "        elif tgt_padding_mask is None and tgt_mask is None:\n",
    "            self_mask = None\n",
    "\n",
    "        if memory_padding_mask is not None:\n",
    "            cross_mask = memory_padding_mask[:,None,None,:]\n",
    "            cross_mask = cross_mask.expand((-1,self.num_heads, tgt.size(1), -1))\n",
    "        else:\n",
    "            cross_mask = None\n",
    "        \n",
    "        # 単語埋め込み [バッチサイズ, 系列長]\n",
    "        # -> [バッチサイズ, 系列長, 埋め込み次元]\n",
    "        embeddings = F.embedding( captions, weights['decoder.embed.weight'], padding_idx = self.pad_idx )  * math.sqrt( self.dim_embedding )\n",
    "        seq = embeddings.shape[1]\n",
    "        \n",
    "        # 位置エンコーディング\n",
    "        #embeddings = self.pos_enc(embeddings)\n",
    "        positions = self.fpos_emb( embeddings, weights )\n",
    "        embeddings = embeddings + positions\n",
    "        #embeddings = self.dropout( embeddings )\n",
    "        ##embeddings = self.norm(embeddings)\n",
    "        #embeddings = F.layer_norm(embeddings, (self.dim_embedding,), weight=weights['decoder.norm.weight'], bias=weights['decoder.norm.bias'], eps=1e-05)\n",
    "        \n",
    "        # Transformerデコーダでキャプション生成\n",
    "        # 画像の特徴も入力する\n",
    "        #for layer in self.decoder_layers:\n",
    "        #    #embeddings = layer( embeddings, features, tgt_key_padding_mask = padding_mask_tgt, \\\n",
    "        #    #                                memory_key_padding_mask = padding_mask_src, tgt_is_causal = True, tgt_mask = mask_tgt )\n",
    "        for block in range( self.num_layers ):\n",
    "            embeddings = self.fdeclayer(embeddings, features, self_mask, cross_mask, block, weights )\n",
    "  \n",
    "        # [バッチサイズ, 系列長, 埋め込み次元]\n",
    "        # -> [バッチサイズ, 系列長, 辞書の次元]\n",
    "        #preds = self.linear(embeddings)\n",
    "        preds = F.linear( embeddings, weights['decoder.linear.weight'], weights['decoder.linear.bias'] )\n",
    "        #print( \"argmax of preds:\", torch.argmax( preds, dim = 2 ))\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14d1d088-4485-4752-b85b-25197f08824b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim_embedding: int, dim_feedforward: int,\n",
    "                 num_heads: int, num_layers: int, enc_vocab_size: int, dec_vocab_size: int,\n",
    "                 j_pad_index: int,e_pad_index: int, dropout: float=0.1, ds_rate: float=0.1 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = TransformerEncoder(enc_vocab_size, dim_embedding, dim_feedforward, num_heads, num_layers, j_pad_index, dropout )\n",
    "        self.decoder = TransformerDecoder(dec_vocab_size, dim_embedding, dim_feedforward, num_heads, num_layers, e_pad_index, dropout )\n",
    "        self.fencoder = fTransformerEncoder(enc_vocab_size, dim_embedding, dim_feedforward, num_heads, num_layers, j_pad_index, dropout )\n",
    "        self.fdecoder = fTransformerDecoder(dec_vocab_size, dim_embedding, dim_feedforward, num_heads, num_layers, e_pad_index, dropout )\n",
    "        self.j_pad_index = j_pad_index\n",
    "        self.e_pad_index = e_pad_index\n",
    "\n",
    "        self._reset_parameters()\n",
    "        #self._reset_parameters2()\n",
    "    \n",
    "    def _reset_parameters(self):\n",
    "        r\"\"\"Initiate parameters in the transformer model.\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                xavier_uniform_(p)\n",
    "                #xavier_normal_(p)\n",
    "                #kaiming_uniform_(p)\n",
    "                #kaiming_normal_(p)\n",
    "\n",
    "    #def _reset_parameters2(self):\n",
    "    #    for module in self.modules():\n",
    "    #        if isinstance(module, nn.Linear):\n",
    "    #            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    #            if module.bias is not None:\n",
    "    #                nn.init.zeros_(module.bias)\n",
    "    #        elif isinstance(module, nn.Embedding):\n",
    "    #            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    #        elif isinstance(module, nn.LayerNorm):\n",
    "    #            nn.init.zeros_(module.bias)\n",
    "    #            nn.init.ones_(module.weight)\n",
    "        \n",
    "    ''' CaptioningTransformerの順伝播処理\n",
    "    features: 画像特徴量 [バッチサイズ, 埋め込み次元]\n",
    "    captions: 正解キャプション [バッチサイズ, 系列長]\n",
    "\n",
    "    '''\n",
    "    def forward(self, text, dec_input):\n",
    "\n",
    "        seq_len_src = text.shape[1]\n",
    "        seq_len_tgt = dec_input.shape[1]\n",
    "\n",
    "        mask_tgt = nn.Transformer.generate_square_subsequent_mask( seq_len_tgt, dtype=bool ).to(device)\n",
    "        mask_src = torch.zeros((seq_len_src, seq_len_src), device=device).type(torch.bool)\n",
    "\n",
    "        padding_mask_src = (text == idx_list['<pad>'])\n",
    "        padding_mask_tgt = (dec_input == idx_list_en['<pad>'])\n",
    "    \n",
    "        x = self.encoder( text, mask_src, padding_mask_src )\n",
    "        preds = self.decoder(x,dec_input, padding_mask_src, padding_mask_tgt, mask_tgt )\n",
    "\n",
    "        return preds\n",
    "    \n",
    "    def adaptation(self, text, dec_input, weights):\n",
    "\n",
    "        seq_len_src = text.shape[1]\n",
    "        seq_len_tgt = dec_input.shape[1]\n",
    "\n",
    "        mask_tgt = nn.Transformer.generate_square_subsequent_mask( seq_len_tgt, dtype=bool ).to(device)\n",
    "        mask_src = torch.zeros((seq_len_src, seq_len_src), device=device).type(torch.bool)\n",
    "\n",
    "        padding_mask_src = (text == idx_list['<pad>']).to(text.device )\n",
    "        padding_mask_tgt = (dec_input == idx_list_en['<pad>']).to(text.device )\n",
    "        \n",
    "        x = self.fencoder( text, mask_src, padding_mask_src, weights)\n",
    "        preds = self.fdecoder( x, dec_input, padding_mask_src, padding_mask_tgt, mask_tgt, weights )\n",
    "        \n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88539768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Transformer(512, 2048, 8, 6, enc_vocab_size, dec_vocab_size, idx_list['<pad>'], idx_list_en['<pad>'] ).to(device)\n",
    "dim_hidden = 768\n",
    "dim_feedforward = dim_hidden * 4\n",
    "heads = 12\n",
    "layers =8\n",
    "dropout = 0.1\n",
    "clip_max = 1.0\n",
    "#dim_hidden = 256\n",
    "#dim_feedforward = dim_hidden * 4\n",
    "#heads = 4\n",
    "#layers =4\n",
    "#dropout = 0.0\n",
    "model = Transformer(dim_hidden, dim_feedforward, heads, layers, enc_vocab_size, dec_vocab_size, idx_list['<pad>'], idx_list_en['<pad>'], dropout = dropout ).to(device)\n",
    "#model = MyTransformer(768, 768 * 4, 12, 12, enc_vocab_size, dec_vocab_size, idx_list['<pad>'], idx_list_en['<pad>'], start_idx = start_idx_t, max_seq = 200  ).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=idx_list_en['<pad>'])\n",
    "loss_fn = criterion\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b845bbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs1 size: torch.Size([8, 120, 20556])\n",
      "tensor(0.6514, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "outputs2 size: torch.Size([8, 120, 20556])\n",
      "tensor(0.6514, device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): TransformerEncoder(\n",
       "    (embed): Embedding(49528, 768, padding_idx=0)\n",
       "    (pos_emb): PositionalEmbedding(\n",
       "      (pos_emb): Embedding(5000, 768)\n",
       "    )\n",
       "    (encoder_layers): ModuleList(\n",
       "      (0-7): 8 x TransformerEncoderLayer(\n",
       "        (attention): MHA(\n",
       "          (proj_in_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_in_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_in_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_out): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (fnn): FNN(\n",
       "          (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELU(approximate='none')\n",
       "        )\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (embed): Embedding(20556, 768, padding_idx=0)\n",
       "    (pos_emb): PositionalEmbedding(\n",
       "      (pos_emb): Embedding(5000, 768)\n",
       "    )\n",
       "    (decoder_layers): ModuleList(\n",
       "      (0-7): 8 x TransformerDecoderLayer(\n",
       "        (selfattn): MHA(\n",
       "          (proj_in_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_in_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_in_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_out): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (crossattn): MHA(\n",
       "          (proj_in_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_in_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_in_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_out): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (fnn): FNN(\n",
       "          (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELU(approximate='none')\n",
       "        )\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (linear): Linear(in_features=768, out_features=20556, bias=True)\n",
       "  )\n",
       "  (fencoder): fTransformerEncoder(\n",
       "    (fpos_emb): fPositionalEmbedding()\n",
       "    (fenclayer): fTransformerEncoderLayer(\n",
       "      (fattention): feMHA()\n",
       "      (fffn): feFNN(\n",
       "        (activation): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (fdecoder): fTransformerDecoder(\n",
       "    (fpos_emb): fPositionalEmbedding()\n",
       "    (fdeclayer): fTransformerDecoderLayer(\n",
       "      (fselfattn): fdsMHA()\n",
       "      (fcrossattn): fdcMHA()\n",
       "      (fffn): fdFNN(\n",
       "        (activation): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#seed = 0\n",
    "\n",
    "#random.seed(seed)\n",
    "#np.random.seed(seed)\n",
    "#torch.manual_seed(seed)\n",
    "#torch.backends.cudnn.benchmark = False\n",
    "#torch.backends.cudnn.deterministic = True\n",
    "\n",
    "text = torch.randint( 0, enc_vocab_size, size=(8, 100 ))\n",
    "dec_input = torch.randint( 0, dec_vocab_size, size=(1,120))\n",
    "\n",
    "seq_len_src = text.shape[1]\n",
    "seq_len_tgt = dec_input.shape[1]\n",
    "\n",
    "mask_tgt = nn.Transformer.generate_square_subsequent_mask( seq_len_tgt, dtype=bool ).to(device)\n",
    "mask_src = torch.zeros((seq_len_src, seq_len_src), device=device).type(torch.bool)\n",
    "\n",
    "padding_mask_src = (text == idx_list['<pad>'])\n",
    "padding_mask_tgt = (dec_input == idx_list_en['<pad>'])\n",
    "\n",
    "weights = OrderedDict(model.named_parameters())\n",
    "model.eval()\n",
    "outputs1 = model( text.to(device), dec_input.to(device) )\n",
    "print( \"outputs1 size:\", outputs1.size())\n",
    "print( outputs1[0][0][0])\n",
    "\n",
    "#for name in weights:\n",
    "#    print( name )\n",
    "model.eval()\n",
    "outputs2 = model.adaptation( text.to(device), dec_input.to(device), weights )\n",
    "print( \"outputs2 size:\", outputs2.size() )\n",
    "print( outputs2[0][0][0])\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b01dfbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_global_steps: 249500\n",
      "num_warmup_steps: 24950.0\n",
      "Train\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 8.012024048096192e-06\n",
      "pred: the that that that <unk> that the to are <unk> to been <unk> of the <eos>\n",
      "tar:  inflation is hiding the fact that basically we really only have an increase of <unk> <eos>\n",
      "pred: we is been to to the the european of the european <eos>\n",
      "tar:  it has been proved as in the case of the <unk> study that they are highly advantageous for the european economy <eos>\n",
      "epoch:1  Step:2000  loss:5.6745519638  WER:0.8146258503\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: we <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: in is in be the <unk> of in the <unk> of of the the <unk> <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:25  loss:5.6870841408  WER:0.8602708158\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 1.602805611222445e-05\n",
      "pred: the <unk> and to made made and the council and the council of the that the to is and <eos>\n",
      "tar:  mrs <unk> has also recently written to the clubs and the players to ensure that finally this social dialogue can get off the ground <eos>\n",
      "pred: in i is be time to to the <unk> years to of example <unk> on <eos>\n",
      "tar:  finally it will take nine years in the best possible case for the working week of doctors in training to be reduced from 58 hours to 48 <eos>\n",
      "epoch:1  Step:4000  loss:4.8672752380  WER:0.7900207900\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: i problems is problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: in is is be the <unk> the in the <unk> of <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:50  loss:4.9651116371  WER:0.7726384311\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 2.4044088176352706e-05\n",
      "pred: the need to be the <eos>\n",
      "tar:  we have to use them in the best possible way <eos>\n",
      "pred: i would to ask a question question <eos>\n",
      "tar:  i wanted to put a supplementary question in any case <eos>\n",
      "epoch:1  Step:6000  loss:4.3076076508  WER:0.7123287671\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there problems is problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: in is is has a sector industry in the industry of is the the industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:75  loss:4.5824247932  WER:0.8125896710\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 3.2060120240480964e-05\n",
      "pred: we can been made by possible <unk> <eos>\n",
      "tar:  questions have been raised as to whether that is acceptable <eos>\n",
      "pred: i am be a example of my fact that my i am not my my to be a that the the country of is be be be the case as <eos>\n",
      "tar:  i will take an example from the country in which i was elected in order to make clear how in my view it will certainly not in the long term remain just a matter of gathering data <eos>\n",
      "epoch:1  Step:8000  loss:4.3128833771  WER:0.8203389831\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: in is is has a industry industry in the industry industry in the the industry industry industry industry industry industry industry sector industry industry industry industry industry industry industry industry industry industry industry sector industry sector industry industry sector industry industry industry industry industry industry industry industry industry industry sector industry industry industry industry industry industry industry industry industry industry industry industry industry\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:100  loss:4.3342044449  WER:0.7605952927\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 4.007615230460922e-05\n",
      "pred: it is not from difficult to the <unk> regime and be not the and and the to to to <unk> of and he <unk> should not desire should not want to be the <eos>\n",
      "tar:  it is far more logical for the iraqi regime to do as resolution 1441 requires and actually open the gates so that dr blix and his team do not have to virtually <unk> their way through iraq with a <unk> <unk> <eos>\n",
      "pred: i would like to ask the commission that it is been mentioned by the commission for that that not it commission commission that that the is is the of be made and and the the countries and that the is be be be made commission of and the <eos>\n",
      "tar:  i should like to ask the commissioner whether what has been written in the press is true or whether the commission's idea is that this burden of proof should be divided equally among all european countries and that it should not always be the farming sector that <unk> the bill for the association agreement being negotiated with morocco <eos>\n",
      "epoch:1  Step:10000  loss:4.2646532059  WER:0.7950963222\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: it is has has a jobs jobs in the field sector and the the sector <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:125  loss:4.1319609070  WER:0.7545573017\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 4.8092184368737474e-05\n",
      "pred: the question is the important matter important question of problem of view is the importance problem of the important are with the <eos>\n",
      "tar:  my committee saw as a second important point the theme of enlargement and the important issue of how we deal with borders in the first place <eos>\n",
      "pred: the is has played a major role in the the european of the social responsibility for the european union <eos>\n",
      "tar:  this parliament has played a major role in forcing the issue of corporate social responsibility onto the european agenda and <unk> is a very important part of that <eos>\n",
      "epoch:1  Step:12000  loss:3.8309583664  WER:0.8360360360\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there problems problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: in is has has the jobs jobs in the world sector <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:150  loss:3.9690947533  WER:0.7360556887\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 5.6108216432865735e-05\n",
      "pred: all is precisely europe in europe be seen europe for europe <eos>\n",
      "tar:  it is a problem that should be a cause for concern to everyone in europe <eos>\n",
      "pred: if i for i would like to take the commission to take forward this to framework work of the of study programme to the the training and the research and that to take that the the states are and the and a opportunity opportunity for to for the are the the necessary of the opportunities and and and training necessary strategies of and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and\n",
      "tar:  as rapporteur therefore i should like to ask the commission to put in place the necessary mechanisms for drafting a framework directive on reconciling professional family and private lives so as to ensure that all member states current and future have the appropriate tools and responses with which to meet the challenges of equal opportunities economic growth and the demographic challenge <eos>\n",
      "epoch:1  Step:14000  loss:3.9932568073  WER:0.7785419532\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there problems are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this would would creates a jobs jobs in the area industry in the the industry industry industry industry industry industry industry industry industry industry industry industry industry industry industry industry industry industry industry industry industry industry industry industry industry industry industry industry industry industry industry industry industry industry industry industry industry industry industry industry industry industry industry industry industry industry industry industry industry\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:175  loss:3.8366946983  WER:0.7535960646\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 6.412424849699399e-05\n",
      "pred: we is trying to the a <unk> <eos>\n",
      "tar:  everyone is wandering around with a <unk> looking for a <unk> way out <eos>\n",
      "pred: the stockholm summit is a to stockholm stockholm stockholm <eos>\n",
      "tar:  the stockholm summit comes precisely at this decisive moment <eos>\n",
      "epoch:1  Step:16000  loss:3.5416345596  WER:0.7645833333\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems in <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: in would has has jobs jobs jobs in the sector industry in in many industry industry sectors industry industry industry industry industry industry industry industry industry sectors industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:200  loss:3.7371856594  WER:0.7456145789\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 7.214028056112225e-05\n",
      "pred: i believe that must to ensure in the same and that the is no and the european union and the member member union which we not the the of the european and the that we is no the <eos>\n",
      "tar:  i think we need to stay at these levels so that there is harmonisation between the european union and the other european countries that are not yet members of the union and so that there are basically unified regulations at european level <eos>\n",
      "pred: as same of the text text is a the previous speakers of as the number text framework which to extend the activities financial to the the community of to the activities measures of the activities framework of down in the activities on in the of the the the <unk> million to the is to to to the the authorities activities <eos>\n",
      "tar:  the purpose of this new text as with the previous one but using a different legal formula is to grant the legitimate authorities of <unk> a sum equivalent to the unpaid part of the financial compensation laid down in the agreement currently in force which totals around eur <unk> million and which is intended solely for supporting local fishing activities for improving monitoring measures and for reconstructing fisheries infrastructure damaged in the intervening period <eos>\n",
      "epoch:1  Step:18000  loss:3.7382111549  WER:0.9141965679\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there is problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore is is means a jobs jobs in the industry industry and in the industry sector sector industry industry sector industry industry industry sector industry industry sector industry industry sector sector sector industry sector sector industry sector industry sector sector industry industry sector sector sector industry industry industry industry sector industry industry industry industry industry sector sector industry industry sector sector industry sector\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:225  loss:3.6249933624  WER:0.7290353096\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 8.015631262525051e-05\n",
      "pred: the rules standards are airports are too being more <eos>\n",
      "tar:  stricter noise standards for <unk> are also a <unk> point <eos>\n",
      "pred: we way we are against opposed to the a for the to the that in the in the are a problem market market in has developing in the are been been the the last 20 years <eos>\n",
      "tar:  by contrast we are categorically opposed to providing aid in order to ensure security of supply because there is a world energy market which is liquid where prices have not changed for the last twenty years and where the countries of origin the united states australia brazil and south africa are stable <eos>\n",
      "epoch:1  Step:20000  loss:4.0027303696  WER:0.7443868739\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems of <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: in will also has jobs jobs jobs in the construction industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:250  loss:3.4645210075  WER:0.6905378878\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 8.817234468937876e-05\n",
      "pred: this other agencies that agencies that the agencies are not responsible be integrated for the control control control and will be control be a effective body post control of control agencies <eos>\n",
      "tar:  among other things this means that the agencies are now to be responsible for their own financial control there will no longer be an independent ex ante control of the agencies <eos>\n",
      "pred: this this is this america is the same is a a is a very of is a different from other other regions <eos>\n",
      "tar:  because what characterises latin america at the moment is that it is a region which is very different from the other regions of the planet <eos>\n",
      "epoch:1  Step:22000  loss:3.0815820694  WER:0.6230248307\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems that <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this sector has creates jobs jobs jobs in the workplace industry or the the construction industry industry industries industries industry of industry industry of industries in industry in industries in industries industry industry industries in in in industry industries industries in industries industry industry in industries in in industry in in industry in industry in in in industries in industry industry industries in\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:275  loss:3.3885314369  WER:0.7302178124\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 9.618837675350701e-05\n",
      "pred: we we do not accept ourselves to be allowed by the country of such and think not to the - and i believe that yesterday s meeting have been <eos>\n",
      "tar:  similarly we must not allow ourselves to be blackmailed by any one country - i am referring to italy - but i believe that yesterday' s events have changed things and mrs <unk> cannot have known about this last night <eos>\n",
      "pred: mr am however that that mr prodi statement will the stability in the to the stability pact will not in in the work of <eos>\n",
      "tar:  i do recognise however that mr prodi's observations about increased flexibility in relation to the stability pact are not reflected in the work programme not directly at least <eos>\n",
      "epoch:1  Step:24000  loss:3.4224185944  WER:0.6406533575\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: in creates has creates jobs jobs jobs in the south of <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:300  loss:3.2888858700  WER:0.6651588270\n",
      "Train\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 9.95328434647072e-05\n",
      "pred: there on by issues in the environment countries and issue and protection and environmental welfare issues i i believe that we to made made to achieve a on and a as as and as and and and and and as and and and and and and and and and as and and and and\n",
      "tar:  discussions covered many areas affecting the developing countries the environment social affairs and animal protection too and i believe that efforts are being made to find consensus here as well <eos>\n",
      "pred: this is here point here we are talking here here <eos>\n",
      "tar:  it is the stages that we are dealing with here <eos>\n",
      "epoch:2  Step:26000  loss:2.9147946835  WER:0.6290322581\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this creates creates creates more jobs jobs in the workplace industry in the the south industries <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:325  loss:3.2013036442  WER:0.7093184895\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 9.864217323535962e-05\n",
      "pred: mr president i also against mr imbeni report on a as example european union' in a a something about important in terms humanitarian aid in the where are been taken by events disasters and in events of humanitarian <eos>\n",
      "tar:  mr president i voted for mr <unk> report as well for the european union is certainly doing something extremely important in organising humanitarian aid for areas which have been struck by natural disasters or the effects of war <eos>\n",
      "pred: as as as i the opinion as i member of the committee on citizens' i regret to with the where which the for not not received for of be of at was not least rate that late by the though citizens had the eu citizens had not to this to the last to and and lack years had the of crime crime <eos>\n",
      "tar:  time and again in my capacity as a member of the committee on petitions i come up against cases in which compensation was either not paid out to victims or it was at any rate unreasonably delayed where even eu citizens in other eu states had applied for it in the past year alone a dozen petitions from victims of violent crime within the eu have been passed on to us <eos>\n",
      "epoch:2  Step:28000  loss:2.9452486038  WER:0.6173393124\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore region has has jobs jobs jobs in the region industry or the the region valley industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:350  loss:3.0914912701  WER:0.6719153772\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 9.775150300601203e-05\n",
      "pred: the believe the adoption of the nos <unk> and <unk> and <unk> <unk> clear and <unk> more because it would lead lead that the uniform system of the uniform system which the the the of competition <eos>\n",
      "tar:  i believe the adoption of amendments nos 75 and 81 to be quite unjustified and even dangerous because this would basically mean turning a binding system into a voluntary one and consequently increasing distortions of competition reducing the liquidity in the market increasing the price per tonne of carbon dioxide and reducing the economic efficiency of the system <eos>\n",
      "pred: approval of the previous of the previous sitting <eos>\n",
      "tar:  approval of the minutes of the previous sitting <eos>\n",
      "epoch:2  Step:30000  loss:3.2334213257  WER:0.7980295567\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there is some <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this is is helps many jobs jobs in the construction industry and in the construction industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:375  loss:2.9973046780  WER:0.6273365469\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 9.686083277666445e-05\n",
      "pred: citizens countries nationals of can be to they are to in the european union in five least five years have be be that fair commitment to they wish to be able the right to vote <eos>\n",
      "tar:  third country nationals who can prove that they have lived in the european union for at least five years must therefore show a voluntary commitment if they are to be granted the right to vote in local elections <eos>\n",
      "pred: so us therefore turn the <unk> of <eos>\n",
      "tar:  let us therefore have the list shortened as soon as possible rather than extended <eos>\n",
      "epoch:2  Step:32000  loss:2.5804269314  WER:0.5521739130\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: it creates creates creates many jobs jobs in the industry industry in the the shipbuilding industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:400  loss:2.9239490318  WER:0.6232379214\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 9.597016254731685e-05\n",
      "pred: the reality is in the fact that the <unk> <unk> of of by the of the and the countries and europe europe eastern europe <eos>\n",
      "tar:  the explanation lies in the fact that the major <unk> routes used by lorries in france linking mediterranean countries with northern and eastern europe have now reached full capacity <eos>\n",
      "pred: ladies this new new human in human human rights <eos>\n",
      "tar:  is this turkey's new way of handling human rights? <eos>\n",
      "epoch:2  Step:34000  loss:3.0147995949  WER:0.8045375218\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: in is is has many jobs jobs in the west industry and in the south <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:425  loss:2.8628279591  WER:0.6141643520\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 9.507949231796927e-05\n",
      "pred: finally would conclude by last by regard issues as regards services of general interest which is of the heart of the programme to idea of a programme of the idea paper and it a are in idea of a a proposal for a regulation on not been rejected <eos>\n",
      "tar:  i will conclude my speech with two points: as regards services of general interest which are at the centre of our attention the programme mentions a <unk> to the white paper because as things stand the idea of presenting a proposal for a directive has not been excluded but has not yet been decided upon <eos>\n",
      "pred: we we are the members will will a be a great on national of national national parliaments in the historic of historic importance <eos>\n",
      "tar:  as you know honourable members there is to be a convention the involvement of the national parliaments in which is of historic importance <eos>\n",
      "epoch:2  Step:36000  loss:2.8564255238  WER:0.6089965398\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: in creates also creates many jobs jobs in the industry industry and in the <unk> industry industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:450  loss:2.7792474365  WER:0.6037849514\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 9.418882208862169e-05\n",
      "pred: mr mr president ladies and gentlemen we can accept a amendments which of which which fundamental and and if necessary to the amendments to amendment amendments of amendment first of nos 2 and and and and 11 first part <eos>\n",
      "tar:  therefore mr president ladies and gentlemen we can accept six amendments one of them a fundamental issue and if necessary with some modifications to the wording of the text amendments nos 1 5 11 12 and the first sentence of no 15 and of no 18 <eos>\n",
      "pred: the most increase of to the year was been the and the transfer transfer of 200 years has been transferred 200 years <eos>\n",
      "tar:  the largest volume transferred in one year has been <unk> and the average volume over twenty years has been over 200 <eos>\n",
      "epoch:2  Step:38000  loss:2.4848225117  WER:0.5272108844\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there is problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this also also creates many jobs jobs in the processing industry in in the future <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:475  loss:2.6994355392  WER:0.5762563903\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 9.329815185927411e-05\n",
      "pred: questions to commissioner patten <eos>\n",
      "tar:  questions to mr verheugen <eos>\n",
      "pred: as i have there are barriers such as the taxation and lack of legislative and the common language and <eos>\n",
      "tar:  as i said there are barriers such as double taxation the lack of legislation and a common technical vocabulary <eos>\n",
      "epoch:2  Step:40000  loss:2.3762688637  WER:0.5960854093\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there is problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this creates creates creates many jobs jobs in the industrial of or in the <unk> <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:500  loss:2.6535219669  WER:0.5988854106\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 9.240748162992653e-05\n",
      "pred: ludford ludford ludford <eos>\n",
      "tar:  report ludford <unk> <eos>\n",
      "pred: the commission is against to this this point to to order to avoid misunderstandings <eos>\n",
      "tar:  the commission is opposed to following this lead precisely in order to prevent misunderstandings <eos>\n",
      "epoch:2  Step:42000  loss:2.2586760521  WER:0.5063291139\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there is problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: in sector is creates many jobs jobs in the industry industry in the the shipbuilding <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:525  loss:2.6116133976  WER:0.5553745683\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 9.151681140057894e-05\n",
      "pred: this would be to a fundamental of our fundamental principles of our civilisation civilisation of guarantees fundamental procedural which the which by ancient american country a be that on the law <eos>\n",
      "tar:  this would amount to a violation of the fundamental principles of our legal system and of basic rights of trial formulated in latin that is to say based on roman law <eos>\n",
      "pred: or will no further in those is longer makes sense to ask those is responsible of who is and the violence of violence is longer is a right end and a <eos>\n",
      "tar:  there comes a point where it no longer makes sense to ask who is guilty and who innocent because the cycle of violence no longer has a discernible beginning or end <eos>\n",
      "epoch:2  Step:44000  loss:2.5691316128  WER:0.5184534271\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this sector is creates many jobs jobs in the land industry and in the factories industries industries <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:550  loss:2.5417647934  WER:0.5601229771\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 9.062614117123136e-05\n",
      "pred: (applause) <eos>\n",
      "tar:  (applause) <eos>\n",
      "pred: proposals proposals of your own of mr fischler on reform the common fisheries policy or the cap and concerned uncertainty and concern for our fishermen and our <eos>\n",
      "tar:  the proposals from your party colleague mr fischler to reform the common fisheries policy or the cap are generating uncertainty and concern among our fishermen and farmers <eos>\n",
      "epoch:2  Step:46000  loss:2.3148965836  WER:0.4945295405\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there is problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this also also creates many jobs jobs in the processing industry or in the shipbuilding industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:575  loss:2.5145520878  WER:0.5486789722\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 8.973547094188376e-05\n",
      "pred: i would insist that the the objective of of this coordination coordination is be to strengthen the disparities between the central and of the outermost and the outermost regions outermost regions as indicated in my of the amendments adopted by my committee <eos>\n",
      "tar:  i must stress particularly that the priority objective of this increased coordination should be to reduce structural divergences between the central part of the union and the peripheral and outermost regions as specified in one of the amendments adopted by my committee <eos>\n",
      "pred: in this sense as has will be the european parliament is currently on the intergovernmental to create the common diplomatic corps of the community of is clearly a clearly to create the diplomatic corps of the member states but to try the services to to their coordination and\n",
      "tar:  in this respect as you will know the european parliament is working on an initiative to form a common diplomatic corps of the community this is clearly not intended to replace the diplomatic services of the member states but to complement these and possibly improve their coordination <eos>\n",
      "epoch:2  Step:48000  loss:2.4015455246  WER:0.4875239923\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore also also creates many jobs jobs in the <unk> industry and in the <unk> <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:600  loss:2.4811144352  WER:0.5535988015\n",
      "Train\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 8.88448007125362e-05\n",
      "pred: i this respect i would request on the to support mr <unk> report and to the <unk> <unk> and to leave it <eos>\n",
      "tar:  in this sense i urgently call on you to support mr swoboda's report or given the present weather not to leave him out in the rain <eos>\n",
      "pred: the vote will take place tomorrow at 12 noon <eos>\n",
      "tar:  the vote will take place tomorrow at 12 noon <eos>\n",
      "epoch:3  Step:50000  loss:1.9094762802  WER:0.4402515723\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this sector creates creates many jobs jobs in the industry industry and in the shipbuilding <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:625  loss:2.4453050804  WER:0.5383539460\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 8.79541304831886e-05\n",
      "pred: each every country has its its own share of scandals and we know that we is necessary to do in carefully and rigorously <eos>\n",
      "tar:  virtually every country has had its fair share of scandals and we know that it is necessary to act very strictly and monitor closely <eos>\n",
      "pred: i hope hope that members will aware of the fact that this proposal proposal proposed in directive on the substances electrical in the and and electronic - and the proposal for a directive on the use of the use of electronic substances substances and electronic and electronic equipment <eos>\n",
      "tar:  i also hope that meps are aware of the fact that the commission has proposed a ban on these three substances - <unk> <unk> and <unk> - in our proposal for a directive on the restriction of the use of certain hazardous substances in electrical and electronic equipment <eos>\n",
      "epoch:3  Step:52000  loss:1.8438407183  WER:0.4150943396\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: yes there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this also also creates many jobs jobs in the <unk> industry or in the shipbuilding or <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:650  loss:2.4316828442  WER:0.5373774642\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 8.706346025384102e-05\n",
      "pred: we is a <eos>\n",
      "tar:  it is schizophrenic <eos>\n",
      "pred: a positive sign of is the the countries are ratify be the convention convention of of child labour <eos>\n",
      "tar:  a positive sign recently is that other countries will probably ratify the ilo worst forms of child labour convention no 182 <eos>\n",
      "epoch:3  Step:54000  loss:2.3527648449  WER:0.5899814471\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates a jobs jobs in the industry industry and in the construction industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:675  loss:2.4080480671  WER:0.5224232164\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 8.617279002449343e-05\n",
      "pred: the measures must be taken in a procedures procedures and the for for of charge <eos>\n",
      "tar:  these measures must be concomitant with simplifying administrative procedures and providing documents free of charge <eos>\n",
      "pred: madam president commissioner whenever i discuss tobacco tobacco of tobacco i am wonder whether if we are against against we we we need to be against <eos>\n",
      "tar:  madam president commissioner whenever we debate the question of tobacco i always wonder even if we are all anti-smoking why do we need to be <unk> <eos>\n",
      "epoch:3  Step:56000  loss:1.9017190933  WER:0.4522522523\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also employs jobs jobs jobs in the shipbuilding industry or in the building industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:700  loss:2.3822935867  WER:0.5236131305\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 8.528211979514585e-05\n",
      "pred: i am sure there there are be links connections in europe which can arise called <unk> <unk> <eos>\n",
      "tar:  i am sure that there must be similar operations in europe that might be termed 'social <unk> <eos>\n",
      "pred: thank you mr olsson <eos>\n",
      "tar:  thank you mr olsson <eos>\n",
      "epoch:3  Step:58000  loss:2.2453160286  WER:0.4673366834\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are certain <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also generates jobs jobs jobs in the industry industry and in the building industry industries <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:725  loss:2.3633499813  WER:0.5241467422\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 8.439144956579827e-05\n",
      "pred: when he awarded the nobel prize peace prize i also a to <unk> to whom to which as also said that that as the provider of the 50% of income income income i would i to congratulate the more contracts to the and i like like that we partnership will be renewed in the partnership in which we partnership partnership <eos>\n",
      "tar:  when <unk> received their <unk> nobel peace prize i sent them a <unk> of congratulations in which i also told them that as the provider of about 50% of their economic basis i felt entitled to extend even warmer congratulations to them and would also hope that our partnership could be reflected in the way in which <unk> present themselves to the public <eos>\n",
      "pred: i use against have against the council are to issues of transparency in legal only the legal of legal opinions and not not only of rights of citizens to the to to the out democratically scrutiny but the european of the european but well are also to the but also to work of the european of of the the the the the the the\n",
      "tar:  the proceedings i instigated against the council relate to issues of transparency including not least the secrecy of legal opinions and concern not only the rights of citizens to access documents to carry out democratic scrutiny of the behaviour of the institutions as they are entitled to do but also the rights of the members <eos>\n",
      "epoch:3  Step:60000  loss:2.0987868309  WER:0.4945226917\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this sector also creates many jobs jobs on the industry industry in in the manufacture industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:750  loss:2.3210516500  WER:0.5156205055\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 8.350077933645069e-05\n",
      "pred: mr president i welcome this document and express support the decision parliament' and the decision and promote and and quality evaluation and school <eos>\n",
      "tar:  mr president i welcome this document and fully support the european parliament and council decision to promote cooperation and quality evaluation in school education <eos>\n",
      "pred: i evans i evans i and i congratulate on his report - and in unanimous in he said that it is an matter important subject in it is also an that the relating to competition issue of in competition field of competition are not reflect the in other world of the interest <eos>\n",
      "tar:  mr president mr evans - whom i congratulate on his report - was absolutely right when he said that this is a very important issue although it is also true that questions relating to the reform procedure in the field of competition do not inspire interest amongst the citizens despite their importance for the economic world <eos>\n",
      "epoch:3  Step:62000  loss:2.1835439205  WER:0.6071428571\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: moreover sector also has many jobs jobs in the industry industry or in the yards industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:775  loss:2.3240585804  WER:0.5209933020\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 8.26101091071031e-05\n",
      "pred: for the first time in our continent we are creating another new and powerful and the free and of nations nations sovereign nations and on the basis of this values which under time under the risk of the weapons of the the moment of weapons gun <eos>\n",
      "tar:  for the first time on our continent we are creating something powerful and new through the free will of free and sovereign peoples and on the basis of shared values not this time at the point of a sword or at the barrel of a gun <eos>\n",
      "pred: mr mulder has tabled an supplementary amendment to i would like to to bourlanges to accept the the <unk> amendment amendment which an separate to which amendment which that you do not have to choose between two two <eos>\n",
      "tar:  mr mulder has tabled a sound amendment and i should like you mr bourlanges to consider adopting mr <unk> s amendment as a supplement to your amendment so that we do not need to choose between the two <eos>\n",
      "epoch:3  Step:64000  loss:1.9905415773  WER:0.5009487666\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this sector creates creates many employment jobs in the processing industry or in the processing industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:800  loss:2.2910455894  WER:0.5025415948\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 8.171943887775552e-05\n",
      "pred: this is a major failure of the commission discussions between the commission and parliament <eos>\n",
      "tar:  that represented a fundamental failure in the early discussions between the commission and parliament <eos>\n",
      "pred: i would two questions to say commission and i hope that when mr liikanen comes me will be me specifically as what question nos 21 nos 21 21 28 <eos>\n",
      "tar:  i have two questions to the commission and i hope that when mr liikanen answers he will address himself specifically to my points on amendments nos 21 and 28 <eos>\n",
      "epoch:3  Step:66000  loss:1.8111327887  WER:0.4273318872\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this also also creates many jobs jobs in the processing industry or in the construction industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:825  loss:2.2696478367  WER:0.4969550408\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 8.082876864840792e-05\n",
      "pred: some would some who would like codecision to be decided by parliament with parliament parliament and would also who would it would be decided on by the commission on the proposal from the commission with the opinion of this parliament <eos>\n",
      "tar:  there are those who would like it to be determined by codecision involving this parliament there are those who think it should be decided ultimately by the council on a proposal by the commission with the assent of this parliament <eos>\n",
      "pred: this particular type of slavery is forced and and this is the case that increases growing faster quickly <eos>\n",
      "tar:  this particular form of trafficking is forced labour and it is the one that is growing most rapidly <eos>\n",
      "epoch:3  Step:68000  loss:1.9245271683  WER:0.4138576779\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this sector also creates many jobs jobs in the processing industry or in the processing industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:850  loss:2.2621247482  WER:0.5029125033\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 7.993809841906034e-05\n",
      "pred: the member member states must to do is agree to the draft prepared the out of the convention and the up the the idea idea of <unk> or or <unk> <unk> it back from <eos>\n",
      "tar:  all the member states have to do is simply accept the project that came out of the convention and give up on the damaging idea of <unk> it or of pulling it apart <eos>\n",
      "pred: i i the face and always noble way of my honourable friend and <unk> noble since the the <unk> <unk> of the labour members recommend the a the way way of <eos>\n",
      "tar:  so in the distinguished and ever noble presence of my honourable friend and <unk> ever noble despite the constitutional machinations of new labour i commend as ever the middle way <eos>\n",
      "epoch:3  Step:70000  loss:2.0127911568  WER:0.4423407917\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this sector also creates many jobs jobs on the industry industry and in the transformation industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:875  loss:2.2428254223  WER:0.4883738505\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 7.904742818971276e-05\n",
      "pred: the degree of good that officials officials servants is worrying <eos>\n",
      "tar:  the lack of motivation amongst excellent civil servants is serious <eos>\n",
      "pred: it is a old <unk> europe is a europe – that of the <unk> – which basically 000 ago – the the same reasons for to cut down the line line <eos>\n",
      "tar:  this is an old <unk> there is a europe – that of mrs sandbæk – which 60 years ago for substantially the same reasons wanted to knock down the <unk> line <eos>\n",
      "epoch:3  Step:72000  loss:2.2170958519  WER:0.4351395731\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: in sector also creates many jobs jobs on the processing industry or in the <unk> industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:900  loss:2.2300232887  WER:0.4950733304\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 7.815675796036518e-05\n",
      "pred: in there differences have detected the rapporteur has analysed at what the eib should be the or less priority <eos>\n",
      "tar:  where significant differences were found the rapporteur has looked at what the eib should give greater or less priority to <eos>\n",
      "pred: i have tabled some amendments to the report both to correct some errors or to to some in committee but these amendments not detract from the importance and relevance of the report <eos>\n",
      "tar:  i have tabled some amendments to this report either to correct minor errors or owing to setbacks in committee but these do not detract from the importance and significance of the report <eos>\n",
      "epoch:3  Step:74000  loss:2.1391487122  WER:0.4682926829\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this sector also creates many jobs jobs in the processing industry or in factories processing industries industries <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:925  loss:2.2037631607  WER:0.4862511108\n",
      "Train\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 7.726608773101759e-05\n",
      "pred: the commission proposal sets than the targets targets targets for recycling targets <unk> targets is the and the the minimum targets for the because a whole ten 10% <eos>\n",
      "tar:  the commission proposal more than <unk> the minimum target for recycling by fixing it at 55% and raises the minimum target for recovery as a whole by 10% to 60% <eos>\n",
      "pred: what would be be to a follow-up of the this assessment and the <unk> <eos>\n",
      "tar:  what would you recommend as a result of all this assessment and forthcoming <unk> <eos>\n",
      "epoch:4  Step:76000  loss:1.7656397820  WER:0.4234592445\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this sector is creates many jobs jobs on the industry industry or in the construction industry industry industry industry industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:950  loss:2.1962904787  WER:0.4794805682\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 7.637541750167001e-05\n",
      "pred: the the swedish to swedish affairs council adopted held new in in new with the turkish caucasus in be be in the dialogue with the the and in in and the in and <eos>\n",
      "tar:  after the visit the general affairs council also adopted conclusions consequently the dialogue on the southern caucasus will regularly continue in high-level meetings with turkey <eos>\n",
      "pred: in win a war on terror across borders is need to achieve up cross-border across national <eos>\n",
      "tar:  to win a war against terror across borders we need to build a consensus across borders <eos>\n",
      "epoch:4  Step:78000  loss:1.8500566483  WER:0.5925925926\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: in sector also creates many jobs jobs in the industry industry and in the shipyards industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:975  loss:2.2100480270  WER:0.4842333564\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 7.548474727232243e-05\n",
      "pred: maastricht maastricht we was were in maastricht was the with the of on the commitments taken by the member states of are the union union member <eos>\n",
      "tar:  at maastricht what we put in place was compliance with obligations dependent on voluntary decisions adopted by the member states which form the european union <eos>\n",
      "pred: on a than one occasion i was the to the rapporteur mr brok to the conference of presidents <eos>\n",
      "tar:  on more than one occasion it was presented by the rapporteur mr brok to the conference of presidents <eos>\n",
      "epoch:4  Step:80000  loss:1.7351545095  WER:0.4060822898\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this sector also creates many jobs jobs in the processing industry or in the building industry industry industry industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:1000  loss:2.1869013977  WER:0.4832042146\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 7.459407704297483e-05\n",
      "pred: mr president the recent years the organisations and europe outside of have been up high international <unk> which make such profits for both example women in women and drugs <eos>\n",
      "tar:  mr president in recent years criminal organisations within and outside europe have set up powerful international networks which make huge profits from for example trafficking in women or drugs <eos>\n",
      "pred: while if the is true case that this has a draft statute on a statute on 3 december 1998 which is to say before the treaty of amsterdam came into force then has had been the new guidelines in the the committee on legal affairs and the internal market <eos>\n",
      "tar:  even if it is the case that parliament adopted a draft proposal for a statute on 3 december 1998 that is to say before the treaty of amsterdam came into force parliament has since debated the new guidelines namely in the committee on legal affairs and the internal market <eos>\n",
      "epoch:4  Step:82000  loss:1.5793761015  WER:0.3790186125\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this area also creates many jobs jobs in the processing industry and in the construction industries industries industries <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:1025  loss:2.1808184433  WER:0.4837694644\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 7.370340681362727e-05\n",
      "pred: in is also to addition view right to it in of basis for the that on the resources such as agriculture and fisheries fishing industry <eos>\n",
      "tar:  it is important in its own right but also forms the basis for activities based on natural resources such as agriculture and the fishing industry <eos>\n",
      "pred: this applies the of for the directive directive and of directive directive which they call require that revision of the and regional planning policies and are be pollution and the of natural habitats <eos>\n",
      "tar:  this is true both of the habitats directive and the nitrates directive since they both require the review of agricultural and regional planning policies which may cause pollution or destruction of natural environments <eos>\n",
      "epoch:4  Step:84000  loss:1.9081877470  WER:0.4424460432\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates many jobs jobs on the textile industry or the the south industry industry industry industry industry industry industry industry industry industry industry industry industry industry industry industry industry industry industry industry industry industry industry sector industry industry industry industry industry industry industry industry industry industry industry industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:1050  loss:2.1721007299  WER:0.4931426285\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 7.281273658427967e-05\n",
      "pred: finally i would like to ask the european commission in mrs wallström when she will present submitting the batteries directive <eos>\n",
      "tar:  finally i should like to ask the european commission namely mrs wallström when she will be reviewing the batteries directive <eos>\n",
      "pred: i wonder whether it has also been possible to israel's on israel to withdraw the the settlements because this would also the situation and send give a clear signal that the be the terrorists out of the terrorists of the terrorists <eos>\n",
      "tar:  i wonder whether pressure has also been brought to bear on israel to start leaving the settlements for that could defuse the situation and could be a clear signal that could take the wind out of the sails of the terrorists <eos>\n",
      "epoch:4  Step:86000  loss:1.4650626183  WER:0.3321678322\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: yes there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates many employment jobs on the processing industry or in works works industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:1075  loss:2.1541360569  WER:0.4691369236\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 7.19220663549321e-05\n",
      "pred: i think that the answer can clear and concise <eos>\n",
      "tar:  i think that the answer is brief and clear <eos>\n",
      "pred: we will also be that the that regards heading 5 the the european union administrative to act is guaranteed guaranteed in it comes to administrative expenditure <eos>\n",
      "tar:  we shall also ensure in future as regards heading 5 that the european union's ability to act is still guaranteed when it comes to administrative expenses <eos>\n",
      "epoch:4  Step:88000  loss:1.9803247452  WER:0.4448979592\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: in sector also creates many jobs jobs in the processing industry or in the shipyards industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:1100  loss:2.1479027891  WER:0.4752048691\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 7.10313961255845e-05\n",
      "pred: the current epidemic is not but <unk> <eos>\n",
      "tar:  the present epidemic is anything but restricted <eos>\n",
      "pred: the commission and the council have taken three length three paths paths to be which the own changing changing ideas ideas and strategy strategy <eos>\n",
      "tar:  the commission and the council have considered at least three hypothetical paths to take in their time often changing their minds and their strategies <eos>\n",
      "epoch:4  Step:90000  loss:1.7010079622  WER:0.3755868545\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this sector also creates many jobs jobs in the processing industry or in the building industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:1125  loss:2.1478546858  WER:0.4828605547\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 7.014072589623692e-05\n",
      "pred: the is is to see this debate on thursday agenda report at the held in the end of the agenda <eos>\n",
      "tar:  this group wishes to have a debate on the maaten report to be included at the end of thursday's agenda <eos>\n",
      "pred: in is actually fact fact very simple and there there have a decision on this <eos>\n",
      "tar:  it is in actual fact very simple and fortunately we have a decision on this matter <eos>\n",
      "epoch:4  Step:92000  loss:2.0031397343  WER:0.4537205082\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this sector also creates many jobs jobs in the processing industry or in the building industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:1150  loss:2.1320814133  WER:0.4669439099\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 6.925005566688934e-05\n",
      "pred: thanks new rules is europe a high level of health consumer protection environmental protection <eos>\n",
      "tar:  the new regulation gives europe a high standard of health consumer and environmental protection <eos>\n",
      "pred: the unprecedented failure in public opinion in certainly doubt been the easier for the majority majority of world governments of the world to take a position on the position of with the governments of the world' of the south and in particular the non-aligned countries which is represents two thirds of the un nations general assembly <eos>\n",
      "tar:  this unprecedented explosion of public opinion has no doubt made it easier for a large majority of the governments of the world to take a stand against the war starting with the governments of the countries of the south and in particular the non-aligned movement which alone represents two thirds of the united nations general assembly <eos>\n",
      "epoch:4  Step:94000  loss:2.0049102306  WER:0.4761904762\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this sector also creates many jobs jobs in the processing industry or in processing construction industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:1175  loss:2.1098144054  WER:0.4697098980\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 6.835938543754176e-05\n",
      "pred: there there moment time is absolutely tangible that allows allow us to debate into question the the state or the airline or professionalism or the airline airline – – or behaviour of the egyptian authorities aviation authority or the technical of the technical control that have carried out by the occasion <eos>\n",
      "tar:  for the present there is nothing tangible that would allow us to call into question either the condition of the aircraft the professionalism of the egyptian charter company <unk> the <unk> of the egyptian civil aviation authority or the reliability of the technical controls that were carried out on that aircraft <eos>\n",
      "pred: as the rapporteur has rightly in rightly following this convergence to convergence with means of certain common measures measures such measures such be to be taken such as those example those relating to <unk> dues <eos>\n",
      "tar:  as the rapporteur has said quite rightly following this attempt at convergence by means of certain common standardisation measures other measures will have to be adopted such as for example those relating to dock terminals <eos>\n",
      "epoch:4  Step:96000  loss:2.0556962490  WER:0.5107142857\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: in sector also creates a jobs jobs on the <unk> industry and in the construction industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:1200  loss:2.1108153343  WER:0.4624824440\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 6.746871520819417e-05\n",
      "pred: we believe the an regard for the united to it need research find the research for a way manner <eos>\n",
      "tar:  we have such high regard for the us that we need to make this appeal in a proper manner <eos>\n",
      "pred: we will firmly support those those important important amendments tabled those those tabled by our group seek to establish a preferential of preferential preferential for young farmers to a view to facilitating the to new generations to productive <eos>\n",
      "tar:  we will firmly support all the genuinely important amendments which like those tabled by our group seek to establish a system of special benefits for young farmers with a view to facilitating access for new generations to production activities <eos>\n",
      "epoch:4  Step:98000  loss:2.2778029442  WER:0.4644549763\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates many jobs jobs on the <unk> industry or in the processing industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:1225  loss:2.0978738880  WER:0.4758823671\n",
      "Train\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 6.657804497884659e-05\n",
      "pred: consequently voting for favour of this resolution we are send sending a a positive signal to a european parliament that is always the with the citizens <eos>\n",
      "tar:  by voting in favour of this resolution we will be sending out a positive signal from a european parliament which is in tune with the citizens <eos>\n",
      "pred: if the legislator of the legislator may be to create the framework for research the is a risk of it will be too <unk> <eos>\n",
      "tar:  although the task of the legislator may be to provide a framework for research there is a risk that this will be too restricted <eos>\n",
      "epoch:5  Step:100000  loss:1.4563369751  WER:0.3333333333\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: in sector also creates many jobs jobs on the textile industry or in the construction industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:5  Step:1250  loss:2.1001781702  WER:0.4555378719\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 6.5687374749499e-05\n",
      "pred: i reply to mrs <unk> i would like to we are go the researchers in question they lines after out after after after september 2003 <eos>\n",
      "tar:  in response to mrs thors i would say that you can ask the researchers in <unk> three lines turned up there sometime after september 2003 <eos>\n",
      "pred: moreover does the right to given for all citizens citizens irrespective of their ethnic origin <eos>\n",
      "tar:  nor has the right been safeguarded for all turkish citizens irrespective of their ethnic origin to enjoy full cultural rights <eos>\n",
      "epoch:5  Step:102000  loss:1.5973647833  WER:0.3706422018\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: in sector also creates a jobs jobs in the textile of and in the yards industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:5  Step:1275  loss:2.1227506495  WER:0.4646791122\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 6.479670452015141e-05\n",
      "pred: the nice of nice does not create a divided union union and larger member states can dictate the changes of change in the union to the expense of smaller smaller countries <eos>\n",
      "tar:  the treaty of nice does not build a two-tier european union whereby larger member states can dictate the pace of change within the union at the expense of the smaller states <eos>\n",
      "pred: mr president commissioner ladies and gentlemen after all has already said already i will be impossible for me to say anything <eos>\n",
      "tar:  mr president commissioner ladies and gentlemen after what has been said already it will be impossible for me to be original <eos>\n",
      "epoch:5  Step:104000  loss:1.6209226847  WER:0.4000000000\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: yes there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this sector also creates many jobs jobs on the <unk> industry or in the building industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:5  Step:1300  loss:2.1142011118  WER:0.4611761785\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 6.390603429080383e-05\n",
      "pred: i will be on you in and i will i answer the which the same time <eos>\n",
      "tar:  i will speak of this briefly if you wish to ask questions at the appropriate time i will be pleased to answer them <eos>\n",
      "pred: we must support initiatives to the availability and improving the management and management of water water water scarce water resources <eos>\n",
      "tar:  we must support initiatives increasing the availability and improving the distribution and management of the region's very scarce water resources <eos>\n",
      "epoch:5  Step:106000  loss:1.4570813179  WER:0.3925925926\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this sector also generates a jobs jobs in the industry industry or in the yards industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:5  Step:1325  loss:2.1264880848  WER:0.4700648417\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 6.301536406145625e-05\n",
      "pred: the second is is to present to the council and parliament what we call a scoreboard table for the action plan on drugs <eos>\n",
      "tar:  the second one is to put before the council and parliament what we call a follow-up table for the action plan on drugs <eos>\n",
      "pred: there have a long long way to go before we free-trade zone is set <eos>\n",
      "tar:  we have a very long way to go before a free-trade area is established <eos>\n",
      "epoch:5  Step:108000  loss:1.2895312309  WER:0.3294797688\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: in sector also creates many jobs jobs in the processing industry or in the building industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:5  Step:1350  loss:2.1232788706  WER:0.4676229211\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 6.212469383210866e-05\n",
      "pred: there emphasis on transport and and fisheries policy emphasised out <eos>\n",
      "tar:  the policies on transport agriculture and fisheries were singled out <eos>\n",
      "pred: - (sv) we have abstained from voting on the <unk> s report on harassment in the workplace <eos>\n",
      "tar:  - (sv) we have abstained from voting on mr <unk> s report on harassment at the workplace <eos>\n",
      "epoch:5  Step:110000  loss:1.5060167313  WER:0.3778705637\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: it industry also creates a jobs jobs in the <unk> industry or in the construction of <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:5  Step:1375  loss:2.1009523201  WER:0.4612493038\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 6.123402360276108e-05\n",
      "pred: <eos>\n",
      "tar:  on amendment no 1 <eos>\n",
      "pred: the council is not in of the idea of establishing up a secretariat to the purpose purpose of creating water fund but perhaps it the could ask the council about this fund i is i to tell the much is the fund <eos>\n",
      "tar:  the council is not fond of the idea of setting up a secretariat for the particular purpose of a water fund - although perhaps you should ask the council about this because who am i to say how it discusses the matter? <eos>\n",
      "epoch:5  Step:112000  loss:1.9070750475  WER:0.4409937888\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: in sector also creates a jobs jobs in the processing industry or in the renovation processing <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:5  Step:1400  loss:2.0950949955  WER:0.4638946766\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 6.034335337341349e-05\n",
      "pred: what practical measures could be taken to would enable implemented to persuade these countries countries the aim of negotiations negotiations was not to than just the liberalisation <eos>\n",
      "tar:  what practical measures can be implemented which would be likely to convince the acp that the objective of the negotiations is about more than just trade <unk> <eos>\n",
      "pred: i point is that before the point - and i propose immediately that - we must to go the statements and verbal condemnation and the so concrete firstly first thing we need to do is we that is the and we to immediately all the from the european union and israel and from flows arms and it is to be a very large network of arms of between israel and the countries of the industries in the european union <eos>\n",
      "tar:  my problem is that at some point - and i would suggest immediately - we need to stop making declarations and verbal accusations and do something and the first thing we need to do - and do at once - is to cut all funding from the european union to israel and all supply lines because there appears to be a fairly dense network of supply lines between israel and the countries and arms industries of the european union <eos>\n",
      "epoch:5  Step:114000  loss:1.8054248095  WER:0.4196891192\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: in sector also creates a jobs jobs in the shipbuilding industry and in the yards industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:5  Step:1425  loss:2.0993851519  WER:0.4621143913\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 5.945268314406591e-05\n",
      "pred: mr mr president ladies and gentlemen i should like to thank robert robert evans for his report <eos>\n",
      "tar:  – mr president ladies and gentlemen i would like to thank mr robert evans for his report <eos>\n",
      "pred: it is the result of the campaign and waged by nationalist and left-wing <unk> <unk> by by the <unk> and former danish communists who have always opposed european cooperation <eos>\n",
      "tar:  that is the fruit of the anti-european campaign run by nationalists and left-wing <unk> and supported by reactionary <unk> and former danish communists who have always opposed european cooperation <eos>\n",
      "epoch:5  Step:116000  loss:1.5251971483  WER:0.3497536946\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are some <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: in sector is creates many jobs jobs in the processing industry or in the yards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:5  Step:1450  loss:2.0969088221  WER:0.4586007670\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 5.856201291471833e-05\n",
      "pred: my said to i was a love to see me that see out a medical who am that <eos>\n",
      "tar:  she said that it was my job to convince him to go to a doctor i did that job and the early stages of ms were first of all diagnosed <eos>\n",
      "pred: the approach of the british government on regard to the contiguous was approved by the european commission <eos>\n",
      "tar:  the approach of the uk government with respect to the <unk> was approved by the european commission <eos>\n",
      "epoch:5  Step:118000  loss:1.8513778448  WER:0.4470842333\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates many jobs jobs on the processing industry in in shipbuilding construction of <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:5  Step:1475  loss:2.0732729721  WER:0.4649810038\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 5.7671342685370746e-05\n",
      "pred: in europe legislation is stricter and the is obliged to have 25% of ships ships vessels which to the directive directive directive <eos>\n",
      "tar:  in europe legislation is stricter and it is compulsory to inspect 25% of the incoming vessels according to the <unk> control directive <eos>\n",
      "pred: the current retirement on retirement and surviving <unk> conflict are with the principle of equal treatment and equality treatment <eos>\n",
      "tar:  the current rules on old-age and surviving <unk> insurance conflict with the principle of equal treatment and equal status <eos>\n",
      "epoch:5  Step:120000  loss:1.7324095964  WER:0.3754578755\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: in sector creates creates many jobs jobs in the processing industry and in the building industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:5  Step:1500  loss:2.0745541525  WER:0.4653964447\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 5.6780672456023166e-05\n",
      "pred: i would like to say out that following misunderstandings were been the the name being taken in the greens in approve the compromise of the compromise amendments but i have not them on the on on on on on on on on on on on on on on on on on on on on on on on on on on on on on\n",
      "tar:  i would like to point out that these misunderstandings have led to my name being used by the greens to support the content of the compromise motions although i had signed nothing for them <eos>\n",
      "pred: i would like pleased if this house were be it so procedure so that we can have good cooperation with this important part of europe <eos>\n",
      "tar:  i should be pleased if this house would support this urgent matter so that we can achieve good cooperation with this important part of europe <eos>\n",
      "epoch:5  Step:122000  loss:1.4945657253  WER:0.4405458090\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: in sector also creates a jobs jobs in the processing industry or in the processing industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:5  Step:1525  loss:2.0673544025  WER:0.4605840508\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 5.589000222667557e-05\n",
      "pred: the report <unk> the lack of any resources particularly helicopters to a main why the lack of any aid in general and the european of resources from the european institutions particularly particular when the with the floods in mozambique when if if there is not - there is no shortage of helicopters helicopters helicopters time of infrastructure of and no helicopters of helicopters at the comes to the in yugoslavia and iraq <eos>\n",
      "tar:  the report cites the absence of operational tools especially helicopters as the reason for the lack of international aid in general and the lack of aid from the european institutions in particular in dealing with the floods in mozambique whereas - and who could <unk> - there was no lack of helicopters aeroplanes every type of transportation equipment and even means of destruction when it came to intervening in yugoslavia or iraq <eos>\n",
      "pred: i can assure you that the presidency sees this as as one a highest priority <eos>\n",
      "tar:  i can assure you that the presidency regards this matter as having the utmost priority <eos>\n",
      "epoch:5  Step:124000  loss:1.7252614498  WER:0.3945686901\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates many jobs jobs in the processing industry and in the shipyards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:5  Step:1550  loss:2.0604583025  WER:0.4667498128\n",
      "Train\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 5.4999331997328e-05\n",
      "pred: the addition the compromise compromise addresses the issue of interoperability and other number of technical issues such as the and and improvement cycle improvement <eos>\n",
      "tar:  in addition the proposed compromise addresses the issue of labelling and a number of technical issues such as <unk> <unk> and test cycle improvements <eos>\n",
      "pred: the report highlights the need to apply systematic use of gender budgeting in a view to incorporating the gender perspective into community and national budget procedures <eos>\n",
      "tar:  this report emphasises the need to make systematic use of gender budgeting with a view to incorporating the gender perspective into community and national budgetary procedures <eos>\n",
      "epoch:6  Step:126000  loss:1.0821124315  WER:0.2604651163\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this also also creates a jobs jobs on the processing industry or in the building <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:6  Step:1575  loss:2.0918579197  WER:0.4627845587\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 5.4108661767980405e-05\n",
      "pred: the americans must to look at their they are doing before they can the finger at others else <eos>\n",
      "tar:  the americans need to look at what they are doing before they point a finger at anyone else <eos>\n",
      "pred: in are the trade trade are indeed a agents <eos>\n",
      "tar:  brokers in the arms trade are quite unique agents <eos>\n",
      "epoch:6  Step:128000  loss:1.6732009649  WER:0.3736951983\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this sector also creates many job jobs in the building industry and in the building construction <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:6  Step:1600  loss:2.1137146425  WER:0.4624725059\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 5.3217991538632825e-05\n",
      "pred: with the point of view of human rights it is also worrying that the accused are still to before the of trial and that virtually almost all exception have conducted secret <eos>\n",
      "tar:  from the point of view of human rights it is also worrying that those accused are considered guilty before commencement of trial and that trials almost without exception are declared secret <eos>\n",
      "pred: this is my main objection at this moment <eos>\n",
      "tar:  that is my main objection at the moment <eos>\n",
      "epoch:6  Step:130000  loss:1.2878068686  WER:0.3036437247\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: moreover also also generates many jobs jobs in the <unk> industry and in the construction construction <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:6  Step:1625  loss:2.0977883053  WER:0.4514697964\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 5.232732130928524e-05\n",
      "pred: he must respond able accountable for their they feed their themselves own not only to safeguard their economic economic return but but is an significant which must not overlook sight of it also to the the <eos>\n",
      "tar:  they must be held accountable for what they feed to their animals not only to secure their own economic success <unk> this is a factor we should not lose sight <unk> but also to satisfy consumers <eos>\n",
      "pred: that is why the council as legislator needs publicly in <eos>\n",
      "tar:  that is why the council as legislator must meet openly <eos>\n",
      "epoch:6  Step:132000  loss:1.2326642275  WER:0.4054487179\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: in sector also creates many jobs jobs in the processing industry and in shipyards building construction industries <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:6  Step:1650  loss:2.1014842701  WER:0.4532016251\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 5.143665107993766e-05\n",
      "pred: because of the reduction of the traffic the tunnel we do not have the <unk> three minutes to morning because because would no the the <unk> from scotland to the continent was been withdrawn <eos>\n",
      "tar:  because of the reduction in transit through the tunnel we do not have a mere three services a night - we have none because the service from scotland to the continent has been withdrawn completely <eos>\n",
      "pred: men and governments must be judged by what they say than than what what they say <eos>\n",
      "tar:  individuals and governments should be judged by what they do rather than by what they say <eos>\n",
      "epoch:6  Step:134000  loss:1.4583033323  WER:0.5028248588\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates many jobs jobs on the <unk> industry and in the construction shops <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:6  Step:1675  loss:2.1114680243  WER:0.4590483367\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 5.054598085059007e-05\n",
      "pred: building is been a different task to create a monitoring structure economic structure for economic financial analysis base with 12 with 12 different countries and we have protect the independence of the central bank but not its independence <eos>\n",
      "tar:  it has been a <unk> task to create a control and monitoring structure an economic financial analysis structure from scratch with 12 different countries and we must safeguard the independence of the central bank but not its isolation <eos>\n",
      "pred: the liberalisation of the energy markets has blocked by my country and france france <eos>\n",
      "tar:  the liberalisation of the energy markets was blocked by my country and by france <eos>\n",
      "epoch:6  Step:136000  loss:1.2138626575  WER:0.3114754098\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also has many jobs jobs in the <unk> industry and in the building industries industries <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:6  Step:1700  loss:2.1045269823  WER:0.4574887711\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 4.965531062124249e-05\n",
      "pred: <eos>\n",
      "tar:  <eos>\n",
      "pred: commissioner my group goes alongside you side <eos>\n",
      "tar:  commissioner my group stands by your side <eos>\n",
      "epoch:6  Step:138000  loss:1.4376463890  WER:0.3580000000\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector is creates many jobs jobs in the processing industry or in works yards industry industry industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:6  Step:1725  loss:2.1057784033  WER:0.4579379693\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 4.8764640391894904e-05\n",
      "pred: however measures have that the have been introduced which are not for citizens to comply <eos>\n",
      "tar:  however we find that measures have been introduced which are impossible for citizens to comply with <eos>\n",
      "pred: the result been been in a closer closer is closer to 0 than in and which no not be referred up <eos>\n",
      "tar:  this has now resulted in a percentage which is nearer to 0 than 90 and which may not be made public <eos>\n",
      "epoch:6  Step:140000  loss:1.5730358362  WER:0.3658146965\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates many jobs jobs on the processing industry and in the processing industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:6  Step:1750  loss:2.0957516193  WER:0.4567743874\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 4.7873970162547324e-05\n",
      "pred: i believe that would consumer would be a a <unk> of <unk> at down to the highest possible level <eos>\n",
      "tar:  i think that the result will be like a game of <unk> right up to the highest official level <eos>\n",
      "pred: the commission will committed to tabling the necessary needed for the <unk> of the <unk> top level domain and order timely manner and will do its best to ensure that the corresponding procedures are followed so that so enable the registry registry to become operational as soon as possible after the entry into force of the regulation without it circumstances remain <eos>\n",
      "tar:  the commission is committed to submitting the measures required for the implementation of the <unk> top level domain in a timely fashion and shall do its utmost to ensure that the relevant procedures are completed and promptly to allow the <unk> registry to become operational as early as possible after the entry into force of the regulation unless exceptional circumstances intervene <eos>\n",
      "epoch:6  Step:142000  loss:1.3269789219  WER:0.3380855397\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates a jobs jobs on the processing industry and in the processing industry sector <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:6  Step:1775  loss:2.0868414974  WER:0.4590893934\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 109\u001b[39m\n\u001b[32m    105\u001b[39m total_token_length = \u001b[32m0\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i3, (pred, tar) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m( \u001b[38;5;28mzip\u001b[39m(predict, target) ):\n\u001b[32m    107\u001b[39m     (error, substitute, \n\u001b[32m    108\u001b[39m         delete, insert, ref_length) = \\\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m         \u001b[43mlevenshtein\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcalculate_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m     \u001b[38;5;66;03m# 誤り文字数を累積する\u001b[39;00m\n\u001b[32m    111\u001b[39m     total_error += error\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NMT_MAML/MAML_NMT-main/Ordinary_IT_NMT/levenshtein.py:66\u001b[39m, in \u001b[36mcalculate_error\u001b[39m\u001b[34m(hypothesis, reference)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# 置換処理，削除処理，挿入処理のうち，\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# どの処理を行えば累積コストが最も小さくなるかを計算\u001b[39;00m\n\u001b[32m     65\u001b[39m cost = [substitute_cost, delete_cost, insert_cost]\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m min_index = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43margmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcost\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m min_index == \u001b[32m0\u001b[39m:\n\u001b[32m     69\u001b[39m     \u001b[38;5;66;03m# 置換処理が累積コスト最小となる場合\u001b[39;00m\n\u001b[32m     70\u001b[39m \n\u001b[32m     71\u001b[39m     \u001b[38;5;66;03m# 遷移元の累積コスト情報をコピー\u001b[39;00m\n\u001b[32m     72\u001b[39m     cost_matrix[i][j] = \\\n\u001b[32m     73\u001b[39m         copy.copy(cost_matrix[i-\u001b[32m1\u001b[39m][j-\u001b[32m1\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/numpy/core/fromnumeric.py:1325\u001b[39m, in \u001b[36margmin\u001b[39m\u001b[34m(a, axis, out, keepdims)\u001b[39m\n\u001b[32m   1238\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1239\u001b[39m \u001b[33;03mReturns the indices of the minimum values along an axis.\u001b[39;00m\n\u001b[32m   1240\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1322\u001b[39m \u001b[33;03m(2, 1, 4)\u001b[39;00m\n\u001b[32m   1323\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1324\u001b[39m kwds = {\u001b[33m'\u001b[39m\u001b[33mkeepdims\u001b[39m\u001b[33m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np._NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m-> \u001b[39m\u001b[32m1325\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43margmin\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/numpy/core/fromnumeric.py:56\u001b[39m, in \u001b[36m_wrapfunc\u001b[39m\u001b[34m(obj, method, *args, **kwds)\u001b[39m\n\u001b[32m     54\u001b[39m bound = \u001b[38;5;28mgetattr\u001b[39m(obj, method, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m bound \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(*args, **kwds)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/numpy/core/fromnumeric.py:45\u001b[39m, in \u001b[36m_wrapit\u001b[39m\u001b[34m(obj, method, *args, **kwds)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[32m     44\u001b[39m     wrap = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m result = \u001b[38;5;28mgetattr\u001b[39m(\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m, method)(*args, **kwds)\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m wrap:\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, mu.ndarray):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "epoch_num = 10\n",
    "\n",
    "# WarmupとCosine Decayを行うスケジューラを利用\n",
    "num_global_steps = len( train_loader ) * epoch_num\n",
    "print( \"num_global_steps:\", num_global_steps )\n",
    "num_warmup_steps = num_global_steps * 0.1\n",
    "print( \"num_warmup_steps:\", num_warmup_steps )\n",
    "scheduler = get_linear_schedule_with_warmup( optimizer, num_warmup_steps, num_global_steps ) \n",
    "\n",
    "tr_print_coef = 2000\n",
    "val_print_coef = 100\n",
    "#tr_print_coef = 10\n",
    "#val_print_coef = 1\n",
    "\n",
    "#PATH = \"./model_NTT_auto_curr3.pt\"\n",
    "\n",
    "#if device != torch.device(\"cpu\"):\n",
    "#    checkpoint = torch.load(PATH)\n",
    "#else:\n",
    "#    checkpoint = torch.load(PATH, map_location=torch.device('cpu'))\n",
    "\n",
    "#model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device( \"cpu\")\n",
    "\n",
    "## optimizerのstateを現在のdeviceに移す。これをしないと、保存前後でdeviceの不整合が起こる可能性がある。\n",
    "#for state in optimizer.state.values():\n",
    "#    for k, v in state.items():\n",
    "#        if isinstance(v, torch.Tensor):\n",
    "#            state[k] = v.to(device)\n",
    "##epoch = checkpoint['epoch']\n",
    "##loss = checkpoint['loss']\n",
    "\n",
    "\n",
    "history = {\"train_loss\": [], \"val_loss\": [], \"train_wer\": [], \"val_wer\": [] }\n",
    "\n",
    "n = 0\n",
    "train_loss = 0\n",
    "val_loss = 0\n",
    "\n",
    "f_train = open( \"train_it3.log\", mode=\"w\", encoding = \"UTF-8\" )\n",
    "f_val = open( \"val_it3.log\", mode=\"w\", encoding = \"UTF-8\" )\n",
    "\n",
    "tra_global_step = 0\n",
    "val_global_step = 0\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    \n",
    "    model.train()\n",
    "    print( \"Train\")\n",
    "    train_loss = 0\n",
    "    train_wer = 0\n",
    "    n = 0\n",
    "    for i, (src, src_len, tar, tar_len) in enumerate(train_loader):\n",
    "    #for i, (src, src_len, tar, tar_len) in enumerate(val_loader):\n",
    "        #display = True\n",
    "\n",
    "        tra_global_step += 1\n",
    "        if tra_global_step % tr_print_coef  == 0:\n",
    "            display = True\n",
    "            train_loss = 0\n",
    "            train_wer = 0\n",
    "            n = 0\n",
    "        else:\n",
    "            display = False\n",
    "        if display:\n",
    "            print(\"\\n-----------------Train Mode-----------------\\n\")\n",
    "        if display:\n",
    "            print(\"lr:\", optimizer.param_groups[0][\"lr\"] )\n",
    "\n",
    "        src = src[:,:max(src_len)].to(device)\n",
    "        tar = tar[:,:max(tar_len)]\n",
    "        dec_in = tar[:,:-1].to(device)\n",
    "        targ = tar[:,1:].to(device)\n",
    "\n",
    "        logits = model( src, dec_in )\n",
    "        loss = loss_fn(logits.transpose( 1, 2 ), targ )\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_max)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        pre_label_id = torch.argmax( logits, dim = 2 )\n",
    "        predict = []\n",
    "        for pre in pre_label_id:\n",
    "            hypo = []\n",
    "            for m in pre:\n",
    "                hypo.append(token_list_en[m.item()])\n",
    "                if token_list_en[m.item()] == '<eos>':\n",
    "                    break            \n",
    "            predict.append( hypo )\n",
    "        target = []\n",
    "        for pre in targ:\n",
    "            reference = []\n",
    "            for m in pre:\n",
    "                reference.append(token_list_en[m.item()])\n",
    "                if token_list_en[m.item()] == '<eos>':\n",
    "                    break            \n",
    "            target.append( reference )\n",
    "        total_error = 0\n",
    "        total_token_length = 0\n",
    "        for i3, (pred, tar) in enumerate( zip(predict, target) ):\n",
    "            (error, substitute, \n",
    "                delete, insert, ref_length) = \\\n",
    "                levenshtein.calculate_error(pred,tar)\n",
    "            # 誤り文字数を累積する\n",
    "            total_error += error\n",
    "            # 文字の総数を累積する\n",
    "            total_token_length += ref_length  \n",
    "\n",
    "            if i3 < 2 and display:\n",
    "                print( \"pred:\", ' '.join(pred) )\n",
    "                print( \"tar: \", ' '.join(tar) )\n",
    "            \n",
    "        train_loss += loss.item()\n",
    "        train_wer += total_error / total_token_length\n",
    "        n += 1\n",
    "        history[\"train_loss\"].append( train_loss / n )\n",
    "        history[\"train_wer\"].append( train_wer / n )\n",
    "        #if i % tr_print_coef == tr_print_coef - 1:\n",
    "        if display:\n",
    "            print(f\"epoch:{epoch+1}  Step:{tra_global_step}  loss:{train_loss/n:.10f}  WER:{ train_wer / n:.10f}\")\n",
    "        if tra_global_step % 2000 == 0:\n",
    "            PATH = './model_NTT_auto_curr_it3.pt'\n",
    "            torch.save({'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss,},\n",
    "               PATH)\n",
    "            #with open(\"history_NTT_auto_curr_it3.pkl\", \"wb\") as f:\n",
    "            #    pickle.dump( history, f )\n",
    "\n",
    "        f_train.write( \"Epoch: \" + str(epoch) + \", Step: \" + str(tra_global_step) + \", Loss: \" + str(train_loss/n) + \", WER: \" + str(train_wer / n ) + \"\\n\" )\n",
    "        f_train.flush()\n",
    "        \n",
    "        display_taihi = display\n",
    "        #if tra_global_step % 100 == 0:\n",
    "        if display:\n",
    "            val_loss = 0\n",
    "            val_wer = 0\n",
    "            val_n = 0\n",
    "            print(\"\\n-----------------Validation Mode-----------------\\n\")\n",
    "\n",
    "            for i_val, (val_src, val_src_len, val_tar, val_tar_len) in enumerate(val_loader):\n",
    "                val_global_step += 1\n",
    "                if i_val == 0:\n",
    "                    display = True\n",
    "                else:\n",
    "                    display = False\n",
    "                val_src = val_src[:,:max(val_src_len)].to(device)\n",
    "                val_tar = val_tar[:,:max(val_tar_len)]\n",
    "                val_dec_in = val_tar[:,:-1].to(device)\n",
    "                val_targ = val_tar[:,1:].to(device)\n",
    "\n",
    "                logits = model( val_src, val_dec_in )\n",
    "                loss = loss_fn(logits.transpose( 1, 2 ), val_targ )\n",
    "\n",
    "                pre_label_id = torch.argmax( logits, dim = 2 )\n",
    "                predict = []\n",
    "                for pre in pre_label_id:\n",
    "                    hypo = []\n",
    "                    for m in pre:\n",
    "                        hypo.append(token_list_en[m.item()])\n",
    "                        if token_list_en[m.item()] == '<eos>':\n",
    "                            break            \n",
    "                    predict.append( hypo )\n",
    "                target = []\n",
    "                for pre in val_targ:\n",
    "                    reference = []\n",
    "                    for m in pre:\n",
    "                        reference.append(token_list_en[m.item()])\n",
    "                        if token_list_en[m.item()] == '<eos>':\n",
    "                            break            \n",
    "                    target.append( reference )\n",
    "                val_total_error = 0\n",
    "                # 文字の総数を累積する\n",
    "                val_total_token_length = 0\n",
    "                for i3, (pred, tar) in enumerate( zip(predict, target) ):\n",
    "                    (error, substitute, \n",
    "                        delete, insert, ref_length) = \\\n",
    "                        levenshtein.calculate_error(pred,tar)\n",
    "                    # 誤り文字数を累積する\n",
    "                    val_total_error += error\n",
    "                    # 文字の総数を累積する\n",
    "                    val_total_token_length += ref_length  \n",
    "\n",
    "                    if i3 < 2 and i_val == 0:\n",
    "                        print( \"pred:\", ' '.join(pred) )\n",
    "                        print( \"tar: \", ' '.join(tar) )\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_wer += val_total_error / val_total_token_length\n",
    "                val_n += 1\n",
    "                #print( \"val_wer:\", val_wer )\n",
    "                #print( \"val_n:\", val_n )\n",
    "                history[\"val_loss\"].append( val_loss / val_n )\n",
    "                history[\"val_wer\"].append( val_wer / val_n )\n",
    "            print(f\"epoch:{epoch+1}  Step:{val_global_step}  loss:{val_loss/val_n:.10f}  WER:{ val_wer / val_n:.10f}\")\n",
    "            f_val.write( \"Epoch: \" + str(epoch) + \", Step: \" + str(val_global_step)  + \", Loss: \" + str(val_loss/val_n) + \", WER: \" + str(val_wer / val_n ) + \"\\n\" ) \n",
    "            f_val.flush()\n",
    "        if tra_global_step % 2000 == 0:\n",
    "            with open(\"history_NTT_auto_curr_it3.pkl\", \"wb\") as f:\n",
    "                pickle.dump( history, f )\n",
    "        display = display_taihi\n",
    "\n",
    "f_train.close()\n",
    "f_val.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d4d9fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
