{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8bf21a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install janome\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "#from torchtext.vocab import vocab\n",
    "#import torchtext.transforms as T\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#from torchvision import transforms\n",
    "import numpy as np\n",
    "import math\n",
    "import janome\n",
    "from janome.tokenizer import Tokenizer\n",
    "#import spacy\n",
    "from collections import Counter\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import time\n",
    "#from torchtext.vocab import build_vocab_from_iterator\n",
    "import levenshtein\n",
    "import json\n",
    "import pickle\n",
    "from timm.scheduler import CosineLRScheduler\n",
    "import nltk\n",
    "from nltk import bleu_score\n",
    "from torch.nn.init import constant_, xavier_uniform_\n",
    "from torch.nn.parameter import Parameter\n",
    "from transformers import  get_linear_schedule_with_warmup\n",
    "from collections import OrderedDict\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "346dde95-1503-4243-9963-31dcdce0e2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device( \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d745f5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49528 20556\n"
     ]
    }
   ],
   "source": [
    "with open( \"corpus/id_to_word_s.pkl\", \"rb\" ) as f:\n",
    "    token_list = pickle.load(f)\n",
    "with open( \"corpus/id_to_word_t.pkl\", \"rb\" ) as f:\n",
    "    token_list_en = pickle.load(f)\n",
    "with open( \"corpus/word_to_id_s.pkl\", \"rb\" ) as f:\n",
    "    idx_list = pickle.load(f)\n",
    "with open( \"corpus/word_to_id_t.pkl\", \"rb\" ) as f:\n",
    "    idx_list_en = pickle.load(f)\n",
    "\n",
    "pad_idx_s = idx_list['<pad>']\n",
    "pad_idx_t = idx_list_en['<pad>']\n",
    "\n",
    "enc_vocab_size, dec_vocab_size = len(token_list), len(token_list_en)\n",
    "print(enc_vocab_size, dec_vocab_size)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86f1161a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<sos>', '<eos>', '<unk>', '<blank>', '<mask>', 'der']\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([ 0, 1,2,3,4,5,6 ])\n",
    "\n",
    "#print( token_list[2] )\n",
    "\n",
    "#n = 0\n",
    "\n",
    "#ii = [ i for i in a ]\n",
    "\n",
    "#print( ii )\n",
    "\n",
    "b = [ token_list[i.item()] for i in a ]\n",
    "\n",
    "print( b )\n",
    "\n",
    "\n",
    "d = idx_list['<pad>']\n",
    "\n",
    "print( d )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "649837a5-2937-4e81-880b-a57f2532b967",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    ''' ミニバッチデータを作成するクラス\n",
    "        torch.utils.data.Datasetクラスを継承し，\n",
    "        以下の関数を定義する\n",
    "        __len__: 総サンプル数を出力する関数\n",
    "        __getitem__: 1サンプルのデータを出力する関数\n",
    "    feat_scp:  特徴量リストファイル\n",
    "    label_scp: ラベルファイル\n",
    "    feat_mean: 特徴量の平均値ベクトル\n",
    "    feat_std:  特徴量の次元毎の標準偏差を並べたベクトル \n",
    "    pad_index: バッチ化の際にフレーム数を合わせる\n",
    "               ためにpaddingする整数値\n",
    "    splice:    前後(splice)フレームを特徴量を結合する\n",
    "               splice=1とすると，前後1フレーム分結合\n",
    "               するので次元数は3倍になる．\n",
    "               splice=0の場合は何もしない\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 et,\n",
    "                 pad_index_s,\n",
    "                 pad_index_t\n",
    "                 ):\n",
    "\n",
    "        # 読み込みながら情報を取得する\n",
    "        self.pad_index_s = pad_index_s\n",
    "        self.pad_index_t = pad_index_t\n",
    "        print( \"pad_index_s:\", self.pad_index_s )\n",
    "        print( \"pad_index_t:\", self.pad_index_t )\n",
    "        self.en_list = []\n",
    "        self.en_lens = []\n",
    "        self.et_list = []\n",
    "        self.et_lens = []\n",
    "        self.num_data = 0\n",
    "        for line in et:\n",
    "            # 各行をスペースで区切り，\n",
    "            # リスト型の変数にする\n",
    "            self.en_list.append( line['target'] )\n",
    "            self.en_lens.append( len( line['target']) )\n",
    "            #self.en_att_list.append( line['target']['attention_mask'] )\n",
    "\n",
    "            self.et_list.append( line['source'] )\n",
    "            self.et_lens.append( len( line['source']) )\n",
    "            #self.et_att_list.append( line['source']['attention_mask'] )\n",
    "            self.num_data += 1\n",
    "\n",
    "        #self.en_list = np.int64( np.array( self.en_list ) )\n",
    "        #self.et_list = np.int64( np.array( self.et_list ) )\n",
    "        self.en_lens = np.int64( np.array( self.en_lens ) )\n",
    "        self.et_lens = np.int64( np.array( self.et_lens ) )\n",
    "\n",
    "        # フレーム数の最大値を得る\n",
    "        self.max_en_len = np.max(self.en_lens)\n",
    "        # ラベル長の最大値を得る\n",
    "        self.max_et_len = np.max(self.et_lens)\n",
    "\n",
    "        for n in range(self.num_data):\n",
    "            if n % 10000 == 0:\n",
    "                print( \"n:\", n )\n",
    "            # 埋めるフレームの数\n",
    "            # = 最大フレーム数 - 自分のフレーム数\n",
    "            pad_len = self.max_en_len - self.en_lens[n]\n",
    "            # pad_indexの値で埋める\n",
    "            tmp = self.en_list[n]\n",
    "            self.en_list[n] = np.pad( tmp, (0, pad_len), mode='constant', constant_values=(self.pad_index_t, self.pad_index_t ))\n",
    "            pad_len = self.max_et_len - self.et_lens[n]\n",
    "            # pad_indexの値で埋める\n",
    "            self.et_list[n] = np.pad(self.et_list[n],[0, pad_len],mode='constant', constant_values=self.pad_index_s)\n",
    "\n",
    "        self.en_list = np.int64( np.array( self.en_list ) )\n",
    "        self.et_list = np.int64( np.array( self.et_list ) )\n",
    "\n",
    "    def __len__(self):\n",
    "        ''' 学習データの総サンプル数を返す関数\n",
    "        本実装では発話単位でバッチを作成するため，\n",
    "        総サンプル数=発話数である．\n",
    "        '''\n",
    "        return self.num_data\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ''' サンプルデータを返す関数\n",
    "        本実装では発話単位でバッチを作成するため，\n",
    "        idx=発話番号である．\n",
    "        '''\n",
    "\n",
    "        # ラベル\n",
    "        et = self.et_list[idx]\n",
    "        et_len = self.et_lens[idx]\n",
    "\n",
    "        # 発話ID\n",
    "        en = self.en_list[idx]\n",
    "        en_len = self.en_lens[idx]\n",
    "\n",
    "        # 特徴量，ラベル，フレーム数，\n",
    "        # ラベル長，発話IDを返す\n",
    "        #return (jps, jp_lens, ens,  en_lens)\n",
    "        return (et, et_len, en, en_len )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa12965b-1beb-4be9-967f-7c533486462d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it_train: 499000\n",
      "it_val: 500\n",
      "pad_index_s: 0\n",
      "pad_index_t: 0\n",
      "n: 0\n",
      "n: 10000\n",
      "n: 20000\n",
      "n: 30000\n",
      "n: 40000\n",
      "n: 50000\n",
      "n: 60000\n",
      "n: 70000\n",
      "n: 80000\n",
      "n: 90000\n",
      "n: 100000\n",
      "n: 110000\n",
      "n: 120000\n",
      "n: 130000\n",
      "n: 140000\n",
      "n: 150000\n",
      "n: 160000\n",
      "n: 170000\n",
      "n: 180000\n",
      "n: 190000\n",
      "n: 200000\n",
      "n: 210000\n",
      "n: 220000\n",
      "n: 230000\n",
      "n: 240000\n",
      "n: 250000\n",
      "n: 260000\n",
      "n: 270000\n",
      "n: 280000\n",
      "n: 290000\n",
      "n: 300000\n",
      "n: 310000\n",
      "n: 320000\n",
      "n: 330000\n",
      "n: 340000\n",
      "n: 350000\n",
      "n: 360000\n",
      "n: 370000\n",
      "n: 380000\n",
      "n: 390000\n",
      "n: 400000\n",
      "n: 410000\n",
      "n: 420000\n",
      "n: 430000\n",
      "n: 440000\n",
      "n: 450000\n",
      "n: 460000\n",
      "n: 470000\n",
      "n: 480000\n",
      "n: 490000\n",
      "pad_index_s: 0\n",
      "pad_index_t: 0\n",
      "n: 0\n"
     ]
    }
   ],
   "source": [
    "with open(\"data_train_it.pkl\", mode=\"rb\") as f:\n",
    "    it_train = pickle.load(f)\n",
    "with open(\"data_val_it.pkl\", mode=\"rb\") as f:\n",
    "    it_val = pickle.load(f)\n",
    "\n",
    "#it_train = it_train[:10000]\n",
    "\n",
    "print( \"it_train:\", len( it_train ) )\n",
    "print( \"it_val:\", len( it_val ) )\n",
    "\n",
    "train_dataset = SequenceDataset( it_train, pad_idx_s, pad_idx_t )\n",
    "val_dataset = SequenceDataset( it_val, pad_idx_s, pad_idx_t  )\n",
    "    \n",
    "batch_size = 20\n",
    "#num_workers = 4 if torch.cuda.is_available() else 0\n",
    "num_workers = 0 if device == torch.device( 'cpu' ) else 4\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers)\n",
    "# 開発データのDataLoaderを呼び出す\n",
    "# 開発データはデータはシャッフルしない\n",
    "val_loader = DataLoader(val_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers)    \n",
    "\n",
    "#Iter = iter(train_loader)\n",
    "#xdata, xatt, ydata, yatt = next(Iter) #教師データ、ラベルデータ\n",
    "#print(xdata, xatt, ydata, yatt)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b825cab-27ba-4da2-8050-2631036aa9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        #self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        #return self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66322069-a9f2-458e-af00-45953e75b074",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    '''\n",
    "    位置埋め込み （Positional embedding）\n",
    "    dim_embedding: 埋込み次元\n",
    "    max_len      : 入力の最大系列長\n",
    "    '''\n",
    "    def __init__(self, dim_embedding: int, max_len: int=5000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pos_emb = nn.Embedding(max_len, dim_embedding)\n",
    "\n",
    "    '''\n",
    "    位置エンコーディングの順伝播\n",
    "    x: 位置エンコーディングを埋め込む対象のテンソル,\n",
    "       [バッチサイズ, 系列長, 埋め込み次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        seq = x.shape[1]\n",
    "        positions = torch.arange(start=0, end=seq, step=1, device=x.device).to(torch.long)\n",
    "        positions = self.pos_emb(positions)[:seq,:]\n",
    "        \n",
    "        return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cea6ff9-e8c6-436a-941b-34a4e2e4fced",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fPositionalEmbedding(nn.Module):\n",
    "    '''\n",
    "    位置埋め込み （Positional embedding）\n",
    "    dim_embedding: 埋込み次元\n",
    "    max_len      : 入力の最大系列長\n",
    "    '''\n",
    "    def __init__(self, dim_embedding: int, name, max_len: int=5000):\n",
    "        super().__init__()\n",
    "\n",
    "        #self.pos_emb = nn.Embedding(max_len, dim_embedding)\n",
    "        self.name = name\n",
    "        \n",
    "    '''\n",
    "    位置エンコーディングの順伝播\n",
    "    x: 位置エンコーディングを埋め込む対象のテンソル,\n",
    "       [バッチサイズ, 系列長, 埋め込み次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor, weights):\n",
    "        seq = x.shape[1]\n",
    "        positions = torch.arange(start=0, end=seq, step=1, device=x.device).to(torch.long)\n",
    "        #positions = self.pos_emb(positions)[:seq,:]\n",
    "        positions = F.embedding( positions, weights[self.name] )[:seq,:]\n",
    "        \n",
    "        return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e05abc0d-48d7-4e58-8bc4-0a5310e59330",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "    '''\n",
    "    自己アテンション\n",
    "    dim_hidden: 入力特徴量の次元\n",
    "    num_heads : マルチヘッドアテンションのヘッド数\n",
    "    qkv_bias  : クエリなどを生成する全結合層のバイアスの有無\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 qkv_bias: bool=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # 特徴量を各ヘッドのために分割するので、\n",
    "        # 特徴量次元をヘッド数で割り切れるか検証\n",
    "        assert dim_hidden % num_heads == 0\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # ヘッド毎の特徴量次元\n",
    "        dim_head = dim_hidden // num_heads\n",
    "\n",
    "        # ソフトマックスのスケール値\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        # ヘッド毎にクエリ、キーおよびバリューを生成するための全結合層\n",
    "        self.proj_in_q = nn.Linear(\n",
    "            dim_hidden, dim_hidden, bias=qkv_bias)\n",
    "        self.proj_in_k = nn.Linear(\n",
    "            dim_hidden, dim_hidden, bias=qkv_bias)\n",
    "        self.proj_in_v = nn.Linear(\n",
    "            dim_hidden, dim_hidden, bias=qkv_bias)\n",
    "\n",
    "        #self.in_proj_weight = nn.Parameter( torch.randn( dim_hidden * 3, dim_hidden ) )\n",
    "        #self.in_proj_bias = nn.Parameter( torch.randn( dim_hidden * 3 ) )\n",
    "        #self.in_proj_weight = Parameter( torch.empty( dim_hidden * 3, dim_hidden ) )\n",
    "        #self.in_proj_bias = Parameter( torch.empty( dim_hidden * 3 ) )\n",
    "\n",
    "        # 各ヘッドから得られた特徴量を一つにまとめる全結合層\n",
    "        self.proj_out = nn.Linear(dim_hidden, dim_hidden)\n",
    "        \n",
    "        #self._reset_parameters()\n",
    "        \n",
    "    #def _reset_parameters(self):\n",
    "    #    xavier_uniform_( self.in_proj_weight )\n",
    "    #    constant_( self.in_proj_bias, 0.0 )\n",
    "\n",
    "    def split_head(self, x):\n",
    "        x = torch.tensor_split(x, self.num_heads, dim = 2)\n",
    "        x = torch.stack(x, dim = 1)\n",
    "        return x\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, mask: torch.Tensor):\n",
    "\n",
    "        #bs_q, ns_q, ds_q = q.size()\n",
    "        #bs_k, ns_k, ds_k = k.size()\n",
    "        \n",
    "        ##  k = v assumpotion\n",
    "        #if q is k:\n",
    "        #    qkv = q @ self.in_proj_weight.transpose(-2,-1) + self.in_proj_bias\n",
    "        #    qkv = qkv.view( bs_q, ns_q, 3, ds_q )\n",
    "        #    q, k, v = torch.unbind( qkv, dim = 2 )\n",
    "        #else:\n",
    "        #    W_q, W_kv = self.in_proj_weight.split([ds_q, ds_q * 2])\n",
    "        #    b_q, b_kv = self.in_proj_bias.split([ds_q, ds_q * 2])\n",
    "        #    q = q @ W_q.transpose(-2,-1) + b_q\n",
    "        #    kv =  k @ W_kv.transpose(-2,-1) + b_kv\n",
    "        #    kv = kv.view( bs_k, ns_k, 2, ds_k )\n",
    "        #    k, v = torch.unbind( kv, dim = 2 )\n",
    "        \n",
    "        q = self.proj_in_q(q)\n",
    "        k = self.proj_in_k(k)\n",
    "        v = self.proj_in_v(v)\n",
    "        \n",
    "        q = self.split_head(q)\n",
    "        k = self.split_head(k)\n",
    "        v = self.split_head(v)\n",
    "\n",
    "        # クエリとキーの行列積とアテンションの計算(今回マスクは不使用)\n",
    "        # attnは[バッチサイズ, ヘッド数, 特徴量数, 特徴量数]\n",
    "        attn = q.matmul(k.transpose(-2, -1))\n",
    "        #print( \"attn size:\", attn.size() )\n",
    "        #print( \"mask size:\", mask.size() )\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill_(mask, -torch.finfo(torch.float).max)\n",
    "        attn = (attn * self.scale).softmax(dim=-1)\n",
    "\n",
    "        # アテンションとバリューの行列積によりバリューを収集\n",
    "        # xは[バッチサイズ, ヘッド数, 特徴量数, ヘッドの特徴量次元]\n",
    "        x = attn.matmul(v)\n",
    "\n",
    "        # permute関数により\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数, ヘッドの特徴量次元]\n",
    "        # flatten関数により全てのヘッドから得られる特徴量を連結して、\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数 * ヘッドの特徴量次元]\n",
    "        x = x.permute(0, 2, 1, 3).flatten(2)\n",
    "        x = self.proj_out(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a51e5fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class feMHA(nn.Module):\n",
    "    '''\n",
    "    自己アテンション\n",
    "    dim_hidden: 入力特徴量の次元\n",
    "    num_heads : マルチヘッドアテンションのヘッド数\n",
    "    qkv_bias  : クエリなどを生成する全結合層のバイアスの有無\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 qkv_bias: bool=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # 特徴量を各ヘッドのために分割するので、\n",
    "        # 特徴量次元をヘッド数で割り切れるか検証\n",
    "        assert dim_hidden % num_heads == 0\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # ヘッド毎の特徴量次元\n",
    "        dim_head = dim_hidden // num_heads\n",
    "\n",
    "        # ソフトマックスのスケール値\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        # ヘッド毎にクエリ、キーおよびバリューを生成するための全結合層\n",
    "        #self.proj_in = nn.Linear(\n",
    "        #    dim_hidden, dim_hidden * 3, bias=qkv_bias)\n",
    "\n",
    "        # 各ヘッドから得られた特徴量を一つにまとめる全結合層\n",
    "        #self.proj_out = nn.Linear(dim_hidden, dim_hidden)\n",
    "\n",
    "    def split_head(self, x):\n",
    "        x = torch.tensor_split(x, self.num_heads, dim = 2)\n",
    "        x = torch.stack(x, dim = 1)\n",
    "        return x\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor,  v: torch.Tensor, mask: torch.Tensor, i, weights ):\n",
    "        bs, ns = q.shape[:2]\n",
    "\n",
    "        #qkv = self.proj_in(x)\n",
    "        q = F.linear(q, weights['encoder.encoder_layers.' + str(i) + '.attention.proj_in_q.weight'], weights['encoder.encoder_layers.' + str(i) + '.attention.proj_in_q.bias'])\n",
    "        k = F.linear(k, weights['encoder.encoder_layers.' + str(i) + '.attention.proj_in_k.weight'], weights['encoder.encoder_layers.' + str(i) + '.attention.proj_in_k.bias'])\n",
    "        v = F.linear(v, weights['encoder.encoder_layers.' + str(i) + '.attention.proj_in_v.weight'], weights['encoder.encoder_layers.' + str(i) + '.attention.proj_in_v.bias'])\n",
    "\n",
    "        q = self.split_head( q )\n",
    "        k = self.split_head( k )\n",
    "        v = self.split_head( v )\n",
    "\n",
    "        # クエリとキーの行列積とアテンションの計算(今回マスクは不使用)\n",
    "        # attnは[バッチサイズ, ヘッド数, 特徴量数, 特徴量数]\n",
    "        attn = q.matmul(k.transpose(-2, -1))\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill_(mask, -torch.finfo(torch.float).max)\n",
    "        attn = (attn * self.scale).softmax(dim=-1)\n",
    "\n",
    "        # アテンションとバリューの行列積によりバリューを収集\n",
    "        # xは[バッチサイズ, ヘッド数, 特徴量数, ヘッドの特徴量次元]\n",
    "        x = attn.matmul(v)\n",
    "\n",
    "        # permute関数により\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数, ヘッドの特徴量次元]\n",
    "        # flatten関数により全てのヘッドから得られる特徴量を連結して、\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数 * ヘッドの特徴量次元]\n",
    "        x = x.permute(0, 2, 1, 3).flatten(2)\n",
    "        #x = self.proj_out(x)\n",
    "        x = F.linear(x, weights['encoder.encoder_layers.' + str(i) + '.attention.proj_out.weight'], weights['encoder.encoder_layers.' + str(i) + '.attention.proj_out.bias'])\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5839e53-a27a-4be2-8e24-b5a5b0deec8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fdsMHA(nn.Module):\n",
    "    '''\n",
    "    自己アテンション\n",
    "    dim_hidden: 入力特徴量の次元\n",
    "    num_heads : マルチヘッドアテンションのヘッド数\n",
    "    qkv_bias  : クエリなどを生成する全結合層のバイアスの有無\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 qkv_bias: bool=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # 特徴量を各ヘッドのために分割するので、\n",
    "        # 特徴量次元をヘッド数で割り切れるか検証\n",
    "        assert dim_hidden % num_heads == 0\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # ヘッド毎の特徴量次元\n",
    "        dim_head = dim_hidden // num_heads\n",
    "\n",
    "        # ソフトマックスのスケール値\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        # ヘッド毎にクエリ、キーおよびバリューを生成するための全結合層\n",
    "        #self.proj_in = nn.Linear(\n",
    "        #    dim_hidden, dim_hidden * 3, bias=qkv_bias)\n",
    "\n",
    "        # 各ヘッドから得られた特徴量を一つにまとめる全結合層\n",
    "        #self.proj_out = nn.Linear(dim_hidden, dim_hidden)\n",
    "\n",
    "    def split_head(self, x):\n",
    "        x = torch.tensor_split(x, self.num_heads, dim = 2)\n",
    "        x = torch.stack(x, dim = 1)\n",
    "        return x\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor,  v: torch.Tensor, mask: torch.Tensor, i, weights ):\n",
    "        bs, ns = q.shape[:2]\n",
    "\n",
    "        #qkv = self.proj_in(x)\n",
    "        #print( \"size q:\", q.size() )\n",
    "        #print( \"size weigths q:\", weights['d_layers.' + str(i) + '.crossattn.proj_in_q.weight'].size() )\n",
    "        #print( \"size k:\", k.size() )\n",
    "        #print( \"size weights k:\", weights['d_layers.' + str(i) + '.crossattn.proj_in_k.weight'].size() )\n",
    "        q = F.linear(q, weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_in_q.weight'], weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_in_q.bias'])\n",
    "        k = F.linear(k, weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_in_k.weight'], weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_in_k.bias'])\n",
    "        v = F.linear(v, weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_in_v.weight'], weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_in_v.bias'])\n",
    "\n",
    "        q = self.split_head( q )\n",
    "        k = self.split_head( k )\n",
    "        v = self.split_head( v )\n",
    "\n",
    "        # view関数により\n",
    "        # [バッチサイズ, 特徴量数, QKV, ヘッド数, ヘッドの特徴量次元]\n",
    "        # permute関数により\n",
    "        # [QKV, バッチサイズ, ヘッド数, 特徴量数, ヘッドの特徴量次元]\n",
    "        #qkv = qkv.view(\n",
    "        #    bs, ns, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        # クエリ、キーおよびバリューに分解\n",
    "        #q, k, v = qkv.unbind(0)\n",
    "\n",
    "        # クエリとキーの行列積とアテンションの計算(今回マスクは不使用)\n",
    "        # attnは[バッチサイズ, ヘッド数, 特徴量数, 特徴量数]\n",
    "        attn = q.matmul(k.transpose(-2, -1))\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill_(mask, -torch.finfo(torch.float).max)      \n",
    "        attn = (attn * self.scale).softmax(dim=-1)\n",
    "\n",
    "        # アテンションとバリューの行列積によりバリューを収集\n",
    "        # xは[バッチサイズ, ヘッド数, 特徴量数, ヘッドの特徴量次元]\n",
    "        x = attn.matmul(v)\n",
    "\n",
    "        # permute関数により\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数, ヘッドの特徴量次元]\n",
    "        # flatten関数により全てのヘッドから得られる特徴量を連結して、\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数 * ヘッドの特徴量次元]\n",
    "        x = x.permute(0, 2, 1, 3).flatten(2)\n",
    "        #x = self.proj_out(x)\n",
    "        x = F.linear(x, weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_out.weight'], weights['decoder.decoder_layers.' + str(i) + '.selfattn.proj_out.bias'])\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0464026d-c2fd-456a-89dc-e70cd632fbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fdcMHA(nn.Module):\n",
    "    '''\n",
    "    自己アテンション\n",
    "    dim_hidden: 入力特徴量の次元\n",
    "    num_heads : マルチヘッドアテンションのヘッド数\n",
    "    qkv_bias  : クエリなどを生成する全結合層のバイアスの有無\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 qkv_bias: bool=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # 特徴量を各ヘッドのために分割するので、\n",
    "        # 特徴量次元をヘッド数で割り切れるか検証\n",
    "        assert dim_hidden % num_heads == 0\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # ヘッド毎の特徴量次元\n",
    "        dim_head = dim_hidden // num_heads\n",
    "\n",
    "        # ソフトマックスのスケール値\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        # ヘッド毎にクエリ、キーおよびバリューを生成するための全結合層\n",
    "        #self.proj_in = nn.Linear(\n",
    "        #    dim_hidden, dim_hidden * 3, bias=qkv_bias)\n",
    "\n",
    "        # 各ヘッドから得られた特徴量を一つにまとめる全結合層\n",
    "        #self.proj_out = nn.Linear(dim_hidden, dim_hidden)\n",
    "\n",
    "    def split_head(self, x):\n",
    "        x = torch.tensor_split(x, self.num_heads, dim = 2)\n",
    "        x = torch.stack(x, dim = 1)\n",
    "        return x\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor,  v: torch.Tensor, mask: torch.Tensor, i, weights ):\n",
    "        bs, ns = q.shape[:2]\n",
    "\n",
    "        #qkv = self.proj_in(x)\n",
    "        #print( \"size q:\", q.size() )\n",
    "        #print( \"size weigths q:\", weights['d_layers.' + str(i) + '.crossattn.proj_in_q.weight'].size() )\n",
    "        #print( \"size k:\", k.size() )\n",
    "        #print( \"size weights k:\", weights['d_layers.' + str(i) + '.crossattn.proj_in_k.weight'].size() )\n",
    "        q = F.linear(q, weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_in_q.weight'], weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_in_q.bias'])\n",
    "        k = F.linear(k, weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_in_k.weight'], weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_in_k.bias'])\n",
    "        v = F.linear(v, weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_in_v.weight'], weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_in_v.bias'])\n",
    "\n",
    "        q = self.split_head( q )\n",
    "        k = self.split_head( k )\n",
    "        v = self.split_head( v )\n",
    "\n",
    "        # view関数により\n",
    "        # [バッチサイズ, 特徴量数, QKV, ヘッド数, ヘッドの特徴量次元]\n",
    "        # permute関数により\n",
    "        # [QKV, バッチサイズ, ヘッド数, 特徴量数, ヘッドの特徴量次元]\n",
    "        #qkv = qkv.view(\n",
    "        #    bs, ns, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        # クエリ、キーおよびバリューに分解\n",
    "        #q, k, v = qkv.unbind(0)\n",
    "\n",
    "        # クエリとキーの行列積とアテンションの計算(今回マスクは不使用)\n",
    "        # attnは[バッチサイズ, ヘッド数, 特徴量数, 特徴量数]\n",
    "        attn = q.matmul(k.transpose(-2, -1))\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill_(mask, -torch.finfo(torch.float).max)      \n",
    "        attn = (attn * self.scale).softmax(dim=-1)\n",
    "\n",
    "        # アテンションとバリューの行列積によりバリューを収集\n",
    "        # xは[バッチサイズ, ヘッド数, 特徴量数, ヘッドの特徴量次元]\n",
    "        x = attn.matmul(v)\n",
    "\n",
    "        # permute関数により\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数, ヘッドの特徴量次元]\n",
    "        # flatten関数により全てのヘッドから得られる特徴量を連結して、\n",
    "        # [バッチサイズ, 特徴量数, ヘッド数 * ヘッドの特徴量次元]\n",
    "        x = x.permute(0, 2, 1, 3).flatten(2)\n",
    "        #x = self.proj_out(x)\n",
    "        x = F.linear(x, weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_out.weight'], weights['decoder.decoder_layers.' + str(i) + '.crossattn.proj_out.bias'])\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88160e0b-1bca-44a6-88d3-22f706a89b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "    '''\n",
    "    Transformerエンコーダ内の順伝播型ニューラルネットワーク\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    dim_feedforward: 中間特徴量の次元\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, dim_feedforward: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(dim_hidden, dim_feedforward)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, dim_hidden)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbd75eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class feFNN(nn.Module):\n",
    "    '''\n",
    "    Transformerエンコーダ内の順伝播型ニューラルネットワーク\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    dim_feedforward: 中間特徴量の次元\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, dim_feedforward: int):\n",
    "        super().__init__()\n",
    "\n",
    "        #self.linear1 = nn.Linear(dim_hidden, dim_feedforward)\n",
    "        #self.linear2 = nn.Linear(dim_feedforward, dim_hidden)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor, i, weights ):\n",
    "        #x = self.linear1(x)\n",
    "        x = F.linear(x, weights['encoder.encoder_layers.' + str(i) + '.fnn.linear1.weight'], weights['encoder.encoder_layers.' + str(i) + '.fnn.linear1.bias'])\n",
    "        x = self.activation(x)\n",
    "        #x = self.linear2(x)\n",
    "        x = F.linear(x, weights['encoder.encoder_layers.' + str(i) + '.fnn.linear2.weight'], weights['encoder.encoder_layers.' + str(i) + '.fnn.linear2.bias'])\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1830c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fdFNN(nn.Module):\n",
    "    '''\n",
    "    Transformerエンコーダ内の順伝播型ニューラルネットワーク\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    dim_feedforward: 中間特徴量の次元\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, dim_feedforward: int):\n",
    "        super().__init__()\n",
    "\n",
    "        #self.linear1 = nn.Linear(dim_hidden, dim_feedforward)\n",
    "        #self.linear2 = nn.Linear(dim_feedforward, dim_hidden)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor, i, weights ):\n",
    "        #x = self.linear1(x)\n",
    "        x = F.linear(x, weights['decoder.decoder_layers.' + str(i) + '.fnn.linear1.weight'], weights['decoder.decoder_layers.' + str(i) + '.fnn.linear1.bias'])\n",
    "        x = self.activation(x)\n",
    "        #x = self.linear2(x)\n",
    "        x = F.linear(x, weights['decoder.decoder_layers.' + str(i) + '.fnn.linear2.weight'], weights['decoder.decoder_layers.' + str(i) + '.fnn.linear2.bias'])\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "caaa86ef-4575-4c9c-9265-a337fb7a9734",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    '''\n",
    "    Transformerエンコーダ層\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    num_heads      : ヘッド数\n",
    "    dim_feedforward: 中間特徴量の次元\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 dim_feedforward: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = MHA(dim_hidden, num_heads, qkv_bias = True)\n",
    "        self.fnn = FNN(dim_hidden, dim_feedforward)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(dim_hidden)\n",
    "        self.norm2 = nn.LayerNorm(dim_hidden)\n",
    "        \n",
    "        self.dropout = nn.Dropout( dropout )\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor):\n",
    "        ''' B2T\n",
    "        x0 = x\n",
    "        x = self.attention( x, x, x, mask )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x = self.norm1(x)\n",
    "        x1 = x\n",
    "        x = self.fnn( x )\n",
    "        x = self.dropout( x )\n",
    "        x =  x + x1 + x0\n",
    "        x = self.norm2( x )\n",
    "        '''\n",
    "        #pre-LN\n",
    "        x0 = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attention( x, x, x, mask )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x1 = x\n",
    "        x = self.norm2( x )\n",
    "        x = self.fnn( x )\n",
    "        x = self.dropout( x )\n",
    "        x =  x + x1\n",
    "        \n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a2533c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fTransformerEncoderLayer(nn.Module):\n",
    "    '''\n",
    "    Transformerエンコーダ層\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    num_heads      : ヘッド数\n",
    "    dim_feedforward: 中間特徴量の次元\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 dim_feedforward: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fattention = feMHA(dim_hidden, num_heads)\n",
    "        self.fffn = feFNN(dim_hidden, dim_feedforward)\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.dropout = nn.Dropout( dropout )\n",
    "        \n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor, i, weights ):\n",
    "        '''B2T\n",
    "        x0 = x\n",
    "        x = self.fattention( x, x, x, mask, i, weights )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['encoder.encoder_layers.' + str(i) + '.norm1.weight'], bias=weights['encoder.encoder_layers.' + str(i) + '.norm1.bias'], eps=1e-05)\n",
    "        x1 = x\n",
    "        x = self.fffn( x, i, weights ) \n",
    "        x = self.dropout( x )\n",
    "        x = x + x1 + x0\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['encoder.encoder_layers.' + str(i) + '.norm2.weight'], bias=weights['encoder.encoder_layers.' + str(i) + '.norm2.bias'], eps=1e-05)\n",
    "        '''\n",
    "        #pre-LN\n",
    "        x0 = x\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['encoder.encoder_layers.' + str(i) + '.norm1.weight'], bias=weights['encoder.encoder_layers.' + str(i) + '.norm1.bias'], eps=1e-05)\n",
    "        x = self.fattention( x, x, x, mask, i, weights )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x1 = x\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['encoder.encoder_layers.' + str(i) + '.norm2.weight'], bias=weights['encoder.encoder_layers.' + str(i) + '.norm2.bias'], eps=1e-05)\n",
    "        x = self.fffn( x, i, weights ) \n",
    "        x = self.dropout( x )\n",
    "        x = x + x1\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "387ffc10-2b89-4039-8a6a-4b4a8fbde394",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    '''\n",
    "    Transformerエンコーダ層\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    num_heads      : ヘッド数\n",
    "    dim_feedforward: 中間特徴量の次元\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 dim_feedforward: int, dropout: float=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.selfattn = MHA(dim_hidden, num_heads, qkv_bias = True)\n",
    "        self.crossattn = MHA(dim_hidden, num_heads, qkv_bias = True)\n",
    "        self.fnn = FNN(dim_hidden, dim_feedforward)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(dim_hidden)\n",
    "        self.norm2 = nn.LayerNorm(dim_hidden)\n",
    "        self.norm3 = nn.LayerNorm(dim_hidden)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor,  self_mask: torch.Tensor,  cross_mask: torch.Tensor):\n",
    "        '''B2T\n",
    "        x0 = x\n",
    "        x = self.selfattn( x, x, x, self_mask )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x = self.norm1( x )\n",
    "        x1 = x\n",
    "        x = self.crossattn( x, y, y, cross_mask )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x1\n",
    "        x = self.norm2( x )\n",
    "        x2 = x\n",
    "        x = self.fnn( x )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x2 + x0\n",
    "        x = self.norm3( x )\n",
    "        '''\n",
    "        #pre-LN\n",
    "        x0 = x\n",
    "        x = self.norm1( x )\n",
    "        x = self.selfattn( x, x, x, self_mask )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x1 = x\n",
    "        x = self.norm2( x )\n",
    "        x = self.crossattn( x, y, y, cross_mask )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x1\n",
    "        x2 = x\n",
    "        x = self.norm3( x )\n",
    "        x = self.fnn( x )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x2\n",
    "        \n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "063637b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fTransformerDecoderLayer(nn.Module):\n",
    "    '''\n",
    "    Transformerエンコーダ層\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    num_heads      : ヘッド数\n",
    "    dim_feedforward: 中間特徴量の次元\n",
    "    '''\n",
    "    def __init__(self, dim_hidden: int, num_heads: int,\n",
    "                 dim_feedforward: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fselfattn = fdsMHA(dim_hidden, num_heads)\n",
    "        self.fcrossattn = fdcMHA(dim_hidden, num_heads)\n",
    "        self.fffn = fdFNN(dim_hidden, dim_feedforward)\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        #self.norm1 = nn.LayerNorm(dim_hidden)\n",
    "        #self.norm2 = nn.LayerNorm(dim_hidden)\n",
    "        #self.norm3 = nn.LayerNorm(dim_hidden)\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor,  self_mask: torch.Tensor,  cross_mask: torch.Tensor, i, weights):\n",
    "        '''B2T\n",
    "        x0 = x\n",
    "        x = self.fselfattn( x, x, x, self_mask, i, weights )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['decoder.decoder_layers.' + str(i) + '.norm1.weight'], bias=weights['decoder.decoder_layers.' + str(i) + '.norm1.bias'], eps=1e-05)\n",
    "        x1 = x\n",
    "        x = self.fcrossattn( x, y, y, cross_mask, i, weights )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x1\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['decoder.decoder_layers.' + str(i) + '.norm2.weight'], bias=weights['decoder.decoder_layers.' + str(i) + '.norm2.bias'], eps=1e-05)\n",
    "        x2 = x\n",
    "        x = self.fffn( x, i, weights )\n",
    "        x = self.dropout( x ) \n",
    "        x = x + x2 + x0\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['decoder.decoder_layers.' + str(i) + '.norm3.weight'], bias=weights['decoder.decoder_layers.' + str(i) + '.norm3.bias'], eps=1e-05)\n",
    "        '''\n",
    "        #pre-LN\n",
    "        x0 = x\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['decoder.decoder_layers.' + str(i) + '.norm1.weight'], bias=weights['decoder.decoder_layers.' + str(i) + '.norm1.bias'], eps=1e-05)\n",
    "        x = self.fselfattn( x, x, x, self_mask, i, weights )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x0\n",
    "        x1 = x\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['decoder.decoder_layers.' + str(i) + '.norm2.weight'], bias=weights['decoder.decoder_layers.' + str(i) + '.norm2.bias'], eps=1e-05)\n",
    "        x = self.fcrossattn( x, y, y, cross_mask, i, weights )\n",
    "        x = self.dropout( x )\n",
    "        x = x + x1\n",
    "        x2 = x\n",
    "        x = F.layer_norm(x, (self.dim_hidden,), weight=weights['decoder.decoder_layers.' + str(i) + '.norm3.weight'], bias=weights['decoder.decoder_layers.' + str(i) + '.norm3.bias'], eps=1e-05)\n",
    "        x = self.fffn( x, i, weights )\n",
    "        x = self.dropout( x ) \n",
    "        x = x + x2\n",
    "        \n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0509fcd8-bf62-4006-a5ca-94f2fe331e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    '''\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    dim_feedforward: FNNにおける中間特徴量の次元\n",
    "    num_heads      : マルチヘッドアテンションのヘッド数\n",
    "    num_layers     : Transformerエンコーダの層数\n",
    "    '''\n",
    "    def __init__(self, text_vocab_size: int, dim_embedding: int, dim_feedforward: int,\n",
    "                 num_heads: int,  num_layers: int, pad_index:int, dropout: float = 0.1 ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 単語埋め込み\n",
    "        self.embed = nn.Embedding(\n",
    "            text_vocab_size, dim_embedding, padding_idx=pad_index)        \n",
    "        \n",
    "        # 位置エンコーディング\n",
    "        #self.pos_enc = PositionalEncoding(dim_embedding)\n",
    "        self.pos_emb = PositionalEmbedding(dim_embedding)\n",
    "        #self.dropout = nn.Dropout( dropout )\n",
    "        \n",
    "        # Transformerエンコーダ層\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(dim_hidden=dim_embedding, num_heads=num_heads, dim_feedforward = dim_feedforward, dropout = dropout )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(dim_embedding)\n",
    "       \n",
    "        self.pad_index = pad_index\n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x           : 入力, [バッチサイズ, 入力チャネル数, 高さ, 幅]\n",
    "    return_embed: 特徴量を返すかロジットを返すかを選択する真偽値\n",
    "    '''\n",
    "    def forward(self, src: torch.Tensor, src_mask: torch.Tensor=None, src_padding_mask: torch.Tensor=None ):\n",
    "\n",
    "        if src_padding_mask is not None and src_mask is None:\n",
    "            mask = src_padding_mask[:,None,:,None]\n",
    "            mask = mask.expand( (-1, self.num_heads, -1, src.size(1)))\n",
    "        elif src_padding_mask is not None and src_mask is not None:\n",
    "            mask1 = src_padding_mask[:,None,:,None]\n",
    "            mask1 = mask1.expand( (-1, self.num_heads, -1, src.size(1)))\n",
    "            mask2 = src_mask[None,None,:,:]\n",
    "            mask2 = mask2.expand( ( src.size(0), self.num_heads, -1, -1 ) )\n",
    "            mask = torch.logical_or(mask1, mask2 )\n",
    "        elif src_padding_mask is None and src_mask is not None:\n",
    "            mask = src_mask[None,None,:,:]\n",
    "            mask = mask.padding( ( src.size(0), src.size(1), -1, -1 ) )\n",
    "        else:\n",
    "            mask = None\n",
    "\n",
    "        x = self.embed( src ) * math.sqrt( self.dim_embedding )\n",
    "\n",
    "        #x = self.pos_enc( x )\n",
    "        position = self.pos_emb( x )\n",
    "        x = x + position\n",
    "        #x = self.dropout( x )\n",
    "        #x = self.norm( x )\n",
    "        \n",
    "        # Transformerエンコーダ層を適用\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer( x, mask )\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd45f25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fTransformerEncoder(nn.Module):\n",
    "    '''\n",
    "    dim_hidden     : 入力特徴量の次元\n",
    "    dim_feedforward: FNNにおける中間特徴量の次元\n",
    "    num_heads      : マルチヘッドアテンションのヘッド数\n",
    "    num_layers     : Transformerエンコーダの層数\n",
    "    '''\n",
    "    def __init__(self, text_vocab_size: int, dim_embedding: int, dim_feedforward: int,\n",
    "                 num_heads: int,  num_layers: int, pad_idx:int, dropout: float = 0.1 ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 単語埋め込み\n",
    "        #self.embed = nn.Embedding(\n",
    "        #    text_vocab_size, dim_embedding, padding_idx=pad_index)        \n",
    "        \n",
    "        # 位置エンコーディング\n",
    "        #self.pos_enc = PositionalEncoding(dim_embedding)\n",
    "        self.fpos_emb = fPositionalEmbedding(dim_embedding, \"encoder.pos_emb.pos_emb.weight\" )\n",
    "        #self.dropout = nn.Dropout( dropout )\n",
    "        \n",
    "        # Transformerエンコーダ層\n",
    "        self.fenclayer = fTransformerEncoderLayer(dim_hidden=dim_embedding, num_heads=num_heads, dim_feedforward = dim_feedforward, dropout = dropout )\n",
    "        \n",
    "        # ロジットを生成する前のレイヤー正規化と全結合\n",
    "        #self.norm = nn.LayerNorm(dim_embedding)\n",
    "        \n",
    "        self.pad_idx = pad_idx\n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    '''\n",
    "    順伝播関数\n",
    "    x           : 入力, [バッチサイズ, 入力チャネル数, 高さ, 幅]\n",
    "    return_embed: 特徴量を返すかロジットを返すかを選択する真偽値\n",
    "    '''\n",
    "    def forward(self, src: torch.Tensor, src_mask: torch.Tensor=None, src_padding_mask: torch.Tensor=None, weights = None ):\n",
    "\n",
    "        if src_padding_mask is not None and src_mask is None:\n",
    "            mask = src_padding_mask[:,None,:,None]\n",
    "            mask = mask.expand( (-1, self.num_heads, -1, src.size(1)))\n",
    "        elif src_padding_mask is not None and src_mask is not None:\n",
    "            mask1 = src_padding_mask[:,None,:,None]\n",
    "            mask1 = mask1.expand( (-1, self.num_heads, -1, src.size(1)))\n",
    "            mask2 = src_mask[None,None,:,:]\n",
    "            mask2 = mask2.expand( ( src.size(0), self.num_heads, -1, -1 ) )\n",
    "            mask = torch.logical_or(mask1, mask2 )\n",
    "        elif src_padding_mask is None and src_mask is not None:\n",
    "            mask = src_mask[None,None,:,:]\n",
    "            mask = mask.padding( ( src.size(0), src.size(1), -1, -1 ) )\n",
    "        else:\n",
    "            mask = None\n",
    "\n",
    "        #x = self.embed( src ) * math.sqrt( self.dim_embedding )\n",
    "        x = F.embedding( src, weights['encoder.embed.weight'], padding_idx = self.pad_idx )  * math.sqrt( self.dim_embedding )\n",
    "        \n",
    "        #x = self.pos_enc( x )\n",
    "        position = self.fpos_emb( x, weights )\n",
    "        x = x + position\n",
    "        #x = self.dropout( x )\n",
    "        ##x = self.norm(x)\n",
    "        #x = F.layer_norm(x, (self.dim_embedding,), weight=weights['encoder.norm.weight'], bias=weights['encoder.norm.bias'], eps=1e-05)\n",
    "    \n",
    "        # Transformerエンコーダ層を適用\n",
    "        #for layer in self.encoder_layers:\n",
    "        #    x = layer( x, mask )\n",
    "        for block in range( self.num_layers ):\n",
    "            x = self.fenclayer(x, mask, block, weights )\n",
    "\n",
    "        #x = self.norm(x)\n",
    "        x = F.layer_norm(x, (self.dim_embedding,), weight=weights['encoder.norm.weight'], bias=weights['encoder.norm.bias'], eps=1e-05)\n",
    "        \n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15582995-ff08-4fbe-9f54-9e61e13f89e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    '''\n",
    "    CaptioningTransformerのコンストラクタ\n",
    "    dim_embedding  : 埋め込み次元\n",
    "    dim_feedforward: FNNの中間特徴次元\n",
    "    num_heads      : マルチヘッドアテンションのヘッド数\n",
    "    num_layers     : Transformerデコーダ層の数\n",
    "    vocab_size     : 辞書の次元\n",
    "    pad_index     : NULLのID\n",
    "    dropout        : ドロップアウト確率\n",
    "    '''\n",
    "    def __init__(self, vocab_size: int, dim_embedding: int, dim_feedforward: int,\n",
    "                 num_heads: int, num_layers: int, \n",
    "                 pad_index: int, dropout: float=0.1, ds_rate: float=0.1 ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 単語埋め込み\n",
    "        self.embed = nn.Embedding(\n",
    "            vocab_size, dim_embedding, padding_idx=pad_index)\n",
    "        \n",
    "        # 位置エンコーディング\n",
    "        #self.pos_enc = PositionalEncoding(dim_embedding)\n",
    "        self.pos_emb = PositionalEmbedding(dim_embedding)\n",
    "        #self.dropout = nn.Dropout( dropout )\n",
    "\n",
    "        # Transformerデコーダ\n",
    "        #self.decoder_layers = nn.ModuleList([\n",
    "        #    TransformerDecoderLayer(\n",
    "        #        dim_embedding, num_heads, dim_feedforward, dropout)\n",
    "        #    for _ in range(num_layers)\n",
    "        #])\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer(dim_hidden=dim_embedding, num_heads=num_heads, dim_feedforward = dim_feedforward, dropout = dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # ロジットを生成する前のレイヤー正規化と全結合\n",
    "        #self.norm = nn.LayerNorm(dim_embedding)\n",
    "        \n",
    "        # 単語出力分布計算\n",
    "        self.linear = nn.Linear(dim_embedding, vocab_size)\n",
    "        \n",
    "        self.pad_index = pad_index\n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "\n",
    "\n",
    "    ''' CaptioningTransformerの順伝播処理\n",
    "    features: 画像特徴量 [バッチサイズ, 埋め込み次元]\n",
    "    captions: 正解キャプション [バッチサイズ, 系列長]\n",
    "\n",
    "    '''\n",
    "    #def forward(self, features: torch.Tensor, caption_lengths: torch.Tensor):\n",
    "    #def forward(self, features: torch.Tensor, captions: torch.Tensor, padding_mask_src: torch.Tensor=None, \\\n",
    "    #            padding_mask_tgt: torch.Tensor=None, mask_tgt: torch.Tensor=None ):\n",
    "    def forward(self, features: torch.Tensor, captions: torch.Tensor, memory_padding_mask: torch.Tensor=None, \\\n",
    "                tgt_padding_mask: torch.Tensor=None, tgt_mask: torch.Tensor=None ):\n",
    "\n",
    "        #feature_lengths = torch.ones( (features.size(0) ), device=features.device ) * features.size(1)\n",
    "\n",
    "        tgt = captions\n",
    "        if tgt_padding_mask is not None and tgt_mask is not None:\n",
    "            self_mask1 = tgt_padding_mask[:,None,:,None]\n",
    "            self_mask1 = self_mask1.expand( (-1, self.num_heads, -1, tgt.size(1)))\n",
    "            #print( \"self_mask1:\", self_mask1.size() )\n",
    "            self_mask2 = tgt_mask[None,None,:,:]\n",
    "            self_mask2 = self_mask2.expand( (tgt.size(0), self.num_heads, -1, -1 ))\n",
    "            #print( \"self_mask2:\", self_mask2.size() )\n",
    "            self_mask = torch.logical_or(self_mask1, self_mask2 )\n",
    "            #print( \"0 self_mask:\", self_mask.size() )\n",
    "        elif tgt_padding_mask is not None and tgt_mask is None:\n",
    "            self_mask = tgt_padding_mask[:,None,:,None]\n",
    "            self_mask = self_mask.expand( (-1, self.num_heads, -1, tgt.size(1)))\n",
    "        elif tgt_padding_mask is None and tgt_mask is not None:\n",
    "            #print(\"tgt_padding_mask is None\")\n",
    "            self_mask = tgt_mask[None,None,:,:]\n",
    "            self_mask = self_mask.expand( (tgt.size(0), self.num_heads, -1, -1 ))\n",
    "            #print( \"0 self_mask size():\", self_mask.size() )\n",
    "        elif tgt_padding_mask is None and tgt_mask is None:\n",
    "            self_mask = None\n",
    "            \n",
    "        if memory_padding_mask is not None:\n",
    "            cross_mask = memory_padding_mask[:,None,None,:]\n",
    "            cross_mask = cross_mask.expand((-1,self.num_heads, tgt.size(1), -1))\n",
    "        else:\n",
    "            cross_mask = None\n",
    "        \n",
    "        # 単語埋め込み [バッチサイズ, 系列長]\n",
    "        # -> [バッチサイズ, 系列長, 埋め込み次元]\n",
    "        embeddings = self.embed(captions) * math.sqrt( self.dim_embedding )\n",
    "        seq = embeddings.shape[1]\n",
    "        \n",
    "        # 位置エンコーディング\n",
    "        #embeddings = self.pos_enc( embeddings )\n",
    "        positions = self.pos_emb(embeddings)\n",
    "        embeddings = embeddings + positions\n",
    "        #embeddings = self.dropout( embeddings )\n",
    "        #embeddings = self.norm(embeddings)\n",
    "        \n",
    "        # Transformerデコーダでキャプション生成\n",
    "        # 画像の特徴も入力する\n",
    "        for layer in self.decoder_layers:\n",
    "            embeddings = layer( embeddings, features, self_mask, cross_mask )\n",
    "\n",
    "        \n",
    "        # [バッチサイズ, 系列長, 埋め込み次元]\n",
    "        # -> [バッチサイズ, 系列長, 辞書の次元]\n",
    "        preds = self.linear(embeddings)\n",
    "        #print( \"argmax of preds:\", torch.argmax( preds, dim = 2 ))\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa9c54fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fTransformerDecoder(nn.Module):\n",
    "    '''\n",
    "    CaptioningTransformerのコンストラクタ\n",
    "    dim_embedding  : 埋め込み次元\n",
    "    dim_feedforward: FNNの中間特徴次元\n",
    "    num_heads      : マルチヘッドアテンションのヘッド数\n",
    "    num_layers     : Transformerデコーダ層の数\n",
    "    vocab_size     : 辞書の次元\n",
    "    pad_index     : NULLのID\n",
    "    dropout        : ドロップアウト確率\n",
    "    '''\n",
    "    def __init__(self, vocab_size: int, dim_embedding: int, dim_feedforward: int,\n",
    "                 num_heads: int, num_layers: int, \n",
    "                 pad_idx: int, dropout: float=0.1, ds_rate: float=0.1 ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 単語埋め込み\n",
    "        #self.embed = nn.Embedding(\n",
    "        #    vocab_size, dim_embedding, padding_idx=pad_index)\n",
    "        \n",
    "        # 位置エンコーディング\n",
    "        #self.pos_enc = PositionalEncoding(dim_embedding)\n",
    "        self.fpos_emb = fPositionalEmbedding(dim_embedding, \"decoder.pos_emb.pos_emb.weight\" )\n",
    "        #self.dropout = nn.Dropout( dropout )\n",
    "        \n",
    "        # Transformerデコーダ\n",
    "        self.fdeclayer = fTransformerDecoderLayer(dim_hidden=dim_embedding, num_heads=num_heads, dim_feedforward = dim_feedforward, dropout = dropout )\n",
    "        \n",
    "        # 単語出力分布計算\n",
    "        #self.linear = nn.Linear(dim_embedding, vocab_size)\n",
    "        \n",
    "        self.pad_idx = pad_idx\n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    ''' CaptioningTransformerの順伝播処理\n",
    "    features: 画像特徴量 [バッチサイズ, 埋め込み次元]\n",
    "    captions: 正解キャプション [バッチサイズ, 系列長]\n",
    "\n",
    "    '''\n",
    "    def forward(self, features: torch.Tensor, captions: torch.Tensor, memory_padding_mask: torch.Tensor=None, \\\n",
    "                tgt_padding_mask: torch.Tensor=None, tgt_mask: torch.Tensor=None, weights = None ):\n",
    "\n",
    "        tgt = captions\n",
    "        if tgt_padding_mask is not None and tgt_mask is not None:\n",
    "            self_mask1 = tgt_padding_mask[:,None,:,None]\n",
    "            self_mask1 = self_mask1.expand( (-1, self.num_heads, -1, tgt.size(1)))\n",
    "            #print( \"self_mask1:\", self_mask1.size() )\n",
    "            self_mask2 = tgt_mask[None,None,:,:]\n",
    "            self_mask2 = self_mask2.expand( (tgt.size(0), self.num_heads, -1, -1 ))\n",
    "            #print( \"self_mask2:\", self_mask2.size() )\n",
    "            self_mask = torch.logical_or(self_mask1, self_mask2 )\n",
    "            #print( \"0 self_mask:\", self_mask.size() )\n",
    "        elif tgt_padding_mask is not None and tgt_mask is None:\n",
    "            self_mask = tgt_padding_mask[:,None,:,None]\n",
    "            self_mask = self_mask.expand( (-1, self.num_heads, -1, tgt.size(1)))\n",
    "        elif tgt_padding_mask is None and tgt_mask is not None:\n",
    "            #print(\"tgt_padding_mask is None\")\n",
    "            self_mask = tgt_mask[None,None,:,:]\n",
    "            self_mask = self_mask.expand( (tgt.size(0), self.num_heads, -1, -1 ))\n",
    "            #print( \"0 self_mask size():\", self_mask.size() )\n",
    "        elif tgt_padding_mask is None and tgt_mask is None:\n",
    "            self_mask = None\n",
    "            \n",
    "        if memory_padding_mask is not None:\n",
    "            cross_mask = memory_padding_mask[:,None,None,:]\n",
    "            cross_mask = cross_mask.expand((-1,self.num_heads, tgt.size(1), -1))\n",
    "        else:\n",
    "            cross_mask = None\n",
    "        \n",
    "        # 単語埋め込み [バッチサイズ, 系列長]\n",
    "        # -> [バッチサイズ, 系列長, 埋め込み次元]\n",
    "        embeddings = F.embedding( captions, weights['decoder.embed.weight'], padding_idx = self.pad_idx )  * math.sqrt( self.dim_embedding )\n",
    "        seq = embeddings.shape[1]\n",
    "        \n",
    "        # 位置エンコーディング\n",
    "        #embeddings = self.pos_enc(embeddings)\n",
    "        positions = self.fpos_emb( embeddings, weights )\n",
    "        embeddings = embeddings + positions\n",
    "        #embeddings = self.dropout( embeddings )\n",
    "        ##embeddings = self.norm(embeddings)\n",
    "        #embeddings = F.layer_norm(embeddings, (self.dim_embedding,), weight=weights['decoder.norm.weight'], bias=weights['decoder.norm.bias'], eps=1e-05)\n",
    "        \n",
    "        # Transformerデコーダでキャプション生成\n",
    "        # 画像の特徴も入力する\n",
    "        #for layer in self.decoder_layers:\n",
    "        #    #embeddings = layer( embeddings, features, tgt_key_padding_mask = padding_mask_tgt, \\\n",
    "        #    #                                memory_key_padding_mask = padding_mask_src, tgt_is_causal = True, tgt_mask = mask_tgt )\n",
    "        for block in range( self.num_layers ):\n",
    "            embeddings = self.fdeclayer(embeddings, features, self_mask, cross_mask, block, weights )\n",
    "  \n",
    "        # [バッチサイズ, 系列長, 埋め込み次元]\n",
    "        # -> [バッチサイズ, 系列長, 辞書の次元]\n",
    "        #preds = self.linear(embeddings)\n",
    "        preds = F.linear( embeddings, weights['decoder.linear.weight'], weights['decoder.linear.bias'] )\n",
    "        #print( \"argmax of preds:\", torch.argmax( preds, dim = 2 ))\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14d1d088-4485-4752-b85b-25197f08824b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim_embedding: int, dim_feedforward: int,\n",
    "                 num_heads: int, num_layers: int, enc_vocab_size: int, dec_vocab_size: int,\n",
    "                 j_pad_index: int,e_pad_index: int, dropout: float=0.1, ds_rate: float=0.1 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = TransformerEncoder(enc_vocab_size, dim_embedding, dim_feedforward, num_heads, num_layers, j_pad_index, dropout )\n",
    "        self.decoder = TransformerDecoder(dec_vocab_size, dim_embedding, dim_feedforward, num_heads, num_layers, e_pad_index, dropout )\n",
    "        self.fencoder = fTransformerEncoder(enc_vocab_size, dim_embedding, dim_feedforward, num_heads, num_layers, j_pad_index, dropout )\n",
    "        self.fdecoder = fTransformerDecoder(dec_vocab_size, dim_embedding, dim_feedforward, num_heads, num_layers, e_pad_index, dropout )\n",
    "        self.j_pad_index = j_pad_index\n",
    "        self.e_pad_index = e_pad_index\n",
    "\n",
    "        self._reset_parameters()\n",
    "        #self._reset_parameters2()\n",
    "    \n",
    "    def _reset_parameters(self):\n",
    "        r\"\"\"Initiate parameters in the transformer model.\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                xavier_uniform_(p)\n",
    "                #xavier_normal_(p)\n",
    "                #kaiming_uniform_(p)\n",
    "                #kaiming_normal_(p)\n",
    "\n",
    "    #def _reset_parameters2(self):\n",
    "    #    for module in self.modules():\n",
    "    #        if isinstance(module, nn.Linear):\n",
    "    #            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    #            if module.bias is not None:\n",
    "    #                nn.init.zeros_(module.bias)\n",
    "    #        elif isinstance(module, nn.Embedding):\n",
    "    #            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    #        elif isinstance(module, nn.LayerNorm):\n",
    "    #            nn.init.zeros_(module.bias)\n",
    "    #            nn.init.ones_(module.weight)\n",
    "        \n",
    "    ''' CaptioningTransformerの順伝播処理\n",
    "    features: 画像特徴量 [バッチサイズ, 埋め込み次元]\n",
    "    captions: 正解キャプション [バッチサイズ, 系列長]\n",
    "\n",
    "    '''\n",
    "    def forward(self, text, dec_input):\n",
    "\n",
    "        seq_len_src = text.shape[1]\n",
    "        seq_len_tgt = dec_input.shape[1]\n",
    "\n",
    "        mask_tgt = nn.Transformer.generate_square_subsequent_mask( seq_len_tgt, dtype=bool ).to(device)\n",
    "        mask_src = torch.zeros((seq_len_src, seq_len_src), device=device).type(torch.bool)\n",
    "\n",
    "        padding_mask_src = (text == idx_list['<pad>'])\n",
    "        padding_mask_tgt = (dec_input == idx_list_en['<pad>'])\n",
    "    \n",
    "        x = self.encoder( text, mask_src, padding_mask_src )\n",
    "        preds = self.decoder(x,dec_input, padding_mask_src, padding_mask_tgt, mask_tgt )\n",
    "\n",
    "        return preds\n",
    "    \n",
    "    def adaptation(self, text, dec_input, weights):\n",
    "\n",
    "        seq_len_src = text.shape[1]\n",
    "        seq_len_tgt = dec_input.shape[1]\n",
    "\n",
    "        mask_tgt = nn.Transformer.generate_square_subsequent_mask( seq_len_tgt, dtype=bool ).to(device)\n",
    "        mask_src = torch.zeros((seq_len_src, seq_len_src), device=device).type(torch.bool)\n",
    "\n",
    "        padding_mask_src = (text == idx_list['<pad>']).to(text.device )\n",
    "        padding_mask_tgt = (dec_input == idx_list_en['<pad>']).to(text.device )\n",
    "        \n",
    "        x = self.fencoder( text, mask_src, padding_mask_src, weights)\n",
    "        preds = self.fdecoder( x, dec_input, padding_mask_src, padding_mask_tgt, mask_tgt, weights )\n",
    "        \n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88539768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Transformer(512, 2048, 8, 6, enc_vocab_size, dec_vocab_size, idx_list['<pad>'], idx_list_en['<pad>'] ).to(device)\n",
    "dim_hidden = 768\n",
    "dim_feedforward = dim_hidden * 4\n",
    "heads = 12\n",
    "layers =8\n",
    "dropout = 0.1\n",
    "clip_max = 1.0\n",
    "#dim_hidden = 256\n",
    "#dim_feedforward = dim_hidden * 4\n",
    "#heads = 4\n",
    "#layers =4\n",
    "#dropout = 0.0\n",
    "model = Transformer(dim_hidden, dim_feedforward, heads, layers, enc_vocab_size, dec_vocab_size, idx_list['<pad>'], idx_list_en['<pad>'], dropout = dropout ).to(device)\n",
    "#model = MyTransformer(768, 768 * 4, 12, 12, enc_vocab_size, dec_vocab_size, idx_list['<pad>'], idx_list_en['<pad>'], start_idx = start_idx_t, max_seq = 200  ).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=idx_list_en['<pad>'])\n",
    "loss_fn = criterion\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b845bbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs1 size: torch.Size([8, 120, 20556])\n",
      "tensor(-0.1940, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "outputs2 size: torch.Size([8, 120, 20556])\n",
      "tensor(-0.1940, device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): TransformerEncoder(\n",
       "    (embed): Embedding(49528, 768, padding_idx=0)\n",
       "    (pos_emb): PositionalEmbedding(\n",
       "      (pos_emb): Embedding(5000, 768)\n",
       "    )\n",
       "    (encoder_layers): ModuleList(\n",
       "      (0-7): 8 x TransformerEncoderLayer(\n",
       "        (attention): MHA(\n",
       "          (proj_in_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_in_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_in_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_out): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (fnn): FNN(\n",
       "          (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELU(approximate='none')\n",
       "        )\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (embed): Embedding(20556, 768, padding_idx=0)\n",
       "    (pos_emb): PositionalEmbedding(\n",
       "      (pos_emb): Embedding(5000, 768)\n",
       "    )\n",
       "    (decoder_layers): ModuleList(\n",
       "      (0-7): 8 x TransformerDecoderLayer(\n",
       "        (selfattn): MHA(\n",
       "          (proj_in_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_in_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_in_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_out): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (crossattn): MHA(\n",
       "          (proj_in_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_in_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_in_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_out): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (fnn): FNN(\n",
       "          (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELU(approximate='none')\n",
       "        )\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (linear): Linear(in_features=768, out_features=20556, bias=True)\n",
       "  )\n",
       "  (fencoder): fTransformerEncoder(\n",
       "    (fpos_emb): fPositionalEmbedding()\n",
       "    (fenclayer): fTransformerEncoderLayer(\n",
       "      (fattention): feMHA()\n",
       "      (fffn): feFNN(\n",
       "        (activation): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (fdecoder): fTransformerDecoder(\n",
       "    (fpos_emb): fPositionalEmbedding()\n",
       "    (fdeclayer): fTransformerDecoderLayer(\n",
       "      (fselfattn): fdsMHA()\n",
       "      (fcrossattn): fdcMHA()\n",
       "      (fffn): fdFNN(\n",
       "        (activation): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#seed = 0\n",
    "\n",
    "#random.seed(seed)\n",
    "#np.random.seed(seed)\n",
    "#torch.manual_seed(seed)\n",
    "#torch.backends.cudnn.benchmark = False\n",
    "#torch.backends.cudnn.deterministic = True\n",
    "\n",
    "text = torch.randint( 0, enc_vocab_size, size=(8, 100 ))\n",
    "dec_input = torch.randint( 0, dec_vocab_size, size=(1,120))\n",
    "\n",
    "seq_len_src = text.shape[1]\n",
    "seq_len_tgt = dec_input.shape[1]\n",
    "\n",
    "mask_tgt = nn.Transformer.generate_square_subsequent_mask( seq_len_tgt, dtype=bool ).to(device)\n",
    "mask_src = torch.zeros((seq_len_src, seq_len_src), device=device).type(torch.bool)\n",
    "\n",
    "padding_mask_src = (text == idx_list['<pad>'])\n",
    "padding_mask_tgt = (dec_input == idx_list_en['<pad>'])\n",
    "\n",
    "weights = OrderedDict(model.named_parameters())\n",
    "model.eval()\n",
    "outputs1 = model( text.to(device), dec_input.to(device) )\n",
    "print( \"outputs1 size:\", outputs1.size())\n",
    "print( outputs1[0][0][0])\n",
    "\n",
    "#for name in weights:\n",
    "#    print( name )\n",
    "model.eval()\n",
    "outputs2 = model.adaptation( text.to(device), dec_input.to(device), weights )\n",
    "print( \"outputs2 size:\", outputs2.size() )\n",
    "print( outputs2[0][0][0])\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b01dfbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_global_steps: 249500\n",
      "num_warmup_steps: 24950.0\n",
      "Train\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 8.012024048096192e-06\n",
      "pred: the are the the the commission of the the of are to <unk> and the european that the the and the of the of to and the european <unk> and <eos>\n",
      "tar:  they rightly pointed to the issue of food security they based their arguments on the fact that in agriculture only 10% of production is traded on the international market <eos>\n",
      "pred: i the are <eos>\n",
      "tar:  thank you commissioner vitorino <eos>\n",
      "epoch:1  Step:2000  loss:5.8646831512  WER:0.7876712329\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: the the are the <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: the is is be the the of to the council of is the the council <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:25  loss:5.7681643295  WER:0.8165668302\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 1.602805611222445e-05\n",
      "pred: the council of the council has been a and that the rights and the important and and the and and the and still the done and and <eos>\n",
      "tar:  the president-in-office of the council has quite rightly said that human rights are an indispensable requirement if peace stability and prosperity are to be secured <eos>\n",
      "pred: we have already this is the <eos>\n",
      "tar:  we have heard this before <eos>\n",
      "epoch:1  Step:4000  loss:4.9611964226  WER:0.7511312217\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: the the is the of <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: in is is have that <unk> in in the other of of the the world of of <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:50  loss:5.0101638412  WER:0.7721286881\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 2.4044088176352706e-05\n",
      "pred: it is be the to be in now the the now last time of the made by to the last and the is and the few of made to <eos>\n",
      "tar:  that cannot be allowed to happen and for that reason the same amount is being made available for the balkans as this year with a part being brought forward to the end of the current budget year <eos>\n",
      "pred: in the is to the new countries on the of the the the countries countries in are countries the on the the of be a into the countries to the countries of the the new countries of are the the used <eos>\n",
      "tar:  when it comes to the continued ban on sales of <unk> to other eu countries we swedish christian democrats consider that account should be taken of the unwillingness of these countries to introduce a new product they consider to be <unk> and destructive from a public health point of view <eos>\n",
      "epoch:1  Step:6000  loss:4.6484732628  WER:0.7593582888\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: the there are problems of problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: it is has has many industry in in the area of or the the area of <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:75  loss:4.6035454178  WER:0.7613849153\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 3.2060120240480964e-05\n",
      "pred: the issue way is as the been made by the the task of the rights and and and with and the as as <eos>\n",
      "tar:  the operative approaches taken as has been said previously promote the defence of human rights development work dialogue peace and democracy <eos>\n",
      "pred: in is why the the are are the <eos>\n",
      "tar:  that is because all instruction etc in <unk> first takes place in english <eos>\n",
      "epoch:1  Step:8000  loss:4.4204754829  WER:0.7335701599\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: we problems are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: we is has has the sectors sector in the field industry or the the sector sector sector <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:100  loss:4.3431129837  WER:0.7346165143\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 4.007615230460922e-05\n",
      "pred: the than the debate of debate is expressed that the is be account debate account of the position on <eos>\n",
      "tar:  more to the point the commission has said that it will take the fullest account of parliament's position <eos>\n",
      "pred: i of of the group of the european on of are that the is a a group parliamentary of <eos>\n",
      "tar:  some members of my group reject the report because they believe that this is not a european issue <eos>\n",
      "epoch:1  Step:10000  loss:3.7716343403  WER:0.6704119850\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems in problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: in is also includes many jobs industry in the sector industry in in many sector sector industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:125  loss:4.1431173325  WER:0.7199540280\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 4.8092184368737474e-05\n",
      "pred: the do however the commission of the and the in the european of the commission in the and the european union and is the single of the <unk> in the european which is not friendly <eos>\n",
      "tar:  we cannot compare the survival of small rum producers in the canaries with the situation of sugar in the european union which is a cartel providing massive profits for a industry that is highly protected and which the commission is not able - or willing - to end <eos>\n",
      "pred: i i all i am not disappointed that i rapporteur life i democrats are not be a as just but a social of the but i a a issue of <eos>\n",
      "tar:  first of all i am very glad that the human and social sciences will henceforth be regarded not just as a means of research but also as an end <eos>\n",
      "epoch:1  Step:12000  loss:3.7669963837  WER:0.6845018450\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: in is also involves a jobs jobs in europe industry industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:150  loss:3.9988853359  WER:0.7122727512\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 5.6108216432865735e-05\n",
      "pred: we we must need to take resolutions resolutions in the issue <eos>\n",
      "tar:  so we still have to adopt un resolutions on this subject <eos>\n",
      "pred: the spanish authorities and and is the and is views is from the own press in it spanish of in the public is as the and the language not <eos>\n",
      "tar:  the spanish government bans newspapers and radio stations whose opinions differ from its own especially when the issue is that another language such as catalan or basque is involved <eos>\n",
      "epoch:1  Step:14000  loss:3.7509870529  WER:0.6759868421\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: in is is has a jobs jobs in the industry industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:175  loss:3.8639644432  WER:0.7183220945\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 6.412424849699399e-05\n",
      "pred: i this i am i time of of i am like to express out that the parliament as i i european parliament has the time time i am not want in much i european parliament has the i am the the vote of the a am sure so as i the speech of vote in the european of of <eos>\n",
      "tar:  at first i felt a bit guilty but i would like to point out on this occasion how much the european parliament saves every second when i do not speak how much the european parliament saves when i keep to the time limit as i am doing now and finish my explanation of vote within the sixty seconds <unk> <eos>\n",
      "pred: i urge urge to the agreement authorities agreement authorities to reach so to to reach agreement agreement agreement on the agreement made the agreement agreement <eos>\n",
      "tar:  i would appeal to the israeli and palestinian authorities to do everything possible to negotiate a settlement based on the recommendations of the mitchell report <eos>\n",
      "epoch:1  Step:16000  loss:3.7113592625  WER:0.6946902655\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: in industry has has jobs jobs jobs in the industry industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:200  loss:3.7113111210  WER:0.6890429832\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 7.214028056112225e-05\n",
      "pred: we therefore believe that is essential that ensure that safety security for being to the that <eos>\n",
      "tar:  we therefore believe it is essential to ensure sufficient financial resources are available to ensure decommissioning takes place safely <eos>\n",
      "pred: i would the commission in in the committee <eos>\n",
      "tar:  i remember the vote held on the matter in the committee on legal affairs and the internal market <eos>\n",
      "epoch:1  Step:18000  loss:3.4302814007  WER:0.6474694590\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: in creates also believes jobs jobs jobs or the industry industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:225  loss:3.5307490158  WER:0.6701598752\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 8.015631262525051e-05\n",
      "pred: we is in to the industry of in the to the than more companies are concerned the of are that is to to we are to maintain in in <eos>\n",
      "tar:  this particularly applies to the paper industry at present where more and more companies are merging because they believe this is essential if they are to remain competitive <eos>\n",
      "pred: the commission has made to the proposal on the reunification is be to on the a common and year step and achieve a common european policy policy and and of of of of of of of of of of of of of of of of of of of of of <eos>\n",
      "tar:  the commission has decided that a proposal on family reunification should start us off on this journey this considerable effort to construct a common european immigration policy <eos>\n",
      "epoch:1  Step:20000  loss:3.4308419228  WER:0.6785714286\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are some <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore creates creates creates many jobs jobs in the sector industry or industry the sector industry industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:250  loss:3.4106709480  WER:0.6689306317\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 8.817234468937876e-05\n",
      "pred: in the contrary the is in that the previous to the countries and and and their <unk> and clearly and the the business and and <unk> state have clearly and and and the interests and clearly to the clearly and they were been single for concern <eos>\n",
      "tar:  on the contrary it was clear in the run-up to accession <unk> public corporations and their wealth were sold off to big business abroad any socialist achievements were abolished sovereign rights and national independence were restricted and so on) that they have every cause for concern <eos>\n",
      "pred: the far the the from the event of the cancellation is not the for the event to right is justified to a result of the agreement <eos>\n",
      "tar:  as to compensation apart from the cases where the debtor is not responsible for the delay the creditor is entitled as a result of this agreement to claim reasonable compensation from the debtor for all recovery costs incurred <eos>\n",
      "epoch:1  Step:22000  loss:3.3594708443  WER:0.6638655462\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this creates also creates many jobs jobs in the industry industry in in the industry industry industry in <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:275  loss:3.3016000271  WER:0.6500581609\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 9.618837675350701e-05\n",
      "pred: i this context i would like to highlight a elements <eos>\n",
      "tar:  in this connection i would like to emphasise various points: <eos>\n",
      "pred: but is no sense to to set the and and regions in in the regions disadvantaged regions such it is done in of the or or the west and to in example reasons that the have be be to use the not a in programmes in the reserve deal of funding to to from fund funding from the areas in such the with <eos>\n",
      "tar:  it makes no sense though to destroy today businesses and jobs especially in disadvantaged structurally weak areas whether this be done out of <unk> <unk> to the wto or conceivably for ideological reasons since we will tomorrow have to replace or create anew these jobs with a great deal of effort and expense to public funds using rural development programmes to do it? <eos>\n",
      "epoch:1  Step:24000  loss:3.4215683937  WER:0.9029126214\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems of <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore is is creates a jobs jobs in the industry industry and the the industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:1  Step:300  loss:3.2030574989  WER:0.7146174707\n",
      "Train\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 9.95328434647072e-05\n",
      "pred: who anyone wish want to speak against the <unk> <eos>\n",
      "tar:  does any member wish to speak against the request? <eos>\n",
      "pred: in the on were a points with the convergence on are discussing the and we are some some in are defects contained in the resolution are are be on the on certain at the some some some some some some the some some some some some some\n",
      "tar:  in committee there was some agreement with the communication we are assessing here but there are also shortcomings which the proposals contained in the resolution we will vote on later are aimed at resolving <eos>\n",
      "epoch:2  Step:26000  loss:2.8049683571  WER:0.6919315403\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector creates creates jobs jobs jobs in the industry industry or in the construction industry industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:325  loss:3.1071467781  WER:0.6294292663\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 9.864217323535962e-05\n",
      "pred: the is the that the member states to stop to renew the embargo <eos>\n",
      "tar:  it is irresponsible of eu member states to refuse to renew the embargo <eos>\n",
      "pred: we are however support this establishment of this constitution area is the step step towards creating the creation of the european borders and european european basis and therefore on the to european european centre of <eos>\n",
      "tar:  we do however welcome the establishment of this one it being a first step towards putting the defence of the external borders on a community footing and consequently transferring competences to the european level <eos>\n",
      "epoch:2  Step:28000  loss:3.0143809319  WER:0.6104129264\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this sector creates creates jobs jobs jobs in the past industry or in the past sector <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:350  loss:2.9953919125  WER:0.6561008221\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 9.775150300601203e-05\n",
      "pred: ultimately the ultimately a final of a period period of three and has been and which major national national economies as the <eos>\n",
      "tar:  are not though the effects of a three-year period of stagnation which has shaken and battered europe’s major national economies in particular ultimately every bit as bad as a <unk> <eos>\n",
      "pred: the report leaves the to issue of gibraltar gibraltar <eos>\n",
      "tar:  this report leaves open the question of the <unk> vote <eos>\n",
      "epoch:2  Step:30000  loss:2.8492460251  WER:0.5548060708\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there is problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this is creates creates jobs jobs jobs in the industry industry or in the processing industry industry in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:375  loss:2.9015392971  WER:0.6001255560\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 9.686083277666445e-05\n",
      "pred: the only way to to make life easier difficult difficult but the member states but they are not have anything they this field and they the public of public public to be made in transparent as possible <eos>\n",
      "tar:  the only way is to make it genuinely very embarrassing for the member states if they do not do something in this area and for the status of their efforts to be made as public as possible <eos>\n",
      "pred: globalisation and competition in technology means be a human and and everyone able humane <eos>\n",
      "tar:  globalisation and competitiveness using technology must have a human face and be more inclusive <eos>\n",
      "epoch:2  Step:32000  loss:2.6740195751  WER:0.6035598706\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this is creates creates many jobs jobs in the processing industry and in the processing <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:400  loss:2.7545316887  WER:0.5876532884\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 9.597016254731685e-05\n",
      "pred: this directive is to establish a regulate a system system for civil reporting in a aviation which all that all players are have access to information accidents <eos>\n",
      "tar:  this directive seeks to establish and regulate a formal system for occurrence reporting in civil aviation to ensure that all players involved have access to these incidents and are aware of them <eos>\n",
      "pred: on this basis i voted for mrs buitenweg report and i wanted tabled the <unk> s which he that report of the report by <eos>\n",
      "tar:  on this ground i voted for the buitenweg report and i previously signed mr <unk> declaration which provided the occasion for the report <eos>\n",
      "epoch:2  Step:34000  loss:2.9281687737  WER:0.6107594937\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems of <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this creates creates creates numerous jobs jobs on the processing of and the the processing industry industry in in in in in in in in in of in in in in in in in of in in in in in in in in in in in in in in in in in in in in of in in in in in in in\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:425  loss:2.6556109428  WER:0.5946616627\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 9.507949231796927e-05\n",
      "pred: we can use our instruments and and us and to this view mind of possibility of being discussed by a a procedure procedure for regard to article under accordance of article 96 of the cotonou agreement <eos>\n",
      "tar:  we can use the tools available to us and with that in view the possibility is being examined of instituting a consultancy procedure with regard to liberia in application of article 96 of the cotonou agreement <eos>\n",
      "pred: mr president the recommendations on health are important of the most important points on of the white paper on food safety <eos>\n",
      "tar:  mr president the recommendations on health are some of the most important action points in the white paper on food safety <eos>\n",
      "epoch:2  Step:36000  loss:2.6852989197  WER:0.5394456290\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this also also creates many jobs jobs in the processing processing or in the processing <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:450  loss:2.5372328854  WER:0.5399864064\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 9.418882208862169e-05\n",
      "pred: when talks talking about transport transport in that <unk> <unk> <unk> <unk> of or board <unk> or <unk> thinks the train about and the <unk> and the <unk> and and it does talking vehicle of the handling vehicle <eos>\n",
      "tar:  nobody when talking about animal transport means the <unk> in the hand luggage on an aircraft and nobody mentions the little dog on a lead on the <unk> train and certainly not the <unk> in its specialised <unk> on the motorway <eos>\n",
      "pred: my second thing i would to say is that if we do not support support support in the cooperation solidarity we will not the necessary solidarity of to be firm on terrorists <eos>\n",
      "tar:  the second thing i want to say is that if we do not gather grassroots support for further european cooperation we shall lack the communal spirit needed to stand firm against terrorists <eos>\n",
      "epoch:2  Step:38000  loss:2.5439820290  WER:0.5192307692\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: in sector creates creates many jobs jobs on the land industry and in the south industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:475  loss:2.4692864513  WER:0.5406312935\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 9.329815185927411e-05\n",
      "pred: the way of have to work preparations preparation to prepare the report have that the staff of personnel committee s staff of generally dependent from of of of <eos>\n",
      "tar:  the efforts we needed to make in order to finish this report suggest that the secretariat of the women' s committee is heavily <unk> <eos>\n",
      "pred: i share the concern about the a rights for european development cooperative society and be for rights and and the development of these rights both in a <unk> constitution a up a a the case process already the <unk> <eos>\n",
      "tar:  i share the concern that creating the statute for the european cooperative society should respect employees' acquired rights and the protection of these rights both when an <unk> is set up and in the structural changes of an <unk> <unk> <eos>\n",
      "epoch:2  Step:40000  loss:2.4711670876  WER:0.5096322242\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: in sector also creates many jobs jobs on the south industry or in the processing processing <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:500  loss:2.4081277657  WER:0.5481588879\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 9.240748162992653e-05\n",
      "pred: on the field of foreign eu' foreign policy the second was given the second expectations response of the a appointment of a foreign foreign foreign for foreign affairs which will be the foreign affairs council and will on prepare that necessary of the common common foreign policy <eos>\n",
      "tar:  in the field of the union's foreign policy the convention has met the citizens' second expectation by proposing the appointment of a european union minister for foreign affairs who will chair the foreign affairs council and work to ensure the development of a much-needed common foreign policy <eos>\n",
      "pred: the first is that i represent wales in the european parliament <eos>\n",
      "tar:  the first is that i represent wales in the european parliament <eos>\n",
      "epoch:2  Step:42000  loss:2.3876707554  WER:0.4570858283\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates many jobs jobs on the processing industry and in the processing industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:525  loss:2.3452079201  WER:0.5284669930\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 9.151681140057894e-05\n",
      "pred: the this respect the european has expressed great satisfaction at the establishment up of the national defence <eos>\n",
      "tar:  in that connection the eu has expressed great satisfaction with the setting up of afghanistan's national defence council <eos>\n",
      "pred: i am a supporter supporter of the european and for been to time long time to have common rules on the and refugee policy but we are are not prepared to accept a of of fortress <eos>\n",
      "tar:  i am a keen supporter of the eu and have wanted for a long time to see common rules within refugee and asylum policy but we liberals are not prepared to accept some kind of <unk> europe <eos>\n",
      "epoch:2  Step:44000  loss:2.1453938484  WER:0.4600760456\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: in also also creates many jobs jobs on the occupied industry or in the <unk> <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:550  loss:2.3128599358  WER:0.5030614180\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 9.062614117123136e-05\n",
      "pred: before the vote on amendment no 18 <eos>\n",
      "tar:  before the vote on amendment no 18 <eos>\n",
      "pred: first of effort to the rights of passengers companies is is at european level and led a being at consultation consultation in european and the airlines and well as the and the with disabilities which specific of to produce to the requirements requirements for the the rights of all the <eos>\n",
      "tar:  first the endeavour concerning the rights of airline passengers this consultation at european level which is now ready for signing involved the airlines and the commission as well as groups of people with disabilities the aim being to agree on specific binding undertakings to safeguard the rights of all passengers and people with disabilities <eos>\n",
      "epoch:2  Step:46000  loss:2.0795028210  WER:0.5478424015\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates many jobs jobs on the processing industry or in the shipyards industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:575  loss:2.2622343159  WER:0.4925692641\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 8.973547094188376e-05\n",
      "pred: it report been prepare a report report and the the to the authorities authorities authorities that are have the task of carrying the site of regular intervals and also to ensuring that external external plans plans are implemented into place regularly that regularly <eos>\n",
      "tar:  <unk> has to prepare a safety report and send it to the public control authorities these authorities have the task of inspecting the site at regular intervals and also of ensuring that its external emergency plans are put in place and tested periodically <eos>\n",
      "pred: that that is a lesson to we must to make <eos>\n",
      "tar:  and that is a lesson that we have to take to heart <eos>\n",
      "epoch:2  Step:48000  loss:2.5548784733  WER:0.5111111111\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates many jobs jobs on the industry industry or in the shipyards industry industry in in in in in in in in in in in in in in in in or in in in in or in in in in in in in in in in in in in in in in in in in or in in in in\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:2  Step:600  loss:2.2266741848  WER:0.5272146329\n",
      "Train\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 8.88448007125362e-05\n",
      "pred: why? <eos>\n",
      "tar:  why? <eos>\n",
      "pred: parliament challenged the regulations passed the european court of justice on 30 april of on the was the on 25 april 1999 <eos>\n",
      "tar:  parliament <unk> the regulations with the european court of justice on 30 april 1997 and this was resolved on 25 april 1999 <eos>\n",
      "epoch:3  Step:50000  loss:1.9860001802  WER:0.5324675325\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates many jobs jobs on the processing industry and in the construction <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:625  loss:2.1805952358  WER:0.4729212465\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 8.79541304831886e-05\n",
      "pred: we also also pleased with see the galileo of the galileo programme and the choices to promote towards peace in the middle east <eos>\n",
      "tar:  we are also pleased to see the launching of the galileo programme and the decisions to work for peace in the middle east <eos>\n",
      "pred: it is also crucial that the that the co-legislator co-legislator the council will open open open own open closed doors <eos>\n",
      "tar:  it is also crucial to recognise that as a co-legislator the council should not hold its discussions behind closed doors and that it should be open in exercising its political responsibilities <eos>\n",
      "epoch:3  Step:52000  loss:1.8951447010  WER:0.4329371817\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also faces many jobs jobs on the processing processing or in the processing industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:650  loss:2.1789498949  WER:0.4729866778\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 8.706346025384102e-05\n",
      "pred: the uribe presence here in that the european union is moving closer a involved in the plan plan through which the usa uses in by this country to order appalling of gain the control of the region <eos>\n",
      "tar:  mr <unk> presence here shows that the european union is moving towards getting involved in the <unk> plan through which the usa intervenes cruelly in this country in an aim to achieve general control over the area <eos>\n",
      "pred: finally on the subject of <unk> <unk> question i am the satisfaction expressed for the visit of palestine delegation in palestine <eos>\n",
      "tar:  lastly on the subject of the israeli-palestinian question i understand the satisfaction expressed at the visit of our delegation to palestine <eos>\n",
      "epoch:3  Step:54000  loss:2.0760402679  WER:0.4644808743\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore industry also creates many jobs jobs on the land industry or in the construction <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:675  loss:2.1513323689  WER:0.4625538064\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 8.617279002449343e-05\n",
      "pred: the text refers to the seamen greek but but the title only only to the seamen <eos>\n",
      "tar:  the text refers to greek and <unk> sailors but the title refers only to greek sailors <eos>\n",
      "pred: the lesson is need learn is that learn that has has that the policy only out that only with the proactive policies action on employment legislation can real be possible to open up real room of jobs and therefore to the children children in italy and in <eos>\n",
      "tar:  the lesson we must learn is to realise as this <unk> to employment often pointed out that only through truly reformist political action on employment legislation will it be possible to open up real areas for employment and hence for our children's future in italy and europe <eos>\n",
      "epoch:3  Step:56000  loss:2.1753580570  WER:0.4725457571\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this also also creates many jobs jobs on the land industry and in the processing processing industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:700  loss:2.1387464523  WER:0.4517204524\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 8.528211979514585e-05\n",
      "pred: the cap and the funds are areas areas which most of of occur partly the eu partly as to the amount of funding funding they <eos>\n",
      "tar:  the cap and structural funds are the areas where most mismanagement problems lie in the eu partly due to the scale of the finance received by these areas <eos>\n",
      "pred: yet of of offering hope and enthusiasm this this this to to be the rise to the about the <eos>\n",
      "tar:  instead however of awakening hope and enthusiasm present developments appear rather to be giving rise to anxiety among governments <eos>\n",
      "epoch:3  Step:58000  loss:1.8167469501  WER:0.3992537313\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: in sector also creates many jobs jobs on the processing industry and in the shipyards industries <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:725  loss:2.1017933321  WER:0.4810061931\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 8.439144956579827e-05\n",
      "pred: this report by by the committee on employment and social affairs takes a position on the report report <eos>\n",
      "tar:  the report submitted by the committee on employment and social affairs adopts a position on this commission report <eos>\n",
      "pred: on are two very concepts of very on this firstly the one hand there need on by the trade of the trade liberalisation the aid subsidies to on the other hand pressure s internal budgetary pressures have with with third relations with countries <eos>\n",
      "tar:  there are two fundamental courses of development behind this: on the one hand the pressures caused by the liberalisation of world trade on reducing export aid and on the other the eu' s internal budgetary pressures mainly associated with external relations <eos>\n",
      "epoch:3  Step:60000  loss:2.1764564514  WER:0.4688644689\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: in sector also creates many jobs jobs in the firm industry or in the yards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:750  loss:2.0857922125  WER:0.4732924934\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 8.350077933645069e-05\n",
      "pred: however a several months there war in been waged in chechnya which is of these arguments is be <eos>\n",
      "tar:  however for several months a war has been raging in chechnya which none of these arguments can justify <eos>\n",
      "pred: please please me put the request of soon as possible so that the can inform the up <eos>\n",
      "tar:  please let me have a note as soon as possible so that i can take it up with the authorities concerned <eos>\n",
      "epoch:3  Step:62000  loss:1.7612857819  WER:0.3741134752\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: moreover also also creates many jobs jobs on the face industry and in the shipyards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:775  loss:2.0770762968  WER:0.4457544579\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 8.26101091071031e-05\n",
      "pred: i have listened listening to attentively to the the been said but i cannot comment on the views of the comments expressed forward by the council has not yet defined a position <eos>\n",
      "tar:  i have been listening very carefully to what has been said unfortunately i cannot comment on the substance of the views put forward as the council has not yet adopted a position <eos>\n",
      "pred: i would like to pick three points in this to this <eos>\n",
      "tar:  i would like to make three points in relation to this <eos>\n",
      "epoch:3  Step:64000  loss:1.7802928686  WER:0.3629489603\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates many jobs jobs on the processing industry and in the shipyards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:800  loss:2.0617894602  WER:0.4516482051\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 8.171943887775552e-05\n",
      "pred: i therefore i at least i personally accept accept the no 168 presented to the chairman of the committee on regional policy transport regional subject <eos>\n",
      "tar:  we - or at least i personally - accept amendment no 168 presented by the chairman of the committee on regional policy on this issue <eos>\n",
      "pred: our regards matter our restraint are be not not not only to the years also also <unk> than 150 <unk> and is a a for a information information campaign to be against the the on the when these <unk> space is <unk> <eos>\n",
      "tar:  as a result child seats will apply to children not only below twelve but are also shorter than 150 <unk> there is also demand for an <unk> information <unk> to warn against transporting children in seats when the air bag is activated <eos>\n",
      "epoch:3  Step:66000  loss:1.8437908888  WER:0.3982869379\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector creates creates many jobs jobs on the firm industry or in the shipbuilding industry industry the the the the the the the the the the the the the the the into the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:825  loss:2.0444264841  WER:0.4545606218\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 8.082876864840792e-05\n",
      "pred: the actions of is the is are not protect the environment outside our waters is not popular matter popular reflection and public the the the the the the the the the the the the the the the the the the the the the the the the of the the the the the the the the the the the the the the the the the\n",
      "tar:  the whole question of what we do to protect marine life outside our waters is not a very popular issue <eos>\n",
      "pred: such a situation and of violence requires a serious political response <eos>\n",
      "tar:  such a destructive cry of despair demands a serious political answer <eos>\n",
      "epoch:3  Step:68000  loss:1.9523164034  WER:0.5259391771\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates many jobs jobs on the land industry or in the shipyards industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:850  loss:2.0236618090  WER:0.4518214307\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 7.993809841906034e-05\n",
      "pred: amendments which undermine attempts to modernise the directives by reducing the scope of using electronic procurement to amendments nos 70 in parts 73 73 102 117 and and and 102 <eos>\n",
      "tar:  amendments which weaken attempts to modernise the directives by reducing the scope for using electronic procurement namely amendments nos 70 in part 73 75 102 103 115 117 and 131 <eos>\n",
      "pred: opening public public on public up procurement is serves small companies to be up the suppliers on over the world scale <eos>\n",
      "tar:  placing no limit on opening public procurement merely allows large suppliers to swallow up small suppliers all over the world <eos>\n",
      "epoch:3  Step:70000  loss:1.8729525805  WER:0.4818355641\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore industry also creates many jobs jobs on the industry industry or in the shipyards industry industry the and the and the the the the the the and the the the the the the the the the the and the and <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:875  loss:1.9884775114  WER:0.4561102925\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 7.904742818971276e-05\n",
      "pred: basically have basically been about the best practices of the member states by create a european european regulations <eos>\n",
      "tar:  we have essentially brought together the best practices of our member states to form a new european standard <eos>\n",
      "pred: the commission has decided to table a proposal which does only complies the ceiling resource ceiling but creates creates sufficient room for the average <eos>\n",
      "tar:  the commission has decided to present a proposal which not only respects this own resources ceiling but also allows sufficient scope below this ceiling based on the average over the period <eos>\n",
      "epoch:3  Step:72000  loss:1.9669544697  WER:0.4517304189\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore creates creates creates many jobs jobs on the manufacturing industry or in the yards industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:900  loss:1.9784897900  WER:0.4469167950\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 7.815675796036518e-05\n",
      "pred: with regard to the comments made the draft report on pluralism pluralism of media media it is be stressed that that digital technology in the particular offers internet offer unprecedented possibilities for the and developing a pluralist society <eos>\n",
      "tar:  with regard to the comments on the draft report concerning the pluralism of the media it should be noted here that digital technology and in particular the internet presents unprecedented possibilities for creating and developing a pluralist society <eos>\n",
      "pred: that is why it is important for us to carefully the the views that is have given <eos>\n",
      "tar:  that is why it is important for us to follow closely the advice that we are given <eos>\n",
      "epoch:3  Step:74000  loss:2.0664911270  WER:0.4405940594\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates many job jobs on the processing industry and in the yards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:3  Step:925  loss:1.9647376537  WER:0.4249356055\n",
      "Train\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 7.726608773101759e-05\n",
      "pred: (da) tourism is a source source of understanding understanding between is important part of the labour market which should to be promoted and promoted <eos>\n",
      "tar:  (da) tourism is a significant source of international understanding and an important part of the labour market which ought to be promoted and encouraged <eos>\n",
      "pred: civil the civil is to be itself for for it must to duty to <unk> it the and and <unk> it through education of education and repression <eos>\n",
      "tar:  if a society wishes to call itself truly civilised it has a duty to track down prohibit punish and eradicate racism by means of education and prosecution <eos>\n",
      "epoch:4  Step:76000  loss:1.8817173243  WER:0.4014084507\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore industry also creates many jobs jobs on the land industry and in the yards industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:950  loss:1.9712823248  WER:0.4389758140\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 7.637541750167001e-05\n",
      "pred: this is a important <eos>\n",
      "tar:  that is very important <eos>\n",
      "pred: i am not in favour of wasting money but the must want that keeping on the promises and historical historical opportunities <eos>\n",
      "tar:  i am not in favour of wasting money but i do believe in delivering on our promises and seizing historical opportunities <eos>\n",
      "epoch:4  Step:78000  loss:1.5342755318  WER:0.4437984496\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: moreover sector also creates many jobs jobs on the land industry and in the yards industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:975  loss:1.9777016878  WER:0.4261710680\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 7.548474727232243e-05\n",
      "pred: it border or group of people who to take part in legitimate demonstrations should not be allowed by all borders or is to cannot be denied the right to travel borders and and are be denied the of movement as is a fundamental right granted to all eu of the union <eos>\n",
      "tar:  any individual or group of people wishing to take part in legitimate demonstrations should not be blocked at the borders that is they cannot be denied the right to cross frontiers <unk> they cannot be denied freedom of movement which is a fundamental right granted to all citizens of the union by the treaty <eos>\n",
      "pred: in believe rather that it would be in a less financial support from industry industry and the would not be <eos>\n",
      "tar:  i would suggest that it might result in even less financial support to the industry and markets would not improve <eos>\n",
      "epoch:4  Step:80000  loss:1.4909702539  WER:0.3596899225\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore also also creates many jobs jobs on the land industry and in the shipyards industry industry to the the the to to the the the the the the to the to the the the the the and the to the the transparent the to the the to to the the the transparent to the the and the to the the the to\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:1000  loss:1.9658697605  WER:0.4458425016\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 7.459407704297483e-05\n",
      "pred: i concludes where i want to say <eos>\n",
      "tar:  that is all i have to say <eos>\n",
      "pred: the current notification instrument of and gentlemen will longer allows this to achieve this objective because it does not guarantee the the commission will informed informed of the most serious restrictions which i would point you that decisions the years only have been been a decisions of the decisions ban is been on prohibition decisions notification and the absence of the complaint and and does it guarantee real and legal a legal certainty for companies which in most form of cases are the simple administrative letter of\n",
      "tar:  the current notification instrument ladies and gentlemen no longer allows us to reach this objective because it does not guarantee that the commission is properly informed about the most serious restrictions - i would remind you that in 35 years there have only been nine cases in which the commission has decided on prohibition following notification in the absence of a complaint - nor does it guarantee transparency or provide real legal certainty for businesses which in the majority of cases receive a simple comfort letter <eos>\n",
      "epoch:4  Step:82000  loss:1.7076869011  WER:0.4072398190\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore industry creates creates many jobs jobs on the industry industry and in the yards industry industry the the and the the the the the the the the the <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:1025  loss:1.9564827442  WER:0.4200609349\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 7.370340681362727e-05\n",
      "pred: thank you mr much mr president <eos>\n",
      "tar:  thank you very much mr president of the commission <eos>\n",
      "pred: we believe that the light of the over recent years that the quantitative approach that a very important role namely the words the definition for to ensure the targets in we is to be pursued and governments will to be judged on an basis of objective objectives <eos>\n",
      "tar:  we believe in the light of experience over recent years that the quantitative approach plays a very important part in other words the guidelines need to include quantitative targets if policy is to be quantifiable and governments need to be judged on the basis of specific targets <eos>\n",
      "epoch:4  Step:84000  loss:1.7491103411  WER:0.4027237354\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: in sector also creates many jobs jobs on the firm industry or in the shipyards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:1050  loss:1.9610654926  WER:0.4468115004\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 7.281273658427967e-05\n",
      "pred: this is something issue that am concerned about <eos>\n",
      "tar:  this is an area i am concerned about <eos>\n",
      "pred: mr president commissioner listening you i air debate i one of the traffic control <eos>\n",
      "tar:  mr president commissioner like aviation the present issue is reminiscent of air traffic control <eos>\n",
      "epoch:4  Step:86000  loss:1.7123914957  WER:0.3939393939\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also generates many jobs jobs on the processing industry or in the yards industry industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:1075  loss:1.9485000658  WER:0.4170857868\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 7.19220663549321e-05\n",
      "pred: there is also talk of forced labour in the development of infrastructure and in the is become the world's s second biggest drug <unk> country after bolivia <eos>\n",
      "tar:  there is also use of forced labour in the construction of infrastructure and meanwhile burma has become the world' s second largest drugs exporting country after bolivia <eos>\n",
      "pred: i have however like to make two fundamental observations <eos>\n",
      "tar:  i should nevertheless like to make two crucial observations <eos>\n",
      "epoch:4  Step:88000  loss:1.5874111652  WER:0.3774597496\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also generates many jobs jobs on the firm industry and in the construction industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:1100  loss:1.9237131691  WER:0.4136228011\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 7.10313961255845e-05\n",
      "pred: parliament has received in in support support to the commission the council has played its part and it is the time to decide on the tenders <eos>\n",
      "tar:  parliament has been constant in its support for the commission the council has played its part and now is the time to decide on the tenders <eos>\n",
      "pred: for far as we are concerned this yardstick of nice codecision voting is be a yardstick for measuring success of nice nice summit and it will linked to the s right in the <eos>\n",
      "tar:  as far as we are concerned the extension of qualified majority voting will be a yardstick for the success of the nice summit and it is linked with parliament' s involvement through codecision <eos>\n",
      "epoch:4  Step:90000  loss:1.5496529341  WER:0.4343257443\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also generates many jobs jobs on the processing industry and in the shipyards industry industry work work work work work work work work <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:1125  loss:1.9150232887  WER:0.4196205023\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 7.014072589623692e-05\n",
      "pred: i congratulate like to again to congratulate the rapporteur and take on favour of this report <eos>\n",
      "tar:  i should like once again to congratulate the rapporteur and vote in favour of this report <eos>\n",
      "pred: lastly we great of caution is the provision of information and is precisely it the relationship between parliament parliament and the commission commission in commission that parliament in the treaty have the and that right to access to information the in principle <eos>\n",
      "tar:  lastly a word of caution about the question of information this is what <unk> the relationship between our parliament and the previous commission: the fact that we under the treaties have access and the right of access to all information in principle <eos>\n",
      "epoch:4  Step:92000  loss:1.6291587353  WER:0.3934065934\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates many jobs jobs on the processing industry and in the yards <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:1150  loss:1.9246984243  WER:0.4139571130\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 6.925005566688934e-05\n",
      "pred: it is our duty to look beyond our borders to to dramatic measures effective measures which to that improve the efficiency efficiency and management of air traffic management without would not lead to this system system which jeopardising the current modus operandi <eos>\n",
      "tar:  it is our duty to look beyond our borders and create meaningful and effective actions and measures to improve the safety efficiency and economics of air traffic management which will not lead to a two-tier system thereby jeopardising the current modus operandi <eos>\n",
      "pred: finally i would to express a special thank you to greece the presidency to the the presidency to the way in has contributed to progress of maritime to maritime with maritime at sea <eos>\n",
      "tar:  finally i wish to say a special thank you to greece the country to hold the presidency for the way it has helped the progress of matters to do with safety at sea <eos>\n",
      "epoch:4  Step:94000  loss:1.6036299467  WER:0.3519021739\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates many jobs jobs in the processing industry and in the manufacturing <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:1175  loss:1.8978701830  WER:0.4309377729\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 6.835938543754176e-05\n",
      "pred: it creates jobs and helps combat marginalisation and improving way and improves improves the environment of affairs environment <eos>\n",
      "tar:  it creates jobs and helps the excluded in this sense and it improves the state of the environment <eos>\n",
      "pred: i would therefore like to ask the president how it view case it ebrd of still still transferred two two billion to euratom to be these projects possible <eos>\n",
      "tar:  i would therefore like to ask the president how in that case the development bank has still transferred eur two billion to euratom to make these projects possible <eos>\n",
      "epoch:4  Step:96000  loss:1.4678413868  WER:0.3275193798\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates many jobs jobs on the processing industry or in the construction industry industry of many of of the <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:1200  loss:1.8830416632  WER:0.4105973838\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 6.746871520819417e-05\n",
      "pred: i the whole i can be be pleased with the proposals proposals in through the the of <eos>\n",
      "tar:  on the whole we should therefore be satisfied with the present proposals <eos>\n",
      "pred: this sort of interaction between lawyers has one of the most that have to to the development and which the our legal civilisation is helped <eos>\n",
      "tar:  that kind of interaction of lawyers is one of the things which contributed most to our civilisation and from which our legal civilisation has grown <eos>\n",
      "epoch:4  Step:98000  loss:1.7655085325  WER:0.4219858156\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore also also creates many jobs jobs in the processing industry or in the manufacturing industry industry the of work of <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:4  Step:1225  loss:1.8776863050  WER:0.4216630842\n",
      "Train\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 6.657804497884659e-05\n",
      "pred: few members will the house will have been informed to know what they were doing at at are in the mercy of their fellow and the party of <eos>\n",
      "tar:  few members in this house can have been expected to know what they were voting on they are at the mercy of their colleagues and the <unk> <eos>\n",
      "pred: mr president of the council the from the start the scientific assessment of the northern hake stocks was been made out in the whole of of there are no sufficient elements for present moment to distinguish between components <eos>\n",
      "tar:  mr president-in-office of the council right from the start the scientific assessment of the northern hake stock has been carried out on the whole stock and there are not sufficient grounds at the moment to distinguish separate components within it <eos>\n",
      "epoch:5  Step:100000  loss:1.3218700886  WER:0.3031250000\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore also also creates many jobs jobs in the processing industry or in the <unk> industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:5  Step:1250  loss:1.8892722130  WER:0.4368816276\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 6.5687374749499e-05\n",
      "pred: the specific issue of oil and natural gas reserves we we are discussing cannot cannot solve this problem problem especially if it comes addressed within the framework of the liberalisation of the market rather than looking the need of the needs <eos>\n",
      "tar:  the particular issue of oil and natural gas reserves which we are examining here cannot resolve the overall problem especially when it is addressed within the framework of the liberalisation of the market rather than with the satisfaction of grass-roots needs as the criterion <eos>\n",
      "pred: the harsh climate of these regions is that <unk> and cycle and higher costs both for arable and <unk> farming <eos>\n",
      "tar:  the cold climate of these regions means a shorter growing season and higher costs both for arable and livestock farming <eos>\n",
      "epoch:5  Step:102000  loss:1.3866131306  WER:0.3200692042\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates many jobs jobs on the processing industry or in the korean industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:5  Step:1275  loss:1.9054901028  WER:0.4224244090\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 6.479670452015141e-05\n",
      "pred: it is now that to better coordinate each s efforts to put an end to this epidemic and prevent prevent other outbreaks in in the future <eos>\n",
      "tar:  it is crucial today to better coordinate everyone' s efforts to put an end to this epidemic and to prevent further crises arising in the future <eos>\n",
      "pred: we believe that these decisions are essential to the of the reform of the union: without any any discussion on its architecture would be academic and that is clear that europe that europe europe european and the central bank need a in the needs in need of a political unity and is has been said a new impetus to economic growth and social cohesion <eos>\n",
      "tar:  we feel that these decisions are crucial in terms of the reform of the union: without them any discussion of its structure will be academic and it is clear that now that the single currency and the central bank have been established europe is in need of new political unity and as has been said a fresh boost for economic growth and social cohesion <eos>\n",
      "epoch:5  Step:104000  loss:1.3252788782  WER:0.3259668508\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this sector also creates many jobs jobs on the processing industry and in the manufacturing industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:5  Step:1300  loss:1.9204598999  WER:0.4249717050\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 6.390603429080383e-05\n",
      "pred: the next item is the report <unk> by mr <unk> on behalf of the committee on budgets on the proposal for a council regulation <unk> ecsc euratom) on the financial regulation applicable to the general budget of the european communities (com(2000) <unk> - <unk> - <unk> <eos>\n",
      "tar:  the next item is the report <unk> by mr dell'alba on behalf of the committee on budgets on the proposal for a council regulation <unk> ecsc euratom) on the financial regulation applicable to the general budget of the european communities (com(2000) <unk> - <unk> - <unk> <eos>\n",
      "pred: the is primarily due to our fact that our is for a measures for manoeuvre in the area chapter the 3 is the budget concerns concerned course concerned by all our our priorities priority enlargement stability and sustainable growth <eos>\n",
      "tar:  this is partly due to the fact that parliament asked for additional room for manoeuvre in this particular area category 3 of the budget is of course characterised by all of our political priorities: enlargement stability and sustainable growth <eos>\n",
      "epoch:5  Step:106000  loss:1.3245363235  WER:0.3131868132\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates many jobs jobs firmly the processing industry or in the construction industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:5  Step:1325  loss:1.8992374563  WER:0.4357744002\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 6.301536406145625e-05\n",
      "pred: i am optimistic and despite all the difficulties the commission commission and the council will agree to further widening in in i would like to thank the rapporteur for his efforts to find compromise solutions which will improve the original proposal <eos>\n",
      "tar:  i am confident that despite all the difficulties the european commission and the council will agree to this increase and finally i should like to thank the rapporteur for his efforts to find compromise solutions which substantially improve the initial proposal <eos>\n",
      "pred: the spanish government always the other hand always <unk> the autonomous by any european communities in the european bodies even when it decisions are decided to decide on the exclusive competences of the autonomous themselves usually happens with ecofin to ecofin and on and fiscal issues on which for basque countries for the basque country and the have full sovereignty <eos>\n",
      "tar:  the spanish government on the other hand repeatedly blocks any participation by the autonomous communities in the european bodies even when the latter are going to decide on the exclusive competences of those regions as usually happens in relation to ecofin and financial and fiscal matters on which the autonomous communities of the basque country and <unk> have full sovereignty <eos>\n",
      "epoch:5  Step:108000  loss:1.1693853140  WER:0.3006230530\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector also creates many jobs jobs in the processing industry or in the shipyards industry industry the the the the the the the renewing renewing the the the renewing the the the the the to the the the the renewing the renewing foot the the renewing to to the <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:5  Step:1350  loss:1.9077993631  WER:0.4291305200\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 6.212469383210866e-05\n",
      "pred: since the commissioner has not a of time i will ask my right to ask a supplementary question <eos>\n",
      "tar:  as the commissioner is under pressure of time i will forego my right to ask a supplementary <eos>\n",
      "pred: we need a european union to have a a a deep and of democratic and to elections need a constitutional reinforcement of the existing parliaments but not need not have a chamber <eos>\n",
      "tar:  we need the european union to be made in a far-reaching way more democratic and parliamentary we need definite constitutional reinforcement of the existing parliaments but we do not need another chamber <eos>\n",
      "epoch:5  Step:110000  loss:1.4321498871  WER:0.3452593918\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: moreover sector also creates many jobs jobs on the processing industry or in the south industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:5  Step:1375  loss:1.8983781242  WER:0.4136943928\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 6.123402360276108e-05\n",
      "pred: similarly with regards as the issue of mixed production is concerned i would like to point that we are here here about food and feedingstuffs and is why we are on the european commission to develop guidelines for the coexistence of genetically modified and conventional production <eos>\n",
      "tar:  similarly as far as the question of mixed crops is concerned i would like to emphasise that we are talking here of foods and feed which is why we call on the european commission to develop guidelines for the coexistence of genetically modified and conventional crops <eos>\n",
      "pred: i believe it is important important that the present socialist government kingdom government is committed to ensuring that the charter is not and does not extend the existing rights to cover that currently covered in the treaties <eos>\n",
      "tar:  i think it is very relevant that the present socialist united kingdom government is committed to ensuring that the charter is <unk> and does not extend beyond existing rights to those not currently contained in the treaties <eos>\n",
      "epoch:5  Step:112000  loss:1.5840489864  WER:0.3622047244\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: in sector also creates many jobs jobs on the processing industry and in the yards industry industry the to progress the the the the the the the the contracts the the the the the the to the the the the and the to the the the the the the the <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:5  Step:1400  loss:1.8990636778  WER:0.4094672871\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 6.034335337341349e-05\n",
      "pred: since 11 events of 11 september terrorism has become a real threat <eos>\n",
      "tar:  since the events of 11 september terrorism has become a genuine threat <eos>\n",
      "pred: moving the <unk> report report on access and interconnection i am grateful for the support for the principles principles of the directive <eos>\n",
      "tar:  on mr <unk> s report on access and interconnection i am thankful for the support for the basic principles of the directive <eos>\n",
      "epoch:5  Step:114000  loss:1.5689029694  WER:0.3947939262\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: this also also creates many jobs jobs on the processing industry or in the south industry industry the progress projects progress the the the on the the projects and the the the the the the projects the the on the projects projects the the the the to the the the the the the the the operation the on the to current the the\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:5  Step:1425  loss:1.8823277283  WER:0.4138073146\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 5.945268314406591e-05\n",
      "pred: i am glad that mr <unk> talked about the european <eos>\n",
      "tar:  i am pleased that mr <unk> talked about the edf <eos>\n",
      "pred: mr president the färm the rapporteur has already everything i i wanted going to say <eos>\n",
      "tar:  mr president mr färm the rapporteur has said all that i was going to say <eos>\n",
      "epoch:5  Step:116000  loss:1.6507749557  WER:0.3896103896\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: in sector also creates many jobs jobs in the processing industry and in the shipyards industry industry the the the on renewable the the region to the the the of on the on site the on the on in on the the on on the the on the on site on the projects the site the on the on renewable site the of\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:5  Step:1450  loss:1.8785231304  WER:0.4221850315\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 5.856201291471833e-05\n",
      "pred: it is particularly important to note that that concerns to the residue limits that is to concept that an is an acceptable dose <eos>\n",
      "tar:  it is particularly important to note this legislation relates to maximum residue limits this is the concept that there is an acceptable dose <eos>\n",
      "pred: all the member states must be involved are involved in this <eos>\n",
      "tar:  all the member states must feel they are participating in this area <eos>\n",
      "epoch:5  Step:118000  loss:1.3659991026  WER:0.3349593496\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: moreover sector also creates many jobs jobs on the processing industry and in the building industry industry <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:5  Step:1475  loss:1.8735203648  WER:0.4015842419\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 5.7671342685370746e-05\n",
      "pred: i is why i warmly the commission's proposal to create up a rapid reaction facility alongside military military academy and a complement it <eos>\n",
      "tar:  that is why i welcome the commission proposal to set up a rapid reaction mechanism alongside the military unit and to supplement echo <eos>\n",
      "pred: i me remind you of article 12 <eos>\n",
      "tar:  let me remind you of article 12 <eos>\n",
      "epoch:5  Step:120000  loss:1.6497399807  WER:0.3904109589\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore also also creates many jobs jobs on industry processing industry and in the yards industry industry and site projects operation operation or or or or into or and operation of and contracts or projects operation site or of operation operation or of operation operation site school site and or site operation operation operation or operation site operation projects operation of operation operation\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:5  Step:1500  loss:1.8641761208  WER:0.4415630767\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 5.6780672456023166e-05\n",
      "pred: i i want like to say in this debate is that i am not satisfied happy with the oral questions to the commission and the council <eos>\n",
      "tar:  what i would like to say in this debate is that i am not very happy about the oral questions to the commission and the council <eos>\n",
      "pred: i am it particularly encouraging in the labour of the labour market in the union union the social protection net and the integration of integrating integration of regard to the integration of third-country citizens into the concept of the family in in this text text to of the family family which other words that the family as defined in the text text restricted to parents and children children <eos>\n",
      "tar:  i consider it especially encouraging for the sake of the labour market in the european union the social safety net and the interests of the union with regard to the integration of non-eu nationals that the concept of the family introduced in this draft is that of the nuclear family in other words that the family as defined in the draft is restricted to parents and minor children <eos>\n",
      "epoch:5  Step:122000  loss:1.5410960913  WER:0.3869346734\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: of there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: furthermore sector creates creates many jobs jobs on the processing industry and in the korean industry industry projects <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:5  Step:1525  loss:1.8618582773  WER:0.4097958112\n",
      "\n",
      "-----------------Train Mode-----------------\n",
      "\n",
      "lr: 5.589000222667557e-05\n",
      "pred: unfortunately has am to say has your <unk> and so i do not think it is be very wise to vote this proposal to the vote now which it would certainly be rejected <eos>\n",
      "tar:  politics i regret to say has its <unk> and so i do not feel it would be very wise to put this proposal to the vote now for it would certainly be rejected <eos>\n",
      "pred: i am not to answer in fully nor in because enlargement will have no impact for the regions before 2006 especially above will because why course because it time time of time will have needed to adapt this policy to the new situation that will arise as the <eos>\n",
      "tar:  i am able to reply neither fully nor otherwise because enlargement will have no consequences for the regions before 2006 and that is precisely of course because a proper amount of time will be needed to adjust this policy to the new situation that will arise following enlargement <eos>\n",
      "epoch:5  Step:124000  loss:1.6207162142  WER:0.3696682464\n",
      "\n",
      "-----------------Validation Mode-----------------\n",
      "\n",
      "pred: there there are problems <eos>\n",
      "tar:  admittedly there are problems <eos>\n",
      "pred: moreover sector creates creates many jobs jobs on the processing industry and in the <unk> industries <eos>\n",
      "tar:  this industry also generates numerous land-based jobs in the processing industry and in the <unk> <eos>\n",
      "epoch:5  Step:1550  loss:1.8603407240  WER:0.4326263429\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 82\u001b[39m\n\u001b[32m     79\u001b[39m loss = loss_fn(logits.transpose( \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m ), targ )\n\u001b[32m     81\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), clip_max)\n\u001b[32m     84\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "epoch_num = 10\n",
    "\n",
    "# WarmupとCosine Decayを行うスケジューラを利用\n",
    "num_global_steps = len( train_loader ) * epoch_num\n",
    "print( \"num_global_steps:\", num_global_steps )\n",
    "num_warmup_steps = num_global_steps * 0.1\n",
    "print( \"num_warmup_steps:\", num_warmup_steps )\n",
    "scheduler = get_linear_schedule_with_warmup( optimizer, num_warmup_steps, num_global_steps ) \n",
    "\n",
    "tr_print_coef = 2000\n",
    "val_print_coef = 100\n",
    "#tr_print_coef = 10\n",
    "#val_print_coef = 1\n",
    "\n",
    "#PATH = \"./model_NTT_auto_curr3.pt\"\n",
    "\n",
    "#if device != torch.device(\"cpu\"):\n",
    "#    checkpoint = torch.load(PATH)\n",
    "#else:\n",
    "#    checkpoint = torch.load(PATH, map_location=torch.device('cpu'))\n",
    "\n",
    "#model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device( \"cpu\")\n",
    "\n",
    "## optimizerのstateを現在のdeviceに移す。これをしないと、保存前後でdeviceの不整合が起こる可能性がある。\n",
    "#for state in optimizer.state.values():\n",
    "#    for k, v in state.items():\n",
    "#        if isinstance(v, torch.Tensor):\n",
    "#            state[k] = v.to(device)\n",
    "##epoch = checkpoint['epoch']\n",
    "##loss = checkpoint['loss']\n",
    "\n",
    "\n",
    "history = {\"train_loss\": [], \"val_loss\": [], \"train_wer\": [], \"val_wer\": [] }\n",
    "\n",
    "n = 0\n",
    "train_loss = 0\n",
    "val_loss = 0\n",
    "\n",
    "f_train = open( \"train_it3.log\", mode=\"w\", encoding = \"UTF-8\" )\n",
    "f_val = open( \"val_it3.log\", mode=\"w\", encoding = \"UTF-8\" )\n",
    "\n",
    "tra_global_step = 0\n",
    "val_global_step = 0\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    \n",
    "    model.train()\n",
    "    print( \"Train\")\n",
    "    train_loss = 0\n",
    "    train_wer = 0\n",
    "    n = 0\n",
    "    for i, (src, src_len, tar, tar_len) in enumerate(train_loader):\n",
    "    #for i, (src, src_len, tar, tar_len) in enumerate(val_loader):\n",
    "        #display = True\n",
    "\n",
    "        tra_global_step += 1\n",
    "        if tra_global_step % tr_print_coef  == 0:\n",
    "            display = True\n",
    "            train_loss = 0\n",
    "            train_wer = 0\n",
    "            n = 0\n",
    "        else:\n",
    "            display = False\n",
    "        if display:\n",
    "            print(\"\\n-----------------Train Mode-----------------\\n\")\n",
    "        if display:\n",
    "            print(\"lr:\", optimizer.param_groups[0][\"lr\"] )\n",
    "\n",
    "        src = src[:,:max(src_len)].to(device)\n",
    "        tar = tar[:,:max(tar_len)]\n",
    "        dec_in = tar[:,:-1].to(device)\n",
    "        targ = tar[:,1:].to(device)\n",
    "\n",
    "        logits = model( src, dec_in )\n",
    "        loss = loss_fn(logits.transpose( 1, 2 ), targ )\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_max)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        pre_label_id = torch.argmax( logits, dim = 2 )\n",
    "        predict = []\n",
    "        for pre in pre_label_id:\n",
    "            hypo = []\n",
    "            for m in pre:\n",
    "                hypo.append(token_list_en[m.item()])\n",
    "                if token_list_en[m.item()] == '<eos>':\n",
    "                    break            \n",
    "            predict.append( hypo )\n",
    "        target = []\n",
    "        for pre in targ:\n",
    "            reference = []\n",
    "            for m in pre:\n",
    "                reference.append(token_list_en[m.item()])\n",
    "                if token_list_en[m.item()] == '<eos>':\n",
    "                    break            \n",
    "            target.append( reference )\n",
    "        total_error = 0\n",
    "        total_token_length = 0\n",
    "        for i3, (pred, tar) in enumerate( zip(predict, target) ):\n",
    "            (error, substitute, \n",
    "                delete, insert, ref_length) = \\\n",
    "                levenshtein.calculate_error(pred,tar)\n",
    "            # 誤り文字数を累積する\n",
    "            total_error += error\n",
    "            # 文字の総数を累積する\n",
    "            total_token_length += ref_length  \n",
    "\n",
    "            if i3 < 2 and display:\n",
    "                print( \"pred:\", ' '.join(pred) )\n",
    "                print( \"tar: \", ' '.join(tar) )\n",
    "            \n",
    "        train_loss += loss.item()\n",
    "        train_wer += total_error / total_token_length\n",
    "        n += 1\n",
    "        history[\"train_loss\"].append( train_loss / n )\n",
    "        history[\"train_wer\"].append( train_wer / n )\n",
    "        #if i % tr_print_coef == tr_print_coef - 1:\n",
    "        if display:\n",
    "            print(f\"epoch:{epoch+1}  Step:{tra_global_step}  loss:{train_loss/n:.10f}  WER:{ train_wer / n:.10f}\")\n",
    "        if tra_global_step % 2000 == 0:\n",
    "            PATH = './model_NTT_auto_curr_it3.pt'\n",
    "            torch.save({'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss,},\n",
    "               PATH)\n",
    "            #with open(\"history_NTT_auto_curr_it3.pkl\", \"wb\") as f:\n",
    "            #    pickle.dump( history, f )\n",
    "\n",
    "        f_train.write( \"Epoch: \" + str(epoch) + \", Step: \" + str(tra_global_step) + \", Loss: \" + str(train_loss/n) + \", WER: \" + str(train_wer / n ) + \"\\n\" )\n",
    "        f_train.flush()\n",
    "        \n",
    "        display_taihi = display\n",
    "        #if tra_global_step % 100 == 0:\n",
    "        if display:\n",
    "            val_loss = 0\n",
    "            val_wer = 0\n",
    "            val_n = 0\n",
    "            print(\"\\n-----------------Validation Mode-----------------\\n\")\n",
    "\n",
    "            for i_val, (val_src, val_src_len, val_tar, val_tar_len) in enumerate(val_loader):\n",
    "                val_global_step += 1\n",
    "                if i_val == 0:\n",
    "                    display = True\n",
    "                else:\n",
    "                    display = False\n",
    "                val_src = val_src[:,:max(val_src_len)].to(device)\n",
    "                val_tar = val_tar[:,:max(val_tar_len)]\n",
    "                val_dec_in = val_tar[:,:-1].to(device)\n",
    "                val_targ = val_tar[:,1:].to(device)\n",
    "\n",
    "                logits = model( val_src, val_dec_in )\n",
    "                loss = loss_fn(logits.transpose( 1, 2 ), val_targ )\n",
    "\n",
    "                pre_label_id = torch.argmax( logits, dim = 2 )\n",
    "                predict = []\n",
    "                for pre in pre_label_id:\n",
    "                    hypo = []\n",
    "                    for m in pre:\n",
    "                        hypo.append(token_list_en[m.item()])\n",
    "                        if token_list_en[m.item()] == '<eos>':\n",
    "                            break            \n",
    "                    predict.append( hypo )\n",
    "                target = []\n",
    "                for pre in val_targ:\n",
    "                    reference = []\n",
    "                    for m in pre:\n",
    "                        reference.append(token_list_en[m.item()])\n",
    "                        if token_list_en[m.item()] == '<eos>':\n",
    "                            break            \n",
    "                    target.append( reference )\n",
    "                val_total_error = 0\n",
    "                # 文字の総数を累積する\n",
    "                val_total_token_length = 0\n",
    "                for i3, (pred, tar) in enumerate( zip(predict, target) ):\n",
    "                    (error, substitute, \n",
    "                        delete, insert, ref_length) = \\\n",
    "                        levenshtein.calculate_error(pred,tar)\n",
    "                    # 誤り文字数を累積する\n",
    "                    val_total_error += error\n",
    "                    # 文字の総数を累積する\n",
    "                    val_total_token_length += ref_length  \n",
    "\n",
    "                    if i3 < 2 and i_val == 0:\n",
    "                        print( \"pred:\", ' '.join(pred) )\n",
    "                        print( \"tar: \", ' '.join(tar) )\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_wer += val_total_error / val_total_token_length\n",
    "                val_n += 1\n",
    "                #print( \"val_wer:\", val_wer )\n",
    "                #print( \"val_n:\", val_n )\n",
    "                history[\"val_loss\"].append( val_loss / val_n )\n",
    "                history[\"val_wer\"].append( val_wer / val_n )\n",
    "            print(f\"epoch:{epoch+1}  Step:{val_global_step}  loss:{val_loss/val_n:.10f}  WER:{ val_wer / val_n:.10f}\")\n",
    "            f_val.write( \"Epoch: \" + str(epoch) + \", Step: \" + str(val_global_step)  + \", Loss: \" + str(val_loss/val_n) + \", WER: \" + str(val_wer / val_n ) + \"\\n\" ) \n",
    "            f_val.flush()\n",
    "        if tra_global_step % 2000 == 0:\n",
    "            with open(\"history_NTT_auto_curr_it3.pkl\", \"wb\") as f:\n",
    "                pickle.dump( history, f )\n",
    "        display = display_taihi\n",
    "\n",
    "f_train.close()\n",
    "f_val.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d4d9fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
